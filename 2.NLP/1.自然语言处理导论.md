#

# 自然语言处理导论

> **【备注】本笔记是全书的精简版，收录个人觉得比较重要的概念和方法。**
>
> **初学者建议去看原书：**https://intro-nlp.github.io/

自然语言处理（Natural Language Processing，NLP）是计算机科学领域和人工智能领域的重要 研究方向之一，旨在探索实现人与计算机之间用自然语言进行有效交流的理论与方法。它融合了语言学、计算机科学、机器学习、数学、认知心理学等多学科内容，涉及从字、词、短语到句子、段落、篇章的多种语言单位，以及处理、理解、生成等不同层面的知识点，研究内容涉及的知识点多且复杂。自20世纪90年代以来，自然语言处理发展迅猛，各类任务和算法和研究范式层出不穷，在搜索引擎、医疗、金融、教育、司法等众多领域展示出重要作用。

## 1.1 自然语言处理基本概念

自然语言处理目标就是**实现人机之间的有效通信**，意味着要使计算机能够理解自然语言的意义，也能以自然语言文本来表达给定的意图、思想等。前者称为**自然语言理解（Natural Language Understanding，NLU）**，后者称为**自然语言生成（Natural Language Generation，NLG）**。需要说明的是，自然语言处理、自然语言理解以及计算语言学这些概念并没有严格统一的定义。本书采用吴立德教授在 1997 年所著的《大规模中文文本处理》中所给出的定义。无论是自然语言理解还是自然语言生成，目前都是开放性问题（Open Problem），通用的高精度高鲁棒自然语言处理系统还没有解决方案，仍然需要长期研究。

### 1.1.1 自然语言处理简史

自然语言处理的研究历史可以追溯到 1947 年，当时第一台通用计算机 ENIAC 也才刚刚面世一年，Warren Weaver 就提出了利用计算机翻译人类语言的可能，并于 1949 年发布了著名的《Translation》（翻译）备忘录。1950年，Alan Turing发表了著名的具有划时代意义的论文《Computing Machinery and Intelligence》（计算机器与智能），提出了使用图灵测试（Turing Test）对机器是否具备智能进行评测，即如果一台机器能够与人类展开对话而不能被辨别出其机器身份，那么这台机器具有智能。1951 年语言学家 Yehoshua Bar-Hillel 在麻省理工学院开始了机器翻译研究。1954 年乔治城大学（Georgetown University）与IBM合作的机器翻译演示系统将 60 多个俄语句子翻译成了英文。研究者们当时期望通过三到五年的时间完全解决机器翻译问题。20 世纪 50 年代初是自然语言处理的萌芽期。自然语言处理简史的时间线如图1.1所示。大体来看自然语言处理经历了 20 世纪 50 年代末到 60 年代的初创期、20 世纪 70 年代到 80 年代的理性主义时代、20 世纪 90 年代到 21 世纪初的经验主义时代以及 2006 年至今的深度学习时代。

![image-20230917221538054](./1.自然语言处理导论.assets/image-20230917221538054.png)

从 20 世纪 90 年代开始，自然语言处理开启了繁荣发展的时代。自 1989 年机器翻译任务中引入语料库方法之后，这种建立在大规模真实语料上的研究方法将自然语言处理研究推向了新的高度。从90年代后期开始，基于机器学习和数据驱动的方法取代了早期基于规则和基于逻辑的方法，成为自然语言处理的标准模式。自然语言处理的各类任务，包括词法分析、词性标注、句法分析、文本分类、机器翻译等都开始引入机器学习算法。这期间朴素贝叶斯（Naive Bayes）[5]、K 近邻（K-nearest neighbor）[6]、支撑向量机（Support Vector Machine，SVM）[7]、最大熵模型（Maximum Entropy, ME）[8]、神经网络（Neural Network）[9]、条件随机场（Conditional Random Fields）[10]、感知机（Perceptron）[11]等方法也都在自然语言处理不同任务上进行了尝试并取得了一定的成功。这种以大规模数据为基础进行分析的方法称为经验主义（Empiricism）。随着数据驱动方法的发展，大部分关于自然语言处理的早期理论都大打折扣，特别是数据量的不断增加以及计算能力的不断提高，经验主义方法直到现在也还在主导着自然语言处理领域。从当前自然语言处理领域重要会议EMNLP（Empirical Methods in Natural Language Processing ）的名称和发展也可以看到经验主义的发展过程。

2006年加拿大多伦多大学教授Geoffery Hinton和他的学生Ruslan Salakhutdinov 在《科学》杂 志上发表了基关于于深度信念网络（DeepBeliefNetworks, DBN）以及无监督预训练结合有监督训练微调的方法解决深层神经网络训练中梯度消失问题的论文[12]，将神经网络重新拉回到机器学习研究者的视野中。2012年基于卷积神经网络（Convolutional Neural Network，CNN）网络的AlexNet在图像识别领域 ImageNet 竞赛中取得惊人的效果，开启了深度学习在学术界和工业界的浪潮[13]。 2011 年论文《Natural language processing (almost) from scratch》（从零开始的 NLP）引起了极大的 关注，深度神经网络可以在不使用人工特征的情况下，用一个统一的网络架构在词性标注、组块 分析、命名实体识别、语义角色标注等任务中都取得了很好的效果[14]。2014 年 Seq2Seq（序列到 序列）的模型[15] 在机器翻译任务上取得了非常好的效果，并且完全不依赖任何人工特征，推动了 神经机器翻译的广泛落地。这种端到端的方式进行编码和解码的方式不仅有效推动了包括生成式 摘要[16]、对话系统[17, 18] 等在内的自然语言生成问题上取得了突破，还应用于自然语言处理中的 很多任务，包括句法分析[19]、问题回答[20]、中文分词[21] 等。此外，循环神经网络（Recurrent neural network，RNN）[22]、长短时记忆网络（Long Short Term Memory Network, LSTM）[23]、递归神经网 络（Recursive Neural Network）[24]、卷积神经网络（Convolutional Neural Network, CNN）[25]、图 神经网络（Graph Neural Networks，GNN）[26, 27] 等神经网络模型也都成功应用于自然语言处理各个任务中。

2018年美国艾伦人工智能研究所（Allen Institute for AI）和华盛顿大学（Washington University） 联合发表的论文中提出了名为 ELMo 的上下文相关的文本表示方法，首先利用语言模型或其他自 监督任务进行预训练，此后在处理下游任务时，从预训练的网络中提取对应单词的网络各层的单 词嵌入作为新特征补充到下游任务中，在多个自然语言处理任务上表现非常突出[28]。此后，深度 学习开启了预训练模型（Pre-trained Models，PTM）结合任务微调的新范式。谷歌、OpenAI、微软、清华大学、百度、智源研究院等先后提出了BERT[29]，GPT[30]，XLNet[31]、ERNIE(THU)[32]、 ERNIE(Baidu)[33]、悟道等大规模预训练模型，在几乎所有自然语言处理任务中都取得了非常好的 效果，甚至在很多任务的标准评测集合上取得了超越人类准确率的水平。尤其是在类似阅读理解、 常识推理等任务上有惊人的效果提升。与此同时，预训练模型的规模也越来越大，2018 年谷歌开 发的BERT-Base 模型有 1.1 亿参数，BERT-Large 模型有 3.4 亿参数，到了 2019 年 OpenAI 开发的 GPT-2 模型就达到了 15 亿参数量。2021 年GPT-3 模型参数量更是达到了 1750 亿，而同年谷歌开 发的 Switch Transformer 模型参数量首次超过万亿，达到了 1.6 万亿。在此之后不久，北京智源研 究院所发布的“悟道 2.0”模型就刷新了上述记录，模型参数量达到了 1.75 万亿。虽然预训练大模 型取得了巨大的成功，但是仍然面临模型鲁棒性亟待提升、超大规模模型如何高效适配下游任务、大模型的理论解释等诸多问题。

### 1.1.2 自然语言处理的主要研究内容

自然语言处理的研究内容十分庞杂，整体上可以分为**基础算法研究**和**应用技术研究**。基础算法研究又可以细分为**自然语言理解**和**自然语言生成**。从语言单位角度看，涵盖了字、词、短语、句 子、段落以及篇章等不同粒度。从语言学研究角度看则涉及形态学、语法学、语义学、语用学等 不同层面。此外，由于目前绝大多数自然语言处理算法采用基于机器学习的方法，针对特定的自然语言处理任务，以有监督、无监督、半监督、强化学习等不同的机器学习算法为基础进行构建。因此，自然语言处理研究又与机器学习和语言学研究交织在一起，使得自然语言处理的研究内容 涉及范围广，学科交叉度大。

自然语言处理研究与语言学密切相关，语言学研究可以划分为形态、语法、语义、语用等几个层面。从自然语言处理研究内容的难度来看，形态、语法、语义到语用是逐层递增的。目前基于机器学习和深度学习的自然语言处理算法处理主要集中在形态、语法以及语义这三个层面，基于目前的处理框架，部分语义层面的任务仍较难突破，语用层面的任务难度更大，在该层面的研究 相对较少。我们从语言单元粒度和语言学研究层次两个维度，对自然语言处理的主要研究内容进行了归类，如图1.2所示。

- 基础算法研究
  - 自然语言理解
  - 自然语言生成
- 应用技术研究

![image-20230917222929111](./1.自然语言处理导论.assets/image-20230917222929111.png)

自然语言处理在词汇粒度下的研究内容主要包括：词形分析、词性标注、词义消歧，分别针对单词的词性、语法、语义开展研究。句法分析则是主要针对句子根据语法进行结构分析。篇章 分析核心是对篇章的连贯性和衔接性进行分析，涉及到篇章级别语法结构，同时也包含部分语义 的内容。而语义分析研究则涉及到从词汇、短语、句子到篇章等各个粒度。语言模型主要聚焦于 句子粒度，但是也包含部分短语和篇章级别的研究。以上研究内容主要围绕自然语言理解的基础问题开展。自然语言生成则主要研究利用常识、逻辑和语法等知识自动生成文本，涉及形态、语法和语义层面，同时也涵盖从短语到篇章多个粒度。在自然语言处理基础研究内容之上，信息抽取、情感分析、文本摘要、机器翻译、智能问答、对话系统等任务则围绕自然语言处理的应用开展，所处理的语言单元也根据任务特性而不尽相同。

整体上来看，自然语言处理的主要研究内容围绕语言学基础理论，在形态、语法以及语义等层面开展自然语言理解基础算法和自然语言生成基础算法研究。在此基础上围绕自然语言处理的重要应用场景开展一系列的应用技术研究。这些研究内容也已经深度应用于信息检索、虚拟助理、推荐系统、量化交易、智能问诊、精准医疗等众多系统中。

### 1.1.3 自然语言处理的主要难点

自然语言理解和自然语言生成都是十分困难的任务，这种困难的根本原因是**自然语言在各个层面都广泛存在的各种各样的歧义性或多义性（Ambiguity）**。自然语言文本从形式上是由字符（包 括中文汉字、英文字母、符号）组成的字符串。由字母或者汉字可以组成词，由词可以组成词组， 由词组可以组成句子，进而组成段落、篇章。无论哪种粒度的语言单元，还是从一个层级向上一个 层级转变中都存在歧义和多义现象。形式上一样的字符串，可以理解为不同的词串、词组串，并 有不同的意义[1]。Joseph F. Kess 和 Ronald A. Hoppe 甚至还提出了“语言无处不歧义”的理论[34]。**在某种程度上，我们也可以说自然语言处理基础任务的核心就在于解决歧义问题。**

1. **语音歧义**：主要体现在口语中，是由于语言中同音异义词（Homophone ）、 爆破音不完全、重音位置不明确等原因造成的。

   > 例如：请问您贵姓？ 免贵姓 zhang。这组对话中“zhang”既可以是“张”，也可以是“章”。汉语中同音异义词也有非常多，例如：“chéng shì：城市、程式、成事、城事”、“jìn shì：近视、进士、尽是”、“shǒu shì：首饰、手势”等。

2. **词语切分歧义**：是由字符组成词语时的歧义现象。对于英语等印 欧语系的语言来说，绝大部分单词之间都由空格或标点分割。但是对于汉语、日语等语言来说，单词之间通常没有分隔符。对于这些语言来说，这些连续的字符切分为单词时就会产生歧义。

   > 例如：语言学是一门基础学科。 这门语言学起来很困难。
   >
   > 该例句中“语言学”、“语言”都是词语，在同一个句子中就会出现多种切分方法。这种切分歧义在汉语中普遍存在。我们将在第 2 章详细讨论词语切分歧义的问题以及词语切分的方法。

3. **词义歧义**：是指词语具有相同形式但是不同意义。这种歧义在各种语言 中都广泛存在，通常越是常见的词语其词义数量就越多。例如“打”字在《现代汉语词典（第七版）》中，有两个读音“dá”和“dǎ”，分别作为量词、动词和介词，在作为动词时“打”字有 24
   个意项[35]。

   > 打 dǎ 动词： (1) 用手或器具撞击物体：~ 门 |~ 鼓 (2) 器皿、蛋类等因撞击而破碎：碗 ~ 了 | 鸡飞蛋 ~ (3) 殴打；攻打：~ 架 |~ 援 (4) 发生与人交涉的行为：~ 官司 |~ 交道 (5) 汲取；盛取：~ 米 |~ 酱油 · · ·
   >
   > 英语中存在大量类似的情况，例如根据WordNet 中给出的定义，单词“bank”具有名词和动词两种词性，作为名词时具有 10 种词义。

4. **结构歧义**：是由词组成词组或者句子时，由于其组成的词或词组间可能存 在不同的语法或语义关系而出现的（潜在）歧义现象。结构歧义有时也称为语法歧义（Grammatical Ambiguity）。冯志伟教授在文献 [37] 中对结构歧义进行了系统的描述，其中一些典型的结构歧义如下：

   > - “VP+ 的 + 是 +NP”型歧义结构： 例如：反对 | 的 | 是 | 少数人
   >   该类型歧义中，VP 是一个双向动词，“VP+ 的”是主语，“是 +NP”是谓语，整个句式是个 一个主谓结构。由于主语部分的“VP+的”既可以是施事，也可以是受事，因而会产生歧义。 这个例子中既可以理解为“提反对意见的是少数人”，也可以理解为“所反对的是少数人”。
   > - “VP+N1+ 的 +N2”型歧义结构： 例如：咬死了 | 猎人 | 的 | 狗
   >   该类型歧义中，N1 作为VP的宾语，述宾结构“VP+N1”加上“的”之后，作为名词N2 的 定语，整个结构是一个定中结构。但是 N1 又可以与“的”结合在一起作为 N2 的定语，构 成“N1+ 的 +N2”，这个名词词组作为 VP 的宾语，整个结构构成一个述宾结构。这个例子 中既可以理解为“咬死了一只猎人的狗”，也可以理解为“一只把猎人咬死的狗”。
   > - “N1+ 和 +N2+ 的 +N3”型歧义结构： 例如：桌子 | 和 | 椅子 | 的 | 腿 该类型歧义是由于连词“和”的管辖范围的不同造成的潜在歧义。这个例子中既可以理解为 “桌子和（椅子的腿）”，也可以理解为“（桌子和椅子）的腿”。 
   >
   > 类似的结构歧义类型有很多，例如：“ADJ+N1+N2”、“VP+ADJ+的+N”等。这些歧义的不同理解会造成不同的句法结构以及语义上的不同。句法分析的主要难度就是解决结构歧义问题。我们将在第 3 章对结构歧义以及如何进行句法分析进行详细介绍。

5. **指代和省略歧义**：在由多个句子组成的段落或篇章中，各种歧义依然存在，例如指代歧义和省略歧义。

   **指代歧义** （Demonstrative Ambiguity）是指代词（如我，你，他等）和代词词组（如“那件事”，“这一点”等） 所指的事件可能存在歧义。 
   
   > 例如：猴子吃了香蕉，因为它饿了。 猴子吃了香蕉，因为它熟透了。
   >
   > 上述两个句子的前半句完全相同，“它”可以指代“猴子”和“香蕉”，需要根据后半句的谓词决定指代关系.
   
   **省略歧义**（Ellipsis Ambiguity）是指自然语言中由于省略所产生的歧义。省略是自然语言中的 一种重要的语言现象，尤其在汉语中省略现象非常常见。省略掉一些成分，在绝大部分情况下不会影响句子的表达，但是还是存在一些由于省略造成歧义的问题。
   
   > 例如：县政府同意乡政府报告。 
   >
   > 这个例子中省略了助词“的”，因此使得该句具有两种解释，一个是县政府同意乡政府的那份报告，
   > 另外一个是县政府同意乡政府作出报告。

6. **语用歧义**：是指由于上下文、说话人属性、场景等语用方面的原因造成 的歧义。一句话在不同的场合、由不同的人说、不同的语境，都可能产生不同的理解。 

   > 例如：下例由于场景的不同，同样的句子可以有不同的意义。 句子：你知道南京路怎么走吗？
   >
   >  (1) 如果说话人是游客，说话的对象是警察，那么这句话的含义就是问路。
   >
   >  (2) 如果说话人同样是游客，但是说话的对象换成出租车司机，那么这句话的含义就是询问出租车司机是否可以送他到南京路。

​	从上述介绍中，可以看到自然语言中存在大量的歧义现象。对人类而言，这些歧义在绝大多 数的情况下都可以根据上下文以及相应的语境和场景得到解决。这也就是为什么我们平时使用自 然语言交流并没有感知到语言的歧义。但是，为了消解这些歧义，需要使用大量的知识进行推理 才能完成。而如何表示知识和使用知识、如何完整收集和整理知识以及常识都是极其困难的问题。 莫拉维克悖论（Moravec’s paradox）对自然语言处理也依旧适用。也正是由于这些问题，才使得消 解歧义是自然语言处理中最大的难点之一。 

​	此外，自然语言并不是一成不变的，而是在动态发展中，存在大量未知语言现象。新词汇、新含义、新用法、新句型等层出不穷[38]。 例如：

- 新词汇：双碳、双减、绝绝子、社恐、元宇宙 
- 新含义：躺平、打工人、凡尔赛、青蛙、潜水、盖楼 
- 新用法：走召弓虽、YYDS、回忆杀、求扩列、orz 
- 新句型：纠结的说、看书ing、一整个无语住

这些层见迭出的语言现象对于自然语言处理系统来说也是巨大的挑战。无论是自然语言处理基础任务还是应用系统，如何应对这些未知的情况都是巨大的挑战。

## 1.2 自然语言处理的基本范式

自然语言处理的发展经历了从理性主义到经验主义，再到深度学习三个大的历史阶段。在发 展过程中也逐渐形成了一定的范式，主要包括：基于规则的方法、基于机器学习的方法以及基于 深度学习的方法。这三种范式也基本对应了自然语言处理的不同发展阶段的重点。需要特别说明 的是，虽然以上三种范式来源于自然语言处理的不同发展阶段，有明显的发展先后顺序，并且在 大部分自然语言处理任务的标准评测集合中基于深度学习的方法都好于基于机器学习的方法，更 优于基于规则的方法很多。但是，这三种范式各有利弊，在实际应用中需要根据任务的特点、计 算量、可控制性以及可解释性等具体情况进行选择。 

上述三种范式虽然有很大的不同，但是都有一个相同点就是需要针对特定任务进行构建。面向不同的任务，按照不同的范式构建数据、模型等不同方面，所得到的算法或者系统仅能够处理 特定的任务。在机器学习和深度学习范式下，甚至对模型预测目标进行微小修正，通常都需要对 模型进行重新训练。对于未知任务的零样本学习（Zero-shot Learning）能力很少在上述范式中进行 讨论和研究。基于机器学习和深度学习范式也很难实现模型对未知任务的泛化。2022 年 11 月随 着 ChatGPT 的发布，大模型所展现出来的文本生成能力以及对未知任务的泛化能力使得未来的自 然语言处理的研究范式很可能会发生非常大的变化。因此，本节中也将简要介绍大模型研究范式的雏形。

## 1.2.1 基于规则的方法

基于规则的自然语言处理方法的主要思想是通过词汇、形式文法等制定的规则引入语言学知 识，从而完成相应的自然语言处理任务。这类方法在自然语言处理早期受到了很大的关注，包括机器翻译在内的很多自然语言处理任务都采用此类方法。甚至目前仍有很多系统还在使用基于规 则的方法。基于规则的方法基本流程如图1.3所示，主要包含：数据构建、规则构建、规则使用和效果评价等四个部分。

![image-20230917230638334](./1.自然语言处理导论.assets/image-20230917230638334.png)

基于规则的方法核心是规则形式定义，其目标是使得语言学家可以在不了解计算机程序设计 的情况下，能够容易地将知识转换为规则。这就要求规则描述要具有足够的灵活性并易于使用和 理解。规则引擎的目标是高效地解析这些人工定义的大量规则，针对输入数据根据规则库进行解释执行，从而完成特定任务。这种方式可以使得语言学家不需要编写代码就可以完成规则库构建。

常见的规则包括产生式、框架、自动机、谓词逻辑、语义网等形式。例如，产生式规则是以“IF-THEN”形式构造，表示如果满足条件，则执行相应的语义动作。举例来说，对于机器翻译任 务可以构造如下规则库： 

> IF 源语言主语 = 我 THEN 英语译文主语 =I 
>
> IF 英语译文主语 =I THEN 英语译文 be 动词为 am/was 
>
> IF 源语言 = 苹果AND 没有修饰量词 THEN 英语译文 =apples

条件判断中也可以结合正则表达式，增强规则的泛化能力。再比如，可以根据英语的词典，构造 有限状态自动机（Finite State Automaton，FSA）进行英语单词的拼写检查。除此之外，非确定有限 状态自动机（Nondeterministic Finite Automaton，NFA）、有限状态转录机（Finite State Transducers，FST）还广泛应用于词法分析、词性标注、句法分析、机器翻译等众多方面。

基于规则的方法从某种程度上可以说是在试图模拟人类完成某个任务时的思维过程。这类方 法主要优点是直观、可解释、不依赖大规模数据。利用规则所表达出来的语言知识具有一定的可 读性，不同的人之间可以相互理解。规则分析引擎通过规则库所得到的分析结果，也具有很好的 解释性。所使用的规则就可以作为系统作出判断的依据。规则库的构造也能够完全不依赖于大规 模的有标注数据，可以仅根据人类背景知识进行构建。但是，基于规则的方法也有明显的缺点，主 要包括覆盖率差、大规模规则构建代价大、难度高等。人工构建规则可以较为容易处理常见现象， 但是对于复杂的语言现象难以描述。由于语言现象的复杂性，使得基于规则的方法整体覆盖率很 难提升到非常高的程度。并且，规则库达到一定数量之后维护困难，新增加的规则与已有规则也 容易发生冲突。不同人对于同一问题的解决思路的不同，也造成了大规模规则库中规则的不一致性，从而使得维护难度进一步提高。

### 1.2.2 基于机器学习的方法

基于机器学习的自然语言处理算法绝大部分采用有监督分类算法，将**自然语言处理任务**转化 为**某种分类任务**，在此基础上根据任务特性构建特征表示，并构建大规模的有标注语料，完成模型训练。其基本流程如图1.4所示，通常分为四个步骤：数据构建、数据预处理、特征构建以及模型学习。

![image-20230918114208716](./1.自然语言处理导论.assets/image-20230918114208716.png)

(1) 数据构建阶段主要工作是针对任务的要求构建训练语料，也称为语料库（Corpus）。随着 自然语言处理研究的不断发展，很多任务都有公开的基准测试集合（Benchmark），可以方便地用 来进行模型训练以及模型之间的横向对比。针对没有公开数据的任务，也可以采用人工标注的方 法构建训练语料。 

(2) 数据预处理阶段主要工作是利用自然语言处理基础算法对原始输入，从词汇、句法、结构、语义等层面进行处理，为特征构建提供基础。根据所处理语言和针对任务的不同，采用不同的模 块和流程。对于汉语通常需要进行分词，对于英语通常需要进行词干提取和单词的规范化。在此 之后，根据特征构建的需求，还可能需要进行词性标注、句法分析、语义角色标注等。

 (3) 特征构建阶段主要工作是针对不同任务从原始输入、词性标注、句法分析、语义分析等结果和数据中提取对于机器学习模型有用的特征。例如，针对属性级情感倾向分析任务，需要根据 目标属性，从句法分析结果提取该属性在对应句子中的评价词等信息。特征定义一般都是由人工 完成，根据经验选取适合的特征，这项工作又被称为特征工程（Feature Engineering）。由于针对 自然语言任务构建的特征通常维数非常高又非常稀疏，因此还会利用特征选择算法降低特征维度。 也可以通过特征变换，根据人工设计的准则进行有效特征提取，例如：主成分分析、线性判别分析、独立成分分析等。

(4) 模型学习阶段主要工作是根据任务，选择合适的机器学习模型，确定学习准则，采用相应的优化算法，利用语料库训练模型参数。机器学习模型有很多类型，从不同的维度可以分为：分类模型、回归模型、排序模型、生成式模型、判别式模型、有监督模型、无监督模型、半监督模型、 弱监督模型等等类别，需要根据任务的目标以及特性选择适合的模型。学习准则是机器学习模型 中重要的因素，包括 0-1 损失函数（0-1 Loss Function）、平方损失（Quadratic Loss Function）、交叉熵损失函数（Cross-Entropy Loss Function）、Hinge 损失函数（Hinge Loss Function）等。针对所选 择的模型和学习准则需要选择相应的优化算法，包括梯度下降（Gradient Descent Method）、牛顿法（Newton Method）、拟牛顿法（Quasi Newton Method）、随机梯度下降（Stochastic Gradient Descent， SGD）等。机器学习三要素：模型、学习准则、优化算法的选择都会对算法的效果产生影响。此外， 模型中通常包含一些可以调整的超参数（Hyper-parameters），也需要通过实验和经验进行选择。 

通过整体流程可以看到，基于机器学习方法的自然语言处理算法需要针对任务构建大规模训练语料，以人工特征构建为核心，针对所需的信息利用自然语言处理基础算法对原始数据进行预 处理，并需要选择合适的机器学习模型，确定学习准则，以及采用相应的优化算法。整个流程中需 要人工参与和选择的环节非常多，从特征设计到模型，再到优化方法以及超参数，并且这些选择 非常依赖经验，缺乏有效的理论支持。也使得基于机器学习的方法需要花费大量的时间和工作在 特征工程上。开发一个自然语言处理算法的主要时间消耗在数据预处理、特征构建以及模型选择 和实验上。此外，对于复杂的自然语言处理任务需要在数据预处理阶段引入很多不同的模块，这 些模块之间需要单独优化，其目标并不一定与任务总体目标一致。其次多模块的级联会造成错误传播，前一步错误会影响后续的模型，这些问题都提高了基于机器学习的方法实际应用的难度。

### 1.2.3 基于深度学习的方法

深度学习（Deep Learning）方法通过构建有一定“深度”的模型，将特征学习和预测模型融 合，通过优化算法使得模型自动地学习出好的特征表示，并基于此进行结果预测。基于深度学习 方法的自然语言处理算法基本流程框架如图1.5所示。与传统机器学习算法的流程相比，基于深度学习方法的流程简化很多，通常仅包含数据构建、数据预处理和模型学习三个部分。同时，在数 据预处理方面也大幅度简化，仅包含非常少量的模块。甚至目前很多基于深度学习的自然语言处理算法可以完全省略数据预处理阶段，对于汉语直接使用汉字作为输入，不提前进行分词，对于英语也可以省略单词的规范化步骤。

![image-20230918210322515](./1.自然语言处理导论.assets/image-20230918210322515.png)

深度学习是机器学习的一个子集，通过多层的特征转换，将原始数据转换为更抽象的表示。这些学习到的表示可以在一定程度上完全代替人工设计的特征，这个过程也叫做表示学习（Representation Learning）。与基于特征工程的方法通常所采用的离散稀疏表示不同，深度学习算法通常使 用分布式表示（Distributed Representation），特征表示为低维稠密向量。分布式表示通常需要从底 层特征开始，经过多次非线性变换得到。由于深层结构可以增加特征的重用性，从而使得表示能力指数级增加。因此，表示学习的关键是构建具有一定深度的多层次特征表示[39]。随着深度学习研究的不断深入和计算能力的快速发展，模型深度也从早期的 5 到 10 层增加到现在的数百层。随着模型深度的不断增加，其特征表示能力也不断增强，从而也使得深度学习模型中的预测部分更加简单，预测也更加容易。

自 2018 年 ELMo 模型[28] 提出之后，基于深度学习的自然语言处理范式又进一步演进为预训练微调范式。首先利用自监督任务对模型进行预训练，通过海量的语料学习到更为通用的语言表 示，然后根据下游任务对预训练网络进行调整。这种预训练范式在几乎所有自然语言处理任务上 都表现非常出色。预训练模型在模型网络结构上可以采用LSTM、Transformer 等具有较好序列建模能力的模型，预训练任务可以采用语言模型、掩码语言模型（Masked Language Model）、机器翻译等自监督或有监督方式，还可以引入知识图谱、多语言、多模态等扩展任务。自 2018 年以来有非常多的相关研究，取得了非常好的效果，但仍然面临模型稳健性提升、模型可解释性等诸多问
题亟待解决。第6.4节将对预训练模型进行详细介绍。

### 1.2.4 基于大模型的方法

大模型是大规模语言模型（Large Language Model）的简称。2018 年开始以BERT[29]、GPT[30] 为代表的预训练语言模型相继推出，在各种自然语言处理任务上都得到了非常好的效果。此后，语 言模型的规模不断扩大，2020 年 Open AI 发布的GPT-3 模型[40] 的规模达到了 1750 亿，Google 发 布的 PaLM 模型[41] 的参数量达到了 5400 亿。这种参数量级的语言模型很难再延续此前针对不同 的任务而使用的预训练微调范式。因此，研究人员们开始探索使用采用提示词（Prompt）模式完成各类型自然语言处理任务。此后又提出了指令微调（Instruction Finetuning）[42] 方案，将大量各类型任务，统一为生成式自然语言理解框架，并构造训练语料进行微调。2022 年ChatGPT 所展现 出来的通用任务理解能力和未知任务泛化能力，使得未来自然语言处理的研究范式可能进一步发 生变化。如图1.6所示，基于大模型的自然语言处理的流程转换为：大规模语言模型构建、通用任务能力训练以及特定任务使用三个主要步骤。

![image-20230918210311627](./1.自然语言处理导论.assets/image-20230918210311627.png)

在大规模语言模型构建阶段，通过大量的文本内容，训练模型对长文本的建模能力，使得模型 具有语言生成能力，并使得模型获得隐式的世界知识。由于模型参数量和训练数据量都十分庞大，普通的服务器单机无法完成训练过程，因此需要解决大模型的稳定分布式架构和训练问题。在通用能力注入阶段，利用包括阅读理解、情感分析、信息抽取等现有任务的标注数据，结合人工设计的指令词对模型进行多任务训练，从而使得模型具有很好的任务泛化能力，能够通过指令完成未知任务。特定任务使用阶段则变得非常简单，由于模型具备了通用任务能力，只需要根据任务 需求设计任务指令，将任务中所需处理的文本内容与指令结合，然后就可以利用大模型得到所需结果。

如果该范式在非常多任务上都达到了目前基于预训练微调范式的结果，那么该范式会使得自然语言处理产生质的飞跃。突破了传统自然语言处理需要针对不同任务进行设计和训练的瓶颈，任务可以不需要预先给定，仅依赖很少的任务特定标注数据，或者完全不依赖任何任务的有监督数据就可以得到相应结果。当然，这种方法也仅仅是刚刚展露出一定的希望，当前使用该范式的大模型在绝大部分任务上所得到的效果仍然具有预训练微调范式有很大差距，模型参数量太大导致训练和使用成本过高，等等这些问题都亟待研究。
