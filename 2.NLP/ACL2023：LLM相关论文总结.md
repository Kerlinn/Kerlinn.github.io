# ACL2023：LLM相关论文总结

本届ACL在LLM领域上总计收录了145篇论文，论文收录分布见下表。

| Main-Oral | Main-Poster | Findings | Industry | Workshop | Demo  |
| :-------: | :---------: | :------: | :------: | :------: | :---: |
|    28     |     66      |    40    |    3     |    2     |   6   |

收录的论文中，**上下文学习**、**微调**、**思维链**、**LLM推理能力研究**、**知识增强**是几个主要的研究热点，分别都有10-15篇相关的论文。**评估**、**数据集**、**知识蒸馏**、**LM偏见问题**、**跨语种**等研究课题处在第二梯队，大概分别有5-10篇相关的论文。此外还有**开放领域QA**、**信息检索**、**多模态**、**预训练模型**、**机器翻译**、**LLM生成数据**、**长度外推**、**上下文窗口大小**、**代码生成**等课题的研究，但是论文数量相对少一点。

---

Table of Contents
- [Main](#main)
    - [Main-Oral](#main-oral)
    - [Main-Poster](#main-poster)
- [Findings](#findings)
- [Industry](#industry)
- [Workshop](#workshop)
- [Demo](#demo)


# Main

## Main-Oral

1. ***[Understanding In-Context Learning via Supportive Pretraining Data](https://aclanthology.org/2023.acl-long.708.pdf)***        

    **摘要**：上下文学习（ICL）通过在推理时简单地演示少量的例子来提高语言模型在各种NLP任务中的表现。人们对ICL能力出现的原因不是很了解，因为模型从来没有在这种演示上进行过专门的训练。与之前探索ICL背后的隐性机制的工作不同，我们通过调查预训练数据来研究ICL。具体来说，我们首先采用一种迭代的、基于梯度的方法来寻找支持ICL的一小部分预训练数据。我们观察到，在这个小的子集上继续进行预训练可以显著提高模型的ICL能力，提高幅度高达18%。然后，我们将支持性子集与预训练数据的随机子集进行对比，发现：（1）支持ICL的预训练数据与下游任务的领域相关性不高。（2）支持性的预训练数据有更多很少出现的长尾符号。（3）支持性的预训练数据是具有挑战性的例子，其中长距离上下文的信息增益低于平均水平，表明学习纳入困难的长距离上下文会鼓励ICL。我们的工作迈出了第一步，通过分析实例级的预训练数据来理解ICL。我们的见解有可能在未来通过积极指导预训练数据的构建来提高语言模型的ICL能力。

    **关键词**：上下文学习（ICL）；通过预训练数据研究ICL

1. ***[Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages](https://aclanthology.org/2023.acl-long.61.pdf)***         

    **摘要**：NLP社区主要集中在纵向扩展大型语言模型（LLMs），也就是说，让它们更好地适用于大约100种语言。我们则是横向扩展LLM：通过持续的预训练，我们创建了Glot500-m，一个涵盖511种主要低资源语言的LLM。这项工作的一个重要部分是收集和清理Glot500-c，一个涵盖这511种语言的语料库，使我们能够训练Glot500-m。我们在这些语言的五个不同的任务上对Glot500-m进行评估。与XLM-R基线相比，我们观察到高资源和低资源语言都有很大的改进。我们的分析表明，没有一个单一的因素可以解释多语言LLM表述的质量。相反，各种因素的组合决定了质量，包括语料库的大小、脚本、相关语言的 "帮助 "以及模型的总容量。我们的工作涉及到NLP研究的一个重要目标：我们不应该把NLP限制在世界语言的一小部分，而应该努力支持尽可能多的语言，把NLP技术的好处带给所有语言和文化。代码、数据和模型都在<https://github.com/cisnlp/Glot500>。

    **关键词**：多语种；低资源语言LLM；数据集

1. ***[Evaluating Open-Domain Question Answering in the Era of Large Language Models](https://aclanthology.org/2023.acl-long.307.pdf)***         

    **摘要**：词汇匹配仍然是开放域问题回答（QA）的事实上的评估方法。不幸的是，当一个合理的候选答案没有出现在黄金答案列表中时，词法匹配就会完全失败，随着我们从抽取式模型转向生成式模型，这种情况越来越多。最近大型语言模型（LLMs）在质量保证方面的成功加剧了词法匹配的失败，因为候选答案变得更长，从而使与黄金答案的匹配更具挑战性。如果没有准确的评估，开放领域的质量保证的真正进展仍然是未知的。在本文中，我们对包括LLM在内的各种开放域QA模型进行了彻底的分析，通过对NQ-open这个流行的基准的一个子集上的答案进行手工评估。我们的评估显示，虽然所有模型的真实性能都被大大低估了，但InstructGPT（zero-shot）LLM的性能增加了近+60%，使其与现有的顶级模型相当，而InstructGPT（few-shot）模型实际上在NQ-open上达到了新的最先进水平。我们还发现，超过50％的词法匹配失败归因于语义上等同的答案。我们进一步证明，尽管仍然存在不必要的严格性，但词条匹配对QA模型的排名与人类判断一致。最后，我们证明自动评估模型在某些情况下是词汇匹配的合理替代物，但对于LLMs产生的长式答案来说却不是。自动模型在检测LLM答案中的幻觉方面很吃力，因此无法评估LLMs。目前，似乎没有任何东西可以替代人工评估。

    **关键词**：开放领域QA评估

1. ***[Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In](https://aclanthology.org/2023.acl-long.136.pdf)***         

    **摘要**：检索增强可以通过向语言模型（LM）提供外部信息来帮助它们完成知识密集型任务。之前关于检索增强的工作通常是对检索器和语言模型进行联合微调，使它们紧密结合。在本文中，我们探讨了通用检索插件的方案：检索器是为了帮助那些可能事先不知道或无法一起微调的目标LMs。为了给未见过的目标LM检索有用的文件，我们提出了增强适应性检索器（AAR），它学习从已知源LM获得的LM的偏好。在MMLU和PopQA数据集上的实验表明，我们用一个小的源LM训练的AAR能够显著提高较大的目标LM的zero-shot泛化能力，范围从250M Flan-T5到175B InstructGPT。进一步的分析表明，不同LM的偏好是重叠的，这使得用单一源LM训练的AAR可以作为各种目标LM的通用插件。我们的代码已在<https://github.com/OpenMatch/Augmentation-Adapted-Retriever>上开源。

    **关键词**：检索增强；检索插件；增强适应性检索器

1. ***[Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering](https://aclanthology.org/2023.acl-long.79.pdf)***         

    **摘要**：尽管上下文学习（ICL）的表现令人惊讶，但随机抽取例子作为上下文仍然是一种常见的做法。本文倡导一种新的ICL原则：自适应的上下文学习。引入自适应机制是为了帮助每个样本找到一个可以得出正确预测的上下文中的例子组织（即选择和排列），从而使性能最大化。为了验证自适应ICL的有效性，我们提出了一个通用的“选择-然后-排名”框架，并将其与新的选择和排名算法实例化。在对8个不同的NLP数据集进行广泛评估后，我们的自适应ICL方法比普通的实践设置实现了40％的相对改进。进一步的分析揭示了自适应ICL的巨大潜力，它可能能够缩小ICL和微调之间的差距，因为有更先进的算法。我们的代码将被发布以促进未来的研究。

    **关键词**：自适应上下文学习

1. ***[Pre-Training to Learn in Context](https://aclanthology.org/2023.acl-long.267.pdf)***         

    **摘要**：上下文学习，即预先训练好的语言模型从任务实例和上下文中的指示中学习执行任务，在NLP界引起了很大关注。然而，上下文学习的能力并没有得到充分的利用，因为语言模型没有被明确地训练为在上下文中学习。为此，我们提出了PICL（Pre-training for In-Context Learning）这个框架，通过在一般纯文本语料库中使用简单的语言建模目标对模型进行大量的“内在任务”的预训练，来提高语言模型的上下文学习能力。PICL鼓励模型通过对上下文的调节来推断和执行任务，同时保持预训练模型的任务泛化。我们在七个广泛使用的文本分类数据集和Super-NaturalInstrctions基准上评估了用PICL训练的模型的上下文学习性能，该基准包含100多个针对文本生成的NLP任务。我们的实验表明，PICL比一系列的基线更有效，任务通用性更强，以近4倍的参数胜过较大的语言模型。该代码可在https://github.com/thu-coai/PICL。

    **关键词**：上下文学习预训练

1. ***[Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference](https://aclanthology.org/2023.acl-long.509.pdf)***         

    **摘要**：微调已被证明是一种简单有效的技术，可将预训练语言模型（PLM）所学知识迁移到下游任务中。然而，普通的微调很容易过度适应目标数据并降低泛化能力。现有的大多数研究将其归因于灾难性遗忘，它们不加区分地保留了预训练知识，却没有确定哪些知识是可迁移的。受此启发，我们将微调框定为因果图，并发现灾难性遗忘的关键在于预训练数据中缺失的因果效应。基于因果关系观点，我们提出了微调的统一目标，以找回因果关系。有趣的是，统一目标可以看作是从目标数据中学习新知识的虚构微调目标和从PLM中保留旧知识的因果目标之和。因此，我们的方法是灵活的，可以在保留知识的同时减轻负迁移。由于赋予模型常识是一个长期的挑战，我们在常识QA上实现了我们的方法，并提出了启发式估计以验证其有效性。在实验中，我们的方法在所有六个常识性质量保证数据集上都优于最先进的微调方法，并且可以作为一个插件模块来实现，以提高现有质量保证模型的性能。

    **关键词**：微调方法；缓解灾难性遗忘

1. ***[WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models](https://aclanthology.org/2023.acl-long.507.pdf)***         

    **摘要**：我们介绍WinoQueer：一个专门用于测量大型语言模型（LLM）是否编码了对LGBTQ+群体有害的偏见的基准。该基准来源于社区，通过应用一种从社区调查中生成偏见基准的新方法。我们将我们的基准应用于几种流行的LLM，发现现成的模型通常确实表现出相当大的反同性恋偏见。最后，我们表明，针对边缘化社区的LLM偏差可以通过对有关该社区或由该社区成员撰写的数据进行微调而得到一定程度的缓解，而且社区成员撰写的社交媒体文本比非成员国撰写的有关该社区的新闻文本更为有效。我们的“社区在环”基准开发方法为未来的研究人员提供了一个蓝图，以便为其他边缘化社区开发由社区驱动的、以危害为基础的LLM基准。

    **关键词**：偏见测量基准

2. ***[Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability](https://aclanthology.org/2023.acl-long.524.pdf)***        

    **摘要**：大型多语种语言模型表现出令人惊讶的良好的zero-shot或few-shot机器翻译能力，尽管它们从未见过提供给典型的神经翻译系统的有意包含的翻译实例。我们以Pathways Language Model （PaLM）为例，研究偶然双语的作用——即无意中消费双语信号，包括翻译范例——来解释大型语言模型的翻译能力。我们引入了一种混合方法来衡量和理解规模化的偶然双语性。我们表明，PaLM至少接触了44种语言中的3000多万个翻译对。此外，附带双语内容的数量与非英语语言的单语内涵的数量高度相关。我们将附带的双语内容与零散的提示语联系起来，并表明它可以用来挖掘新的提示语，以提高PaLM的英语以外的零散翻译质量。最后，在一系列小规模的消融中，我们表明它的存在对翻译能力有很大的影响，尽管这种影响随着模型规模的扩大而减弱。

    **关键词**：LLM翻译能力；偶然双语性

3. ***[KILM: Knowledge Injection into Encoder-Decoder Language Models](https://aclanthology.org/2023.acl-long.275.pdf)***        

    **摘要**： 大型预训练语言模型（PLMs）已被证明在其参数中保留了隐性知识。为了增强这种隐性知识，我们提出了知识注入语言模型（KILM），这是一种新的方法，通过持续的预训练的生成性知识填充目标，将实体相关的知识注入到编码器-解码器PLMs中。这是在不对PLM的结构进行修改或增加额外参数的情况下完成的。一套跨越众多数据集的知识密集型任务的实验结果表明，KILM使模型能够保留更多的知识，减少幻觉，同时保持其在一般NLU和NLG任务上的原始性能。KILM还在实体消歧等任务上表现出更好的零散性能，超过了拥有30倍参数的最先进的模型。

    **关键词**：知识注入；幻觉减少

4. ***[When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories](https://aclanthology.org/2023.acl-long.546.pdf)***         

    **摘要**：尽管大型语言模型（LMs）在不同的任务上有令人印象深刻的表现，但在需要丰富的世界知识的任务上仍然很吃力，这意味着在其参数中编码大量的世界知识是很困难的。本文旨在通过对两个以实体为中心的开放领域QA数据集进行大规模的知识探测实验，了解LM在记忆事实知识方面的优势和局限：PopQA是我们的新数据集，包含14000个关于长尾实体的问题，而EntityQuestions是一个广泛使用的开放域QA数据集。我们发现，LM在处理不太常见的事实性知识时很吃力，而检索增强在这些情况下有很大帮助。另一方面，扩展主要提高了对常见知识的记忆，而对尾部事实性知识的记忆则没有明显的改善。基于这些发现，我们设计了一种新的检索增强方法，通过只在必要时检索非参数记忆来提高性能并降低推理成本。

    **关键词**：开放领域QA；检索增强

1. ***[Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations](https://aclanthology.org/2023.acl-long.632.pdf)***         

    **摘要**：上下文学习（ICL）是使大型语言模型（LLM）适应新任务的一个重要范例，但人们对ICL的泛化行为仍然知之甚少。我们从特征偏差的角度研究了ICL的归纳偏差：在一组未指定的演示中，两个特征对标签的预测性相同，那么ICL更倾向于使用哪个特征。首先，我们通过从一系列NLP数据集和特征组合中构建未指定演示来描述GPT-3模型的特征偏差。我们发现LLMs表现出明显的特征偏差——例如，LLMs表现出强烈的偏向于根据情感而非浅层词汇特征（如标点符号）预测标签。其次，我们评估了不同干预措施的效果，这些措施旨在施加有利于特定特征的归纳偏差，例如添加自然语言指令或使用语义相关的标签词。我们发现，尽管许多干预措施可以影响学习者偏好某一特定特征，但很难克服强烈的先验偏见。总之，我们的研究结果为ICL更有可能利用的特征类型以及如何施加更符合预期任务的归纳偏差提供了更广阔的视野。

    **关键词**：上下文学习归纳偏差

2. ***[Unified Demonstration Retriever for In-Context Learning](https://aclanthology.org/2023.acl-long.256.pdf)***         

    **摘要**：上下文学习是一种新的学习范式，即语言模型以一些输入-输出对（示范）和测试输入为条件，并直接输出预测结果。它对所提供的示例非常敏感，因此促进了示例检索的研究：给定测试输入，从训练集中检索相关示例，作为上下文学习的信息示例。以往的研究针对多个任务分别训练特定任务的检索器，但这些方法很难在不同任务上移植和扩展，而且单独训练的检索器会造成大量的参数存储和部署成本。在本文中，我们提出了统一演示检索器（Unified Demonstration Retriever，UDR），它是一种能够检索多种任务演示的单一模型。为了训练UDR，我们通过语言模型的反馈将各种任务的训练信号转换成统一的列表排序表。然后，我们提出了一种多任务列表式排序训练框架，通过迭代挖掘策略找到高质量的候选模型，从而帮助UDR充分吸收各种任务信号。在13个任务族和多个数据域的30多个任务上的实验表明，UDR的性能明显优于基线。进一步的分析表明，在不同的LMs（1.3B ~ 175B）、未见数据集、不同的演示量等各种场景下，UDR的每个建议组件和UDR的强大能力都是有效的。我们将在审查后发布代码和模型检查点。

    **关键词**：上下文学习；统一演示检索器

3. ***[RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models](https://aclanthology.org/2023.acl-long.148.pdf)***         

    **摘要**：为了更好地支持信息检索任务，如网络搜索和开放域问题解答，越来越多的人致力于开发面向检索的语言模型，如RetroMAE等。现有的大多数工作都集中在提高[CLS]标记的上下文化嵌入的语义表示能力上。然而，最近的研究表明，[CLS]之外的普通标记可能提供额外的信息，有助于产生更好的表示效果。因此，有必要对现有方法进行扩展，使所有的上下文化嵌入都可以针对检索任务进行联合预训练。在这项工作中，我们提出了一种新颖的预训练方法，称为双向掩码自动编码器（Duplex Masked Auto-Encoder, a.k.a.DupMAE）。该方法旨在提高语义表示的质量，预训练模型的所有上下文嵌入都可以被利用。它利用了两个互补的自动编码任务：一个任务是在[CLS]嵌入的基础上重建输入句子；另一个任务是在普通词组嵌入的基础上预测输入句子的词袋特征。这两个任务共同训练出一个统一的编码器，整个上下文化的嵌入以一种紧凑的方式聚合在一起，产生最终的语义表示。DupMAE很简单，但在经验上却很有竞争力：它大大提高了预训练模型的表示能力和可移植性，在MS MARCO和BEIR等流行基准上实现了卓越的检索性能。我们在<https://github.com/staoxiao/RetroMAE>上公开了我们的代码。

    **关键词**：信息检索；双向掩码自动编码器

4. ***[ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models](https://aclanthology.org/2023.acl-short.97.pdf)***        

    **摘要**：知识蒸馏（KD）是在低延迟环境中部署大规模预训练语言模型的最有效方法之一，它将大规模模型中包含的知识转移到较小的学生模型中。之前的KD方法仅使用教师生成的软标签和中间激活将知识转移到学生模型参数中。在本文中，我们展示了以包含教师软标签和预测的知识库形式访问非参数记忆可以进一步提高学生的泛化能力。为了使学生能够有效地从知识库中进行检索，我们提出了一种新的框架和损失函数，以保留教师和学生训练示例的语义相似性。我们通过大量实验表明，我们的检索机制可以在GLUE基准上实现最先进的任务特定知识提炼性能。

    **关键词**：知识蒸馏；检索增强

5. ***[Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions](https://aclanthology.org/2023.acl-long.34.pdf)***        

    **摘要**：大型语言模型（LLM）可用于生成文本数据，以训练和评估其他模型。然而，使用LLMs创建高质量的数据集可能具有挑战性。在这项工作中，我们探索了人类与人工智能的合作，以促进基于LLM的文本数据生成的高多样性和高准确性。我们首先研究了两种文本生成多样化的方法：1）logit抑制，这种方法最大限度地减少了已经频繁生成的语言的生成；2）温度采样，这种方法使标记采样概率扁平化。我们发现，多样化方法可以增加数据的多样性，但往往以数据的准确性（即文本和标签适合目标领域）为代价。为了解决这个问题，我们研究了两种人工干预方法：1）标签替换（LR），纠正错误对齐的标签；2）范围外过滤（OOSF），去除用户感兴趣的领域之外的实例或没有考虑过的标签适用的实例。通过oracle研究，我们发现LR将使用多样化数据集训练的模型的绝对准确率提高了14.4%。此外，我们还发现，使用LR干预生成的数据训练的一些模型的表现优于基于LLM的少量分类。相比之下，OOSF在提高模型准确率方面效果不佳，这意味着未来需要在人工在环文本数据生成方面开展工作。

    **关键词**：文本数据生成；数据多样性；人工干预

6. ***[Prompting PaLM for Translation: Assessing Strategies and Performance](https://aclanthology.org/2023.acl-long.859.pdf)***        

    **摘要**：在多语言而非平行文本上训练过的大型语言模型（LLMs）表现出卓越的语言间翻译能力。我们通过对路径语言模型（PaLM）的深入研究来探究这种能力，迄今为止，该模型在类似训练的LLM中表现出了最强的机器翻译（MT）性能。我们研究了为少量提示选择翻译示例的各种策略，得出的结论是示例质量是最重要的因素。通过使用优化的提示，我们用最新的测试集、现代MT指标和人工评估重温了之前对PaLM的MT能力的评估，发现其性能虽然令人印象深刻，但仍然落后于最先进的监督系统。最后，我们对PaLM的MT输出进行了分析，揭示了一些有趣的特性和未来工作的前景。

    **关键词**：机器翻译

7. ***[Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models](https://aclanthology.org/2023.acl-short.45.pdf)***        

    **摘要**：用检索机制增强语言模型已被证明可以显著提高其性能，同时保持较低的参数数量。检索增强模型通常依赖于一种语义检索机制，该机制基于查询块的密集表示与潜在邻域之间的相似性。在本文中，我们对最先进的Retro模型进行了研究，发现其性能增益能够更好地通过表面级别的相似性（如标记重叠）来解释。受此启发，我们用基于BM25的表层方法取代了Retro中的语义检索，从而显著降低了复杂度。由于完整的BM25检索对于大型数据集来说计算成本较高，我们也将其应用于重新排序的场景中，以最小的计算开销获得了部分复杂度的降低。

    **关键词**：检索增强

8. ***[Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment](https://aclanthology.org/2023.acl-long.346.pdf)***        

    **摘要**：上下文学习（ICL）是指大型语言模型能够根据少量标注样本推断测试标签，而无需任何梯度更新。支持ICL的大型语言模型为在低资源环境下绕过重复标注成本提供了一个很好的解决方案。然而，过去只有少数研究在跨语言环境中探索了ICL，在这种环境中，将标签知识从高资源语言转移到低资源语言是非常关键的。为了弥补这一空白，我们首次对跨语言文本分类中的ICL进行了深入分析。我们发现，在跨语言ICL的情况下，选择随机输入标签对来构建提示上下文的普遍模式受到严重限制，这主要是由于输入和输出空间缺乏对齐。为了缓解这一问题，我们提出了一种新颖的提示构建策略--跨语言上下文源目标对齐（X-InSTA）。通过在输入示例的语义中注入一致性，以及基于任务的源语言和目标语言对齐，X-InSTA能够在使用44个不同的跨语言对的三个不同任务中，以较大的优势优于随机提示选择。

    **关键词**：上下文学习；跨语言

9. ***[Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models](https://aclanthology.org/2023.acl-long.828.pdf)***        

    **摘要**：时间推理至关重要。许多事实都与时间有关。例如，运动员会不时更换队伍，不同的政府官员会定期选举。以往与时间相关的问题解答（QA）数据集往往在时间跨度或问题类型的覆盖上存在偏差。在本文中，我们引入了一个全面的探测数据集TempReason来评估大型语言模型的时间推理能力。我们的数据集包括三个时间推理级别的问题。此外，我们还提出了一种基于时间跨度提取和时敏强化学习的新型学习框架来提高大型语言模型的时间推理能力。我们在闭卷质检、开卷质检和推理质检中进行了实验，证明了我们方法的有效性。

    **关键词**：时间推理；数据集；新型学习框架

10. ***[MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies](https://aclanthology.org/2023.acl-long.502.pdf)***        

    **摘要**：自回归语言模型是通过最小化模型分布Q相对于数据分布P的交叉熵来训练的，也就是最小化前向交叉熵，相当于最大似然估计 (MLE)。我们观察到，以这种方式训练的模型可能会“过度泛化”，即产生非人类文本。此外，我们认为反向交叉熵（即P相对于Q的交叉熵）能够更好地反映人类如何评价模型生成的文本。因此，我们建议使用 MixCE 进行学习，这是一种混合正向和反向交叉熵的目标。我们在合成数据设置（其中P是已知的）和真实数据上评估了使用该目标训练的模型，结果表明所生成的模型无需复杂的解码策略即可生成更好的文本。

    **关键词**：正反向混合交叉熵

11. ***[Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale](https://aclanthology.org/2023.acl-long.660.pdf)***        

    **摘要**：通过上下文学习范式，语言模型在各种任务中的表现随着规模的扩大而变得更好。在本文中，我们研究了这样一个假设：大型语言模型在上下文学习中执行任务的能力并不是均匀分布在其所有底层组件上的。通过使用一个660亿参数的语言模型（OPT-66B）来完成14个不同的下游任务，我们发现情况确实如此：约70%的注意头和约20%的前馈网络可以被移除，而任务性能的下降却很小。我们发现在不同的任务和情境中例子的数量中，对上下文学习（不）重要的注意头的集合有很大的重叠。我们还通过任务无关的视角探讨了我们的假设，发现OPT-66B中的一小部分注意头在执行与上下文学习相关的原始归纳操作（即前缀匹配和复制）的能力上得分很高。这些诱导头与特定任务的重要头重叠，加强了Olsson等人（2022年）关于诱导头通用于与情境学习相关的更复杂行为的论点。总之，我们的研究提供了一些见解，表明大型语言模型可能在上下文学习方面训练不足，并提出了如何预先训练语言模型以更有效地进行上下文学习的问题。

    **关键词**：上下文学习

12. ***[LAMBADA: Backward Chaining for Automated Reasoning in Natural Language](https://aclanthology.org/2023.acl-long.361.pdf)***        

    **摘要**：通过使用大语言模型（LLMs）以及思维链提示和选择推理等方法，自然文本的自动推理已经取得了显著的进展。这些技术在从公理到结论的正向方向上搜索证明，但搜索空间会出现组合爆炸，因此在需要较长推理链的问题上失败率较高。经典的自动推理文献表明，逆向推理（即从预期结论到支持公理）的求证效率要高得多。将这一直觉引入到LM环境中，我们开发了一种称为LAMBADA的后向链算法，它将推理分解为四个子模块，这些子模块可以简单地通过少量提示的LLM推理来实现。我们表明，在两个具有挑战性的逻辑推理数据集上，LAMBADA比最先进的前向推理方法实现了可观的准确性提升，尤其是在需要深入和准确的证明链时。

    **关键词**：思维链；逆向推理

13. ***[Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models](https://aclanthology.org/2023.acl-long.410.pdf)***        

    **摘要**：生成式大语言模型（LLMs）的广泛适用性和适应性使其得以迅速应用。虽然预先训练的模型可以执行许多任务，但这些模型通常会进行微调，以提高其在各种下游应用中的性能。然而，这导致了违反模型许可、模型盗窃和版权侵犯等问题。此外，最近的进展表明，生成技术能够产生有害内容，这加剧了模型供应链中的责任问题。因此，我们需要一种方法来调查一个模型是如何训练的，或者一段文本是如何生成的，以及它们的预训练基础模型是什么。在本文中，我们迈出了解决这一开放性问题的第一步，即从给定的微调LLM追溯到其相应的预训练基础模型。我们考虑了不同的知识水平和归因策略，发现用我们的最佳方法可以正确地回溯10个微调模型中的8个。

    **关键词**：LLMs责任问题；回溯

14. ***[Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework](https://aclanthology.org/2023.acl-long.320.pdf)***        

    **摘要**：随着大型语言模型（LLMs）成为NLP的标准，在生成和推理任务中表现出良好的性能，其最致命的缺点之一是缺乏事实正确性。生成不符合事实的文本不仅会降低性能，还会降低其应用的可信度和有效性。思维链（CoT）提示通过生成可解释的推理链提高了复杂推理任务中的信任度和模型性能，但在知识密集型任务中仍然存在事实性问题。在本文中，我们提出了用于CoT提示的“验证与编辑”（Verify-and-Edit）框架，旨在通过根据外部知识对推理链进行后期编辑来提高预测的真实性。在GPT-3的基础上，我们的框架提高了多个开放域问题解答任务的准确性。

    **关键词**：事实性问题；思维链；prompt框架

15. ***[Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step](https://aclanthology.org/2023.acl-long.150.pdf)***        

    **摘要**：思维链提示（例如，“让我们逐步思考”）促使大型语言模型对其预测进行口头合理化。虽然思维链可以带来显著的性能提升，但只有足够大的模型（超过50B个参数）才会出现这种优势。我们的研究表明，数量级更小的模型（1.25亿-1.3亿个参数）仍然可以从思维链提示中获益。为了实现这一目标，我们引入了符号思维链蒸馏（SCoTD），这是一种从更大的教师模型中抽取合理化信息来训练更小的学生模型的方法。多个常识性基准的实验表明1）SCoTD提高了学生模型在有监督和少数人参与的设置中的性能，尤其是在挑战集中；2）从教师模型中抽样每个实例的许多推理链是最重要的；3）蒸馏后，尽管参数数量级较少，但人类判断学生的思维链与教师模型相当。我们测试了关于思维链样本的重要属性的几个假设，例如多样性与教师可能性与开放性。我们发布了我们的思维链样本语料库和代码。

    **关键词**：思维链；符号思维链蒸馏

16. ***[Pre-trained Language Models Can be Fully Zero-Shot Learners](https://aclanthology.org/2023.acl-long.869.pdf)***        

    **摘要**：我们如何在没有标注数据或额外的非标注数据的情况下将预训练模型扩展到许多语言理解任务中？预训练语言模型（PLMs）已经在广泛的NLP任务中发挥了作用。然而，现有的方法要么需要在下游标注数据集上进行微调，要么需要手动构建适当的提示。在本文中，我们提出了非参数提示PLM（NPPrompt），用于完全zero-shot语言理解。与之前的方法不同，NPPrompt仅使用预训练的语言模型，不需要任何标记数据或额外的原始语料库进行进一步微调，也不依赖于人工构建一套完整的提示标记词。我们评估了NPPrompt与之前主要的“zero-shot学习”和“few-shot学习”方法在不同NLP任务上的效果，包括文本分类、文本引申、相似文本检索、仿写和多选题回答。实验结果表明，我们的NPPrompt在文本分类准确率和GLUE基准准确率上分别提高了12.8%和15.6%，远远超过了之前最好的完全zero-shot学习方法。我们的源代码可在<https://github.com/XuandongZhao/NPPrompt>获取。

    **关键词**：非参数提示

17. ***[mCLIP: Multilingual CLIP via Cross-lingual Transfer](https://aclanthology.org/2023.acl-long.728.pdf)***        

    **摘要**：像CLIP这样的大规模视觉语言预训练（VLP）模型在各种下游跨模态任务中表现出了卓越的性能。然而，由于缺乏足够的非英语图像-文本对，这些模型通常偏重于英语。现有的多语言VLP方法通常通过翻译增强非英语图像-文本对来学习检索效率低下的单流模型。在本文中，我们介绍了一种检索效率高的双流多语VLP模型mCLIP，它是通过一种新颖的三角跨模态知识蒸馏（TriKD）方法将CLIP模型和多语文本编码器（MTE）对齐而训练出来的。由于在蒸馏过程中只更新其顶部的两个光投影器，因此该方法具有参数效率高的特点。此外，为了增强MTE的标记和句子级多语言表征，我们建议在TriKD之前对其进行机器翻译和对比学习联合训练，以提供更好的初始化。实证结果表明，mCLIP在零检和微调多语图像-文本检索任务中都取得了新的一流性能。

    **关键词**：多语言；CLIP

## Main-Poster

1. ***[Is GPT-3 a Good Data Annotator?](https://aclanthology.org/2023.acl-long.626.pdf)***

    **摘要**：数据标注是对可用于训练机器学习模型的数据进行标注的过程。高质量的标注至关重要，因为它能让模型学习输入数据与所需输出之间的关系。GPT-3 是 OpenAI 开发的一个大型语言模型，在各种 NLP 任务中都表现出了令人印象深刻的zero-shot和few-shot性能。因此，我们很自然地想知道它是否能用于为 NLP 任务有效地标注数据。在本文中，我们将 GPT-3 与传统的数据注释方法进行了比较，并分析了它在一系列任务中的输出结果，从而评估了 GPT-3 作为数据标注器的性能。通过分析，我们希望深入了解 GPT-3 作为 NLP 通用数据标注器的潜力。

    **关键词**：自动数据标注；评估

2. ***[Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge](https://aclanthology.org/2023.acl-long.300.pdf)***        

    **摘要**：预训练的语言模型（LMs）被用于知识密集型任务，如问题回答，但随着世界的变化，他们的知识会不断过时。之前的工作研究了对语言模型的定向更新，注入个别事实并评估模型是否学习了这些事实，同时不改变对其他背景的预测。我们向前迈进了一步，研究LM根据注入的事实（或传播这些事实）进行推断的能力：例如，在得知某物是一个电视节目后，LM是否预测你可以观看它？我们用两个cloze-style任务来研究这个问题：一个是现有的关于新实体的真实世界句子的数据集（ECBD），另一个是一个新的受控基准，其中人工设计的模板要求对注入的知识进行不同程度的推理。令人惊讶的是，我们发现现有的更新知识的方法（基于梯度的微调和对这种方法的修改）几乎没有显示出注入知识的传播。这些方法只有在注入的事实和目标推论之间存在词汇重叠的情况下，才会提高cloze实例的性能。然而，在LM的上下文中预置实体定义可以提高所有设置的性能，这表明参数更新方法在知识注入方面还有很大的空间。

    **关键词**：知识注入

3. ***[Should you marginalize over possible tokenizations?](https://aclanthology.org/2023.acl-short.1.pdf)***        

    **摘要**：自回归语言模型（LMs）将标记序列映射为概率。计算任何字符串（如英语句子）的概率的通常做法是，首先将其转化为由模型打分的标记序列。然而，代表任何给定字符串的令牌序列有指数级的数量。为了真正计算一个字符串的概率，应该对所有标记化进行边际化处理，这通常是难以做到的。在这里，我们分析了忽略边际化的做法是否合理。为此，我们设计了一种基于重要性抽样的算法，使我们能够计算出边际概率的估计值，并将其与一系列最先进的模型和数据集中的默认程序进行比较。我们的结果表明，在大多数情况下，对数可能性的差距不超过0.5％，但对于具有长的复杂词汇的数据，它变得更加明显。

    **关键词**：边际化；重要性抽样算法

4. ***[Let Me Check the Examples: Enhancing Demonstration Learning via Explicit Imitation](https://aclanthology.org/2023.acl-short.93.pdf)***        

    **摘要**：演示学习的目的是通过在few-shot设置中提供回答的演示来指导提示预测。尽管取得了可喜的成果，但现有的工作只是将回答过的例子作为演示连接到提示模板（包括原始上下文），而没有任何额外的操作，忽略了提示-演示的依赖关系。此外，先前的研究发现，随机替换演示的标签会略微损害性能，说明模型不能正确学习演示带来的知识。受人类学习过程的启发，在本文中，我们引入了模仿演示学习（Imitation DEMOnstration learning），通过明确地模仿人类的审查行为来加强演示学习，其中包括：(1)对比性学习机制，集中学习相似的演示。(2)演示-标签再预测方法，巩固已知知识。实验结果表明，我们提出的方法在14个分类语料库中的5个取得了最先进的性能。进一步的研究还证明，Imitation-Demo加强了提示和演示之间的关联，这可以为探索演示学习的工作方式提供基础。

    **关键词**：演示学习；模仿演示学习（Imitation-Demo）

5. ***[HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation](https://aclanthology.org/2023.acl-long.475.pdf)***        

    **摘要**：为了充分发挥大规模预训练语言模型（PLMs）在下游任务中的优势，对PLMs的全部参数进行微调已经成为一种普遍的适应范式。然而，由于PLM的参数规模较大，在数据稀缺和资源有限的情况下，这种模式带来了更新效率低下和微调资源过度消耗的问题。为了缓解这些问题，在本文中，我们提出了一种参数高效的微调方法HiFi，即只对特定任务的高信息量和强相关的注意力头进行微调。为了寻找那些重要的注意力头，我们开发了一个新的框架来分析头的有效性。具体来说，我们首先从信息丰富度和相关性两个角度将各头之间的关系建模为一个图，然后应用PageRank算法来确定各头的相对重要性。在GLUE基准上进行的大量实验证明了我们方法的有效性，并表明HiFi获得了比先前基准更先进的性能。

    **关键词**：参数高效微调方法；信息丰富度与相关性

6. ***[Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method](https://aclanthology.org/2023.acl-long.482.pdf)***        

    **摘要**：自动摘要可生成包含源文件关键观点的简明摘要。作为新闻子领域最主流的数据集，CNN/DailyMail 和 BBC XSum 已被广泛用于性能基准测试。然而，这些数据集的参考摘要存在一定的噪声，主要表现在事实幻觉和信息冗余方面。为了应对这一挑战，我们首先按照拉斯韦尔提出的 "拉斯韦尔传播模型"，注释了新的专家写作要素感知测试集，使参考摘要能够客观、全面地关注更细粒度的新闻要素。利用新的测试集，我们观察到了 LLMs 令人惊讶的零样本摘要能力，这解决了之前工作中 LLMs 零样本摘要的人工偏好和自动评估指标结果不一致的问题。此外，我们还提出了一种 "思维链摘要"（SumCoT）技术，诱导 LLM 逐步生成摘要，帮助他们将源文件中更多细粒度的细节整合到最终摘要中，从而与人类的写作思维相关联。实验结果表明，在两个数据集上，我们的方法在 ROUGE-L 中分别以 +4.33/+4.77 的成绩优于最先进的微调 PLM 和零样本 LLM。数据集和代码可通过 https://github.com/Alsace08/SumCoT 公开获取。

    **关键词**：自动摘要；摘要思维链

7. ***[A Length-Extrapolatable Transformer](https://aclanthology.org/2023.acl-long.816.pdf)***        

    **摘要**：位置建模在Transformers中起着关键作用。在本文中，我们专注于长度外推，即在评估较长序列的同时对短文进行训练。我们定义注意力分辨率作为外推的指标。然后，我们提出两个设计来改善Transformers的上述指标。具体来说，我们引入了一个相对位置嵌入来明确地最大化注意力分辨率。此外，我们在推理过程中使用顺时针方向的因果注意来提高分辨率。我们用语言建模评估了不同的Transformer变体。实验结果表明，我们的模型在插值和外推设置中都取得了强大的性能。代码将在<https://aka.ms/LeX-Transformer>上提供。

    **关键词**：长度外推

8. ***[Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters](https://aclanthology.org/2023.acl-long.153.pdf)***        

    **摘要**：思维链（CoT）提示可以极大地提高大型语言模型（LLM）的多步骤推理能力。CoT通过在演示中提供一系列的推理步骤，明确地鼓励LLM为解决一个问题而产生中间理由。尽管它很成功，但人们对是什么使CoT提示有效，以及演示的推理步骤的哪些方面有助于它的表现仍然了解甚少。在本文中，我们表明CoT推理即使在无效的演示中也是可能的——在各种指标下，用无效的推理步骤进行提示可以达到使用CoT获得的性能的80-90％以上，同时在推理过程中仍然会产生连贯的推理线。进一步的实验表明，理由的其他方面，如与查询相关和正确的推理步骤排序，对于有效的CoT推理来说更为重要。总的来说，这些发现既加深了我们对CoT提示的理解，也为LLMs学习上下文推理的能力提出了新问题。

    **关键词**：思维链；上下文推理

9.  ***[Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers](https://aclanthology.org/2023.acl-long.724.pdf)***        

    **摘要**：本文探讨了模型产生的信号在改善文本到文本Transformer（如T5）的zero-shot泛化方面的有效性。我们研究了各种设计，利用辅助模型对T5进行预训练，为主模型去噪构建更具挑战性的标记替代物。研究的关键方面包括解码目标、RTD头的位置和掩码模式。基于这些研究，我们开发了一个新的模型，METRO-T0，它使用重新设计的ELECTRA-Style预训练策略进行预训练，然后在混合的NLP任务上进行prompt-finetuned。METRO-T0在提示的NLP基准上的表现优于所有类似规模的基准，如：T0 Eval和MMLU，并且仅用8%的参数就能与最先进的T0-11B模型相媲美。我们对模型的神经激活和参数敏感性的分析表明，METRO-T0的有效性源于参数的更均衡贡献和对其能力的更好利用。代码和模型检查点在[https://github.com/gonglinyuan/metro\_t0](https://github.com/gonglinyuan/metro\_t0)。

    **关键词**：预训练模型

10. ***[Benchmarking Large Language Model Capabilities for Conditional Generation](https://aclanthology.org/2023.acl-long.511.pdf)***        

    **摘要**：预训练的大型语言模型（PLMs）是自然语言处理中大多数新发展的基础。它们已经将该领域从特定应用的模型管道转移到适应各种任务的单一模型上。像GPT-3或PaLM这样的自回归PLM以及像few-shot learning这样的相关技术，还将输出模式转移到了生成而不是分类或回归。尽管它们的使用无处不在，但当这些模型被引入时，语言模型的生成质量很少被评估。此外，目前还不清楚现有的生成任务——虽然它们可以用来在高水平上比较系统——与人们一直在采用它们的真实世界的用例有什么关系。在这项工作中，我们讨论了如何使现有的特定应用的生成基准适用于PLM，并对PLM在自然语言生成任务中的局限性和能力进行了深入的实证研究，如规模、架构、输入和输出语言等方面。我们的研究结果表明，PLM在对不同数据制度的适用性和对多种语言的通用性方面有所不同。它们进一步告诉实践者，在给定的生成任务设置中应该使用哪种PLMs。我们分享了在开发新的PLM过程中对生成能力进行基准测试时应考虑的最佳做法。

    **关键词**：基准测试

11. ***[Parallel Context Windows for Large Language Models](https://aclanthology.org/2023.acl-long.352.pdf)***        

    **摘要**：当应用于处理长文本时，大型语言模型（LLMs）受到其上下文窗口的限制。现有的解决这一限制的努力涉及训练专门的架构，不容易应用于现成的LLM。我们提出了平行上下文窗口（PCW），这是一种无需进一步训练就能缓解任何现成的LLM的上下文窗口限制的方法。该方法的关键是将一个长的上下文分割成几块（"窗口"），限制注意力机制只在每个窗口内应用，并在各窗口内重新使用位置嵌入。我们的主要结果测试了PCW方法在上下文学习中的作用，模型的规模在7.5亿到1780亿个参数之间，并显示出对具有不同输入和输出空间的任务的实质性改进。我们在长上下文窗口可能有益的其他环境中显示了额外的好处：多跳问题和具有多个检索文件的检索增强型问题回答。我们的结果强调平行上下文窗口是一种有前景的方法，可以在一系列需要长文本序列的环境中应用现成的LLMs。我们在<https://github.com/ai21labs/parallel-context-windows>上公开了我们的代码。

    **关键词**：缓解上下文限制；平行上下文窗口

12. ***[WebCPM: Interactive Web Search for Chinese Long-form Question Answering](https://aclanthology.org/2023.acl-long.499.pdf)***        

    **摘要**：长式问题回答（LFQA）旨在用详细的、长达一段的回答来回答复杂的、开放式的问题。LFQA的事实范式需要两个过程：信息检索，即搜索相关的支持事实，以及信息综合，即把这些事实整合成一个连贯的答案。在本文中，我们介绍了WebCPM，这是第一个中文LFQA数据集。WebCPM的一个独特的特点是它的信息检索是基于交互式网络搜索的，它与搜索引擎实时接触。继WebGPT之后，我们开发了一个网络搜索界面。我们招募注释者，使用我们的界面搜索相关信息，然后回答问题。同时，我们的注释者的网络搜索行为会被记录下来。我们总共收集了5500个高质量的问题-答案对，以及15372个支持性事实和125954个网络搜索行为。我们对预先训练好的语言模型进行微调，以模仿人类的网络搜索行为，并根据收集的事实生成答案。我们的LFQA管道建立在这些微调的模型上，在我们的数据集和DuReader上，分别有32.5％和47.5％的情况下产生的答案不比人类写的差。该界面、数据集和代码在https://github.com/thunlp/WebCPM。

    **关键词**：LFQA；中文数据集；模仿人类搜索行为

13. ***[Ellipsis-Dependent Reasoning: a New Challenge for Large Language Models](https://aclanthology.org/2023.acl-short.4.pdf)***        

    **摘要**：我们为大型语言模型提出了一个新的挑战：依赖省略号的推理。我们定义了几种配对例子的结构，其中一个省略号的例子与它的非省略号的对应例子相匹配，并提出了一个需要解决省略号的问题。测试结果显示，最好的模型在非省略号的例子上表现良好，但在除最简单的省略号结构之外的所有例子上都很困难。

    **关键词**：依赖省略号的推理

14. ***[In-Context Analogical Reasoning with Pre-Trained Language Models](https://aclanthology.org/2023.acl-long.109.pdf)***        

    **摘要**：类比推理是人类认知的一种基本能力，它使我们能够通过将新情况与过去的经验联系起来对其进行抽象推理。虽然它被认为是人工智能系统稳健推理的关键，但传统的方法需要大量的训练和/或硬编码领域知识才能应用到基准任务中。受认知科学研究的启发，我们探索了使用基于语言的直观抽象来支持人工智能系统的类推。具体来说，我们将大型预训练的语言模型（PLMs）应用于视觉上的瑞文渐进矩阵（RPM），这是一个常见的关系推理测试。通过简单地将问题的感知特征编码为语言形式，我们发现PLMs表现出惊人的zero-shot关系推理能力，超过了人类的表现，接近于基于视觉的监督方法。我们探索了不同的编码，这些编码对任务特征的抽象程度不同，发现更高层次的抽象会进一步加强PLM的类比推理。我们的详细分析揭示了模型的复杂性、语境中的学习以及解决RPM任务的先验知识的作用。

    **关键词**：类比推理；领域知识硬编码

15. ***[Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis](https://aclanthology.org/2023.acl-long.756.pdf)***        

    **摘要**：长度外推法允许在短序列上训练转化器语言模型，在对更长的序列进行测试时，可以保留困惑。一种相对位置嵌入设计，即ALiBi，迄今已得到最广泛的应用。我们通过接受域分析的视角来剖析ALiBi，该分析由一个新的累积归一化梯度工具授权。接受场的概念进一步使我们能够修改虚无的Sinusoidal位置嵌入，创造出Sandwich，这是第一个无参数的相对位置嵌入设计，真正的长度信息使用比训练序列长。Sandwich与KERPLE和T5共享相同的对数衰减的时间偏差模式，具有可学习的相对位置嵌入；这些阐明了未来可推断的位置嵌入设计。

    **关键词**：长度外推；相对位置嵌入设计

16. ***[Training Trajectories of Language Models Across Scales](https://aclanthology.org/2023.acl-long.767.pdf)***        

    **摘要**：扩大语言模型的规模带来了前所未有的性能提升，但人们对模型变大后训练的动态变化了解甚少。不同规模的语言模型在预训练中是如何学习的？为什么更大的语言模型会表现出更理想的行为？在本文中，我们分析了不同规模的OPT模型（Zhang等人，2022）的中间训练检查点——从125M到175B的参数——下一个标记预测、序列级生成和下游任务。我们发现：1）在给定的困惑和独立于模型大小的情况下，类似的训练标记子集看到了最显著的损失减少，其余的停滞不前或表现出双倍的下降行为（Nakkiran等人、2020）；2）在训练的早期，所有的模型都学会减少包含幻觉的语法序列的困惑，小的模型在这种次优分布中停止，大的模型最终学会给这些序列分配较低的概率；3）困惑是BIG-Bench的74个多选任务中上下文学习表现的有力预测因素，这与模型大小无关。总之，这些结果表明，与模型规模或训练计算相比，困惑度对模型行为的预测作用更大。

    **关键词**：不同规模模型训练轨迹

17. ***[Distilling Script Knowledge from Large Language Models for Constrained Language Planning](https://aclanthology.org/2023.acl-long.236.pdf)***        

    **摘要**：在日常生活中，人类经常通过遵循以目标为导向的脚本形式的分步指示来计划他们的行动。以前的工作利用语言模型（LMs）来计划定型活动的抽象目标（例如，"做一个蛋糕"），但对具有多面约束的更具体的目标（例如，"为糖尿病患者做一个蛋糕"）却没有研究。在本文中，我们首次定义了约束性语言规划的任务。我们提出了一种 "先生成后过滤 "的方法来改进这个任务的大型语言模型（LLMs），并使用它来提炼出一个新的约束性语言规划数据集--Coscript，它由55000个脚本组成。实证结果表明，我们的方法明显提高了大型语言模型的约束性语言规划能力，尤其是在约束的忠实度方面。此外，Coscript被证明在赋予较小的LM以约束性语言规划能力方面相当有效。

    **关键词**：蒸馏；脚本数据集；约束性语言规划能力

18. ***[MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning](https://aclanthology.org/2023.acl-long.641.pdf)***        

    **摘要**：指令微调是一种新的学习范式，在通过指令指定的任务上对预先训练好的语言模型进行微调，在各种自然语言处理任务上显示出有希望的zeor-shot性能。然而，它还没有在视觉和多模态任务中得到探索。在这项工作中，我们介绍了MultiInstruct，这是第一个多模态指令微调基准数据集，由62个不同的多模态任务组成，以统一的序列对序列格式，涵盖10大类。这些任务来自21个现有的开源数据集，每个任务都配备了5个专家编写的指令。我们将OFA作为多模态指令调整的基础预训练模型，为了进一步提高其zero-shot性能，我们探索了多种转移学习策略，以利用大规模的自然指令数据集。实验结果表明，在各种未见过的多模态任务上有很强的zero-shot性能，以及从纯文本指令数据集迁移学习的好处。我们还设计了一个新的评估指标——敏感性，以评估该模型对各种指令的敏感程度。我们的结果表明，在一组不同的任务和指令上对模型进行微调，可以降低对每个任务的指令变化的敏感性。

    **关键词**：指令微调；多模态；数据集；评估指标

19. ***[Black-box language model explanation by context length probing](https://aclanthology.org/2023.acl-short.92.pdf)***        

    **摘要**：大型语言模型越来越广泛地被采用，凸显了提高其可解释性的必要性。我们提出了*语境长度探测*，这是一种新的因果语言模型的解释技术，基于跟踪模型的预测作为可用语境长度的函数，并允许为不同的语境分配*不同的重要性分数*。该技术与模型无关，除了计算标记级的概率外，不依赖对模型内部的访问。我们将上下文长度探测法应用于大型预训练语言模型，并提供一些初步的分析和见解，包括研究长程依赖的潜力。该方法的[源代码](https://github.com/cifkao/context-probing/)和[互动演示](https://cifkao.github.io/context-probing/)可供查阅。

    **关键词**：可解释性研究

20. ***[Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models](https://aclanthology.org/2023.acl-long.658.pdf)***        

    **摘要**：在许多任务设置中，文本分类模型可能会遇到它们无法正确预测的新类别的例子。选择性预测，即模型放弃对低置信的例子的预测，提供了一个可能的解决方案，但现有的模型往往对未见过的类别过于自信。为了弥补这种过度自信，我们引入了对比性新颖性增强学习（Contrastive Novelty-Augmented Learning，CoNAL），这是一种分两步走的方法，生成代表新颖类的OOD例子，然后进行训练以降低对它们的置信。首先，我们通过两次提示大型语言模型来生成OOD例子：我们提示它列举相关的新类，然后从每个新类中生成符合任务格式的例子。其次，我们用一个新的对比目标来训练分类器，鼓励对生成的OOD例子的置信低于训练例子。当用CoNAL训练时，分类器在检测和放弃新类例子的能力上比以前的方法平均提高了2.3％，在准确率-覆盖率曲线（AUAC）下的准确率为5.5％，在4个NLP数据集上的AUROC为5.5％，对分布内准确率没有影响。

    **关键词**：文本分类

21. ***[Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions](https://aclanthology.org/2023.acl-long.562.pdf)***        

    **摘要**：虽然最近人们对研究为什么基于Transformer的大型语言模型会以这样的方式进行预测很感兴趣，但每一层内进行的复杂计算使其行为变得有些不透明。为了减轻这种不透明性，这项工作提出了基于每个初始输入标记的自回归语言模型的最终隐藏状态的线性分解，这对几乎所有当代的Transformer架构都是准确的。这种分解允许定义概率分布，以消除特定输入标记的贡献，这可以用来分析它们对模型概率的影响，在一连串即将到来的单词中，只需从模型中向前传递一次。使用下一个词概率的变化作为重要性的衡量标准，这项工作首先研究了哪些语境词对语言模型预测的贡献最大。回归实验表明，基于Transformer的语言模型在进行下一个词的预测时，主要依赖搭配关联，其次是语言因素，如句法依赖和核心推理关系。此外，使用这些措施来预测句法依赖性和核心词提及跨度的分析表明，搭配关联和同一标记的重复在很大程度上解释了语言模型对这些任务的预测。

    **关键词**：可解释性研究

22. ***[Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations](https://aclanthology.org/2023.acl-long.129.pdf)***        

    **摘要**：虽然大型语言模型可以使用zeor-shot学习和few-shot学习，但在没有示范的情况下，性能会明显下降。在本文中，我们介绍了Z-ICL，一种新的zero-shot学习方法，它通过使用原始文本语料库为给定的测试输入构建伪演示来弥补这一差距。具体来说，伪演示的构建方法是：（1）从语料库中找到与测试输入最近的邻居，并将其与随机任务标签配对；（2）应用一套技术来减少模型从所产生的演示中直接复制的数量。对9个分类数据集的评估表明，Z-ICL比以前的零zero-shot方法要好得多，并且在few-shot的情况下，与带有标签的训练数据的上下文学习相当。总的来说，Z-ICL为一个模型的zero-shot性能水平提供了一个明显更高的估计，并支持未来开发更好的伪演示的努力，进一步提高zero-shot的结果。

    **关键词**：zero-shot方法

23. ***[Sequence Parallelism: Long Sequence Training from System Perspective](https://aclanthology.org/2023.acl-long.134.pdf)***        

    **摘要**：Transformer在各种任务上取得了令人鼓舞的结果。然而，相对于序列长度而言，自注意力存在着二次内存要求。现有的工作侧重于从算法的角度减少时间和空间的复杂度。在这项工作中，我们提出了序列并行，一种内存效率高的并行，以代替从系统角度解决这个问题。我们的方法与大多数现有的并行机制（如数据、管道和张量并行）兼容，这意味着我们的序列并行使4D并行成为可能。更重要的是，我们不再需要一个单一的设备来容纳整个序列。此外，利用具有线性复杂性的高效注意力，我们的序列并行性使我们能够用无限长的序列来训练Transformer。具体来说，我们将输入序列分成多个块，并将每个块送入其相应的设备（即GPU）。为了计算注意力的输出，我们将环形通信与自注意力的计算结合起来，提出了环形自注意力（RSA）。实验表明，序列并行性在随批处理规模和序列长度扩展时表现良好。与张量并行相比，当扩展到64个NVIDIA P100 GPU时，我们的方法分别实现了13.7美元和3.0美元的最大批量大小和序列长度。通过高效关注，序列可以处理超过114K个标记的序列，这比现有的高效关注工作在单个设备上保持整个序列的时间长27美元。

    **关键词**：计算复杂度；序列并行；系统角度

24. ***[Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning](https://aclanthology.org/2023.acl-short.107.pdf)***        

    **摘要**：在各种下游任务中，对预先训练好的大型语言模型进行全参数微调的成本过高。因此，参数高效微调引起了人们的关注，这种微调只对冻结预训练模型的少数特定任务参数进行优化。在这项工作中，我们将重点放在前缀调整上，即仅优化插入Transformer层的连续前缀向量（即伪标记）。基于所学到的语法和语义表征在不同层中有很大差异这一观察，我们认为自适应前缀将比固定前缀更适合每一层，从而使微调更加有效和高效。因此，我们提出了自适应前缀调整（APT），通过门机制在细粒度标记层和粗粒度层两个方面调整前缀。在SuperGLUE和NER数据集上的实验表明了APT的有效性。此外，以门作为探测，我们验证了可变前缀的效率和有效性。

    **关键词**：参数高效微调；自适应前缀调整

25. ***[Knowledge of cultural moral norms in large language models](https://aclanthology.org/2023.acl-long.26.pdf)***        

    **摘要**：不同文化背景下的道德规范各不相同。最近的一项研究表明，英语大型语言模型包含类似人类的道德偏差，但这些研究通常没有考察不同文化背景下的道德变异。我们研究了单语英语模型包含不同国家道德规范知识的程度。我们考虑了两个层面的分析：1）语言模型是否捕捉到了不同国家在诸如“同性恋”和“离婚”等各种话题上的细粒度道德差异；2）语言模型是否捕捉到了文化多样性以及全球各地人们在道德判断上趋于分歧或一致的共同倾向。我们利用世界价值观调查（涵盖55个国家）和PEW全球道德调查（涵盖40个国家）的两个公共数据集进行分析。我们发现，预训练的英语语言模型对各国经验道德规范的预测比之前报告的英语道德规范更差。然而，对调查数据的语言模型进行微调改善了对各国的推断，但代价是对英语道德规范的估计不够准确。我们讨论了将文化知识纳入道德规范自动推断的意义和挑战。

    **关键词**：语言模型道德规范

26. ***[Reasoning with Language Model Prompting: A Survey](https://aclanthology.org/2023.acl-long.294.pdf)***        

    **摘要**：推理作为解决复杂问题的基本能力，可以为医疗诊断、谈判等各种实际应用提供后台支持。本文全面考察了利用语言模型提示进行推理的前沿研究。我们通过比较和总结介绍了相关研究工作，并提供了系统资源以帮助初学者。我们还讨论了出现这种推理能力的潜在原因，并强调了未来的研究方向。资源地址<https://github.com/zjunlp/Prompt4ReasoningPapers>（定期更新）。

    **关键词**：综述；语言模型推理

27. ***[Training-free Neural Architecture Search for RNNs and Transformers](https://aclanthology.org/2023.acl-long.142.pdf)***        

    **摘要**：神经架构搜索(NAS)可以自动创建新的有效的神经网络架构，为人工设计复杂架构的费力过程提供了替代方案。然而，传统的NAS算法速度较慢，需要巨大的计算能力。最近的研究调查了图像分类架构的免训练NAS指标，大大加快了搜索算法的速度。在本文中，我们研究了针对语言建模任务的循环神经网络（RNN）和基于BERT的Transformer架构的免训练NAS指标。首先，我们开发了一种新的免训练度量，名为隐藏协方差，它可以预测RNN架构的训练性能，并显著优于现有的免训练度量。我们在NAS-Bench-NLP基准上对隐藏协方差指标的有效性进行了实验评估。其次，我们发现当前变压器架构的搜索空间范式并未针对免训练神经架构搜索进行优化。相反，简单的定性分析可以有效地将搜索空间缩小到性能最好的架构。这一结论是基于我们对现有的免训练指标和从最近的变压器剪枝文献中开发的新指标的研究，并在我们自己的训练有素的BERT架构基准上进行了评估。最终，我们的分析表明，为了获得有效的结果，必须同时开发架构搜索空间和免训练度量。我们的源代码在<https://github.com/aaronserianni/training-free-nas>。

    **关键词**：神经架构搜索

28. ***[Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](https://aclanthology.org/2023.acl-long.147.pdf)***        

    **摘要**：大语言模型（LLMs）最近在各种NLP任务中表现出了令人印象深刻的性能。为了处理多步骤推理任务，Few-shot-chain-of-thought（CoT）提示包括一些手工制作的分步推理演示，这使得LLM能够明确地生成推理步骤，并提高其推理任务的准确性。为了减少人工操作，Zero-shot-CoT将目标问题陈述与*Let's think step by step*连接起来，作为LLMs的输入提示。尽管Zero-shot-CoT取得了成功，但它仍然存在三个缺陷：计算错误、漏步错误和语义误解错误。为了解决步骤遗漏错误，我们提出了计划与解决（PS）提示。它包括两个部分：首先，设计一个计划，将整个任务划分为更小的子任务，然后根据计划执行子任务。为了解决计算错误并提高推理步骤的质量，我们将PS提示扩展为更详细的指令，并衍生出PS+提示。我们在三个推理问题的十个数据集上评估了我们提出的提示策略。在GPT-3上的实验结果表明，在所有数据集上，我们提出的zero-shot提示策略始终远远优于zero-shot-CoT策略，与Zero-shot-Program-of-Thought提示策略相当或超过后者，并且在数学推理问题上与8-shot-CoT提示策略性能相当。代码见<https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting>。

    **关键词**：思维链；提示策略

29. ***[Exploring Large Language Models for Classical Philology](https://aclanthology.org/2023.acl-long.846.pdf)***        

    **摘要**：近来NLP领域的进步为包括古希腊语和拉丁语在内的许多语言创建了强大的语言模型。虽然之前关于古典语言的工作一致使用BERT，但在这项工作中，我们为古希腊语创建了四个语言模型，这些模型在两个维度上各不相同，以研究它们在古典语言感兴趣的任务中的通用性：我们探索了(i)使用RoBERTa和T5作为强模型类型的纯编码器和编码器-解码器架构，并为它们中的每一种创建了(ii)单语言古希腊语和包括拉丁语和英语在内的多语言实例。我们在形态和句法任务（包括词法化）上对所有模型进行了评估，这证明了T5解码能力的附加价值。我们进一步定义了两个探测任务，以研究在经典文本上预先训练的模型所获得的知识。我们的实验首次对现有的古希腊语模型进行了基准分析。结果表明，与SoTA相比，我们的模型有了明显的改进。对模型类型的系统分析可以为未来设计古希腊语言模型的研究提供信息，包括开发新的生成任务。我们将我们的所有模型作为社区资源提供给大家，同时提供一个大型的古希腊语预训练语料库，以支持为古典语言学建立一个更大的、可比较的模型库。

    **关键词**：古典语言

30. ***[Learning Better Masking for Better Language Model Pre-training](https://aclanthology.org/2023.acl-long.400.pdf)***        

    **摘要**：掩码语言建模（MLM）已被广泛用作预训练语言模型（PrLMs）的去噪目标。现有的PrLMs通常采用随机token掩码策略，即在整个训练过程中采用固定的掩码率，不同的内容以相等的概率被屏蔽。然而，模型可能会受到训练前状态的复杂影响，随着训练时间的推移而发生相应的变化。在本文中，我们证明了这种对掩码比率和掩码内容的时变MLM设置不可能提供最优结果，这促使我们探索时变MLM设置的影响。我们提出了两种预定掩码方法，在不同训练阶段自适应地调整掩码率和掩码内容，从而提高了预训练的效率和在下游任务中验证的有效性。我们的工作是对掩码比和掩码内容的时变掩码策略的一项开创性研究，使我们更好地理解了掩码比和掩码内容如何影响MLM预训练。

    **关键词**：掩码设置方法

31. ***[Explanation-based Finetuning Makes Models More Robust to Spurious Cues](https://aclanthology.org/2023.acl-long.242.pdf)***        

    **摘要**：大型语言模型（LLMs）功能强大，但它们有时会学习与任务无关的标签和特征之间的相关性，从而导致对分布外数据的泛化效果不佳。我们提出了基于解释的微调方法，作为一种通用方法来减轻LLM对虚假相关性的依赖。与标准微调不同的是，在标准微调中，模型仅预测输入的答案，而我们的微调则额外生成支持其答案的自由文本解释。为了评估我们的方法，我们在人为构建的包含不同类型虚假线索的训练集上对模型进行微调，并在不包含这些线索的测试集上对模型进行测试。与标准微调相比，我们的方法使GPT-3 (davinci)在四种分类任务中的准确率下降方面对虚假线索的鲁棒性明显提高：ComVE（+1.2）、CREAK（+9.1）、e-SNLI（+15.4）和SBIC（+6.5）。我们的方法在多个模型族和不同规模的模型中都有很好的效果，在大型模型中的效果更好。最后，我们的方法还能很好地使用模型生成的解释，这意味着它适用于更多没有人工编写解释的数据集。

    **关键词**：基于解释的微调方法

32. ***[Multi-target Backdoor Attacks for Code Pre-trained Models](https://aclanthology.org/2023.acl-long.399.pdf)***        

    **摘要**：由于代码智能的进步，针对神经代码模型的后门攻击已经获得了相当大的关注。然而，大多数现有工作都是在代码相关下游任务的特定任务数据中插入触发器，从而限制了攻击的范围。此外，大多数针对预训练模型的攻击都是为理解任务而设计的。在本文中，我们提出了针对代码预训练模型的任务无关后门攻击。我们的后门模型采用两种学习策略（即中毒Seq2Seq学习和标记表示学习）进行预训练，以支持下游代码理解和生成任务的多目标攻击。在部署阶段，受害者模型中植入的后门可以通过设计的触发器激活，从而实现有针对性的攻击。我们在七个数据集上的两个代码理解任务和三个代码生成任务中评估了我们的方法。广泛的实验结果表明，我们的方法能够有效、隐蔽地攻击与代码相关的下游任务。

    **关键词**：后门攻击

33. ***[Large Language Models Are Reasoning Teachers](https://aclanthology.org/2023.acl-long.830.pdf)***        

    **摘要**：最近的研究表明，思维链（CoT）提示可以诱导语言模型来逐步解决复杂的推理任务。然而，基于提示的CoT方法依赖于非常大的模型，如GPT-3 175B，这对于大规模部署来说是令人望而却步的。在本文中，我们使用这些大型模型作为推理教师，在较小的模型中实现复杂推理，并将模型大小要求降低几个数量级。我们提出了Fine-tune-CoT，一种从超大型教师模型中生成推理样本以微调较小模型的方法。我们在广泛的公共模型和复杂任务中评估了我们的方法。我们发现，Fine-tune-CoT能够在小型模型中实现强大的推理能力，在许多任务中远远优于基于提示的基线，甚至优于教师模型。此外，我们还利用教师模型为每个原始样本生成多个不同推理的能力扩展了我们的方法。通过这种多样化的推理来丰富微调数据，即使对于非常小的模型，也能在不同数据集上大幅提升性能。我们进行了消融和样本研究，以了解学生模型推理能力的出现。我们的代码实现和数据可在<https://github.com/itsnamgyu/reasoning-teacher。>

    **关键词**：思维链；Fine-tune-CoT；大模型指导

34. ***[Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning](https://aclanthology.org/2023.acl-long.172.pdf)***        

    **摘要**：大语言模型（LLM）在根据自然语言指令解决未知任务方面表现出了令人印象深刻的性能。然而，模型是否真正理解任务定义以及人类编写的定义是否是最优的，这些问题仍然不清楚。在本文中，我们系统地研究了任务定义在指令学习中的作用。我们首先根据人类注释进行了消减分析，以了解任务定义中哪些部分最重要，并发现只有当删除描述任务输出的内容，特别是标签信息时，模型性能才会大幅下降。接下来，我们提出了一种自动算法，将任务定义压缩到最小的支持标记集，并发现可以在保持甚至提高模型性能的同时去除60%的标记。基于这些结果，我们提出了两种策略来帮助模型更好地利用任务指令：(1)仅以通用的结构化格式提供任务的关键信息；(2)增加一个元调整阶段来帮助模型更好地理解定义。通过这两种策略，我们在119个未见测试任务中实现了4.2 Rouge-L的改进。

    **关键词**：自然语言指令；指令策略

35. ***[How Do In-Context Examples Affect Compositional Generalization?](https://aclanthology.org/2023.acl-long.618.pdf)***        

    **摘要**：组合泛化——理解所见基元的未知组合——是人类智能的一种基本推理能力。人工智能界主要通过在大量训练样本上对神经网络进行微调来研究这种能力，而目前流行的基于大型语言模型的few-shot学习范式——上下文学习——是否以及如何表现出组合泛化能力尚不清楚。在本文中，我们介绍了CoFe，这是一个用于研究上下文构图泛化的测试套件。我们发现，组合泛化性能很容易受到上下文示例选择的影响，因此提出了一个研究问题：组合泛化的良好上下文示例的关键因素是什么？我们研究了三个潜在因素：相似性、多样性和复杂性。我们的系统实验表明，上下文示例应在结构上与测试用例相似、彼此不同且各自简单。此外，我们还发现了两个很强的局限性：对虚构词的上下文构词泛化比对常用词的泛化要弱很多；尽管骨干模型已经在大型语料库中进行了预训练，但上下文示例必须涵盖所需的语言结构，这一点仍然至关重要。我们希望我们的分析能够促进对上下文学习范式的理解和利用。

    **关键词**：组合泛化；上下文学习

36. ***[Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer](https://aclanthology.org/2023.acl-short.24.pdf)***        

    **摘要**：最近的研究表明，大规模预训练的语言模型可以以参数高效的方式有效地适应特定任务。经过训练的轻量级参数集（如适配器）可以作为一种能力与相应的模型轻松存储和共享。拥有许多轻量级参数后，我们专注于在任务间转移这些参数，以获得新任务性能的改善，其关键点在于获得任务间的相似性。在本文中，我们探索了5种参数高效的权重集合方法来实现这种可转移性，并验证了它们的有效性。这些方法从不同角度提取数据集信息和训练过的轻量级参数来获取任务间的相似性，并根据可比性对已有的轻量级参数进行加权，从而获得适合新任务初始化的模块。我们将其应用于三种参数高效调优方法，并在广泛的下游任务集上进行了测试。实验结果表明，我们的方法比基线方法提高了5%~8\%，在很大程度上促进了任务级的知识迁移。

    **关键词**：参数高效调整；知识迁移

37. ***[Revisiting Token Dropping Strategy in Efficient BERT Pretraining](https://aclanthology.org/2023.acl-long.579.pdf)***        

    **摘要**：丢弃token是最近提出的一种策略，通过跳过几个中间层的输入token子集的计算来加速掩码语言模型（如BERT）的预训练。这种方法可以有效缩短训练时间，同时不会降低下游任务的性能。然而，我们通过实证研究发现，丢弃token容易造成语义损失问题，在处理语义密集型任务时表现不佳。受此启发，我们提出了一种简单而有效的语义一致性学习方法（ScTD）来改进token替换。ScTD旨在鼓励模型学习如何在表示空间中保留语义信息。在12个任务上进行的广泛实验表明，在我们的ScTD的帮助下，丢弃token可以在所有任务类型和模型大小上实现一致且显著的性能提升。更令人鼓舞的是，ScTD节省了多达57%的预训练时间，与传统的token替换相比，平均提高了+1.56%。

    **关键词**：语义一致性学习方法

38. ***[Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning](https://aclanthology.org/2023.acl-short.113.pdf)***        

    **摘要**：最近关于指令微调（IT）的研究已经取得了很好的性能，对未见过的任务具有zero-shot泛化的能力。通过向模型提供额外的上下文（如任务定义、示例）进行微调，这些模型取得了比未经微调的模型高得多的性能。尽管性能提升令人印象深刻，但模型从IT中学到了什么仍未得到充分研究。在这项工作中，我们通过比较模型训练中的修改指令与原始指令，分析了模型在IT过程中如何利用指令。具体来说，我们通过去除所有语义成分并仅保留输出空间信息来创建简化的任务定义，并创建包含错误输入输出映射的虚假示例。我们的实验表明，在简化的任务定义或虚假示例基础上训练的模型可以获得与在原始指令和示例基础上训练的模型相当的性能。此外，我们还引入了一个随机基线来执行zero-shot分类任务，并发现它在低资源环境下取得了与IT类似的性能（42.6%的精确匹配）（43%的精确匹配），而这两种方法都显著优于naive T5（30%的精确匹配）。我们的分析提供了证据，证明当前IT模型令人印象深刻的性能增益可能来自于拾取表面模式，例如学习输出格式和猜测。我们的研究强调了对更可靠的IT方法和评估的迫切需求。

    **关键词**：指令微调

39. ***[Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge](https://aclanthology.org/2023.acl-long.550.pdf)***        

    **摘要**：大语言模型（LLM）因其存储和利用正面知识的能力而被广泛研究。然而，诸如“狮子不生活在海洋里”等负面知识在世界上也无处不在，但却很少在文本中明确提及。LLMs对负面知识了解多少？这项工作研究了LLMs在负面常识知识方面的能力。我们设计了一个有限制的
    **关键词到句子生成任务（CG）和一个布尔问题回答任务（QA）来探究LLMs。我们的实验发现，LLMs经常无法生成基于负面常识知识的有效句子，但他们却能正确回答极性的“是”或“否”问题。我们将这种现象称为LLMs的信念冲突。我们的进一步分析表明，语言建模预训练中的统计捷径和否定报告偏差导致了这种冲突。

    **关键词**：负面知识；信念冲突

40. ***[Mitigating Label Biases for In-context Learning](https://aclanthology.org/2023.acl-long.783.pdf)***        

    **摘要**：上下文学习（ICL）的各种设计设置，例如上下文中示例的选择和顺序，都会使模型的预测产生偏差。尽管许多研究讨论了这些设计选择，但很少有系统的研究对其进行分类并减轻其影响。在这项工作中，我们定义了ICL文本分类中的三种标签偏差类型：虚标偏差、上下文标签偏差和域标签偏差（我们首次将其概念化并进行了检测）。我们的分析表明，先前的标签偏差校准方法无法解决所有三种类型的偏差。具体来说，无论选择何种上下文示例，领域标签偏差都会限制LLM在许多任务中的随机水平表现。为了减轻这些偏差的影响，我们提出了一种简单的偏差校准方法，即使用任务语料库中的随机域内词估计语言模型的标签偏差。在预测时控制这种估计偏差后，我们新颖的域语境校准方法显著提高了GPT-J和GPT-3在各种任务上的ICL性能。在具有较大领域标签偏差的任务上（在Macro-F1中高达37%），这种增益是巨大的。此外，我们的结果还可以推广到具有不同规模、预训练方法和人工设计任务指令的模型中，显示了标签偏差在ICL中的普遍性。

    **关键词**：上下文学习；标签偏差类型

41. ***[Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark](https://aclanthology.org/2023.acl-long.423.pdf)***        

    **摘要**：大型语言模型（LLMs）在文本理解和生成方面表现出强大的能力。一些公司已经开始提供基于这些LLM的嵌入即服务（EaaS），这将有利于客户完成各种自然语言处理（NLP）任务。然而，先前的研究表明，EaaS很容易受到模型提取攻击，这会给LLM的所有者造成巨大损失，因为训练这些模型的成本极其昂贵。为了保护EaaS的LLM版权，我们提出了一种名为 "嵌入水印"（Embedding Watermark）的方法。我们的方法从一般文本语料库中选取一组中等频率的词组成触发集，然后选取目标嵌入作为水印，并将其作为后门插入到包含触发词的文本嵌入中。插入的权重与文本中包含的触发词的数量成正比。这使得水印后门可以有效地转移到EaaS-窃取者的模型中进行版权验证，同时将对原始嵌入式的效用的不利影响降到最低。我们在各种数据集上的大量实验表明，我们的方法可以在不影响服务质量的前提下有效保护EaaS模型的版权。我们的代码在<https://github.com/yjw1029/EmbMarker>。

    **关键词**：EaaS；嵌入水印；版权保护

42. ***[ThinkSum: Probabilistic reasoning over sets using large language models](https://aclanthology.org/2023.acl-long.68.pdf)***

    **摘要**：大语言模型（LLMs）在高级类比推理方面具有很强的能力：在线性文本中再现训练数据中出现的模式（zero-shot评估）或在提供的上下文中出现的模式（few-shot上下文学习）。然而，最近的研究表明，即使是更高级的LLM，在需要对多个对象或事实进行推理并进行逻辑推理序列的场景中也会失败。我们提出了一种两阶段概率推理范式ThinkSum，它以结构化的方式对对象或事实集进行推理。在第一阶段（Think--关联检索），LLM对从提示或辅助模型调用中提取的一组短语进行并行查询。在第二阶段（Sum--概率推断或推理），这些查询的结果被汇总以进行最终预测。我们在LLM评估任务的BIG-bench套件中展示了ThinkSum的可能性和优势，在13个高难度任务中，ThinkSum比使用GPT-family模型的现有技术水平有所提高，通常使用的模型变体要小得多。我们还将ThinkSum与其他针对LLM直接提示的修改建议进行了比较和对比，例如思维链提示的变体。我们的研究结果表明，由于ThinkSum中的概率推理是在调用LLM之外进行的，因此ThinkSum对提示设计的敏感性较低，能够产生更多可解释的预测，并且能够灵活地与潜在变量模型相结合，从LLM中提取结构化知识。总之，我们提出的范式是增强LLM推理能力的一种有前途的方法。

    **关键词**：推理增强；两阶段概率推理；检索

43. ***[Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models' Memories](https://aclanthology.org/2023.acl-long.280.pdf)***        

    **摘要**：预训练语言模型（PLMs）在理解通用领域的文本方面表现出卓越的能力，但在特定领域却举步维艰。尽管在大型特定领域语料库上进行持续的预训练是有效的，但在该领域调整所有参数的成本很高。在本文中，我们研究了是否只需调整几个参数就能有效且高效地调整PLM。具体而言，我们将Transformer架构中的前馈网络（FFN）解耦为两部分：原始预训练的FFN，用于维护旧领域知识；我们新颖的特定领域适配器，用于并行注入特定领域知识。然后，我们采用混合适配器门动态融合来自不同领域适配器的知识。我们提出的混合领域适配器（MixDA）采用两阶段适配器调整策略，利用未标记数据和标记数据帮助领域适配：$i$）未标记数据上的特定领域适配器；其次是$ii$）标记数据上的特定任务适配器。MixDA可以无缝地插入到预训练-调谐范式中，我们的实验证明MixDA在域内任务(GLUE)、域外任务(ChemProt, RCT, IMDB, Amazon)和知识密集型任务(KILT)上都取得了优异的性能。进一步的分析证明了我们方法的可靠性、可扩展性和高效性。

    **关键词**：领域适配

44. ***[LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion](https://aclanthology.org/2023.acl-long.792.pdf)***        

    **摘要**：我们提出的LLM-Blender是一个集合框架，旨在通过利用多个开源大型语言模型（LLM）的不同优势来获得持续的卓越性能。我们的框架由两个模块组成：PairRanker和GenFuser，解决了不同示例的最佳LLM可能存在显著差异的问题。PairRanker采用专门的成对比较方法来区分候选输出之间的细微差别。它对输入文本和一对候选结果进行联合编码，使用交叉注意编码器来确定优选结果。我们的结果表明，PairRanker与基于ChatGPT的排名相关性最高。然后，GenFuser旨在合并排名靠前的候选词，通过利用它们的长处并减少它们的短处来产生改进的输出。为了便于大规模评估，我们引入了一个基准数据集MixInstruct，它是一个多指令混合数据集，具有oracle成对比较的特点。在各种指标上，我们的LLM-Blender明显优于单个LLM和基线方法，建立了一个实质性的性能差距。

    **关键词**：LLM集合框架；基准数据集

45. ***[Making Language Models Better Reasoners with Step-Aware Verifier](https://aclanthology.org/2023.acl-long.291.pdf)***        

    **摘要**：few-shot学习是一项具有挑战性的任务，它要求语言模型从有限的示例中进行泛化。像GPT-3和PaLM这样的大型语言模型已经在这一领域取得了令人瞩目的进展，但它们在推理任务（如GSM8K，一种算术问题基准）中仍然面临困难。为了提高它们的推理能力，以前的工作已经提出在给出最终答案之前用提示来引导语言模型，从而在GSM8K上实现了问题解决率从17.9%到58.1%的显著提高。在本文中，我们介绍了DiVeRSe（推理步骤多样化验证器），这是一种进一步增强语言模型推理能力的新方法。DiVeRSe由三个主要部分组成：首先，它生成多样化的提示，以探索同一问题的不同推理路径；其次，它使用验证器根据加权投票方案过滤掉不正确的答案；第三，它单独验证每个推理步骤，而不是整个推理链。我们在最新的语言模型code-davinci-002上对DiVeRSe进行了评估，结果表明它在8个推理基准中的6个基准上取得了新的一流结果（例如，GSM8K 74.4%到83.2%）。

    **关键词**：推理增强；推理步骤多样化

46. ***[Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification](https://aclanthology.org/2023.acl-long.312.pdf)***        

    **摘要**：事件模式是一种关于事件典型进展的世界知识。最近的事件模式归纳方法使用信息提取系统从文档中构建大量事件图实例，然后从这些实例中学习归纳模式。与此相反，我们建议将事件模式视为一种常识性知识，可以从大型语言模型（LLM）中推导出来。这种新范式大大简化了模式归纳过程，使我们能够以一种简单的方式处理事件之间的层次关系和时间关系。由于事件模式具有复杂的图结构，我们设计了一种增量提示和验证方法IncPrompt，将复杂事件图的构建分解为三个阶段：事件骨架构建、事件扩展和事件-事件关系验证。与直接使用LLMs生成线性化图相比，IncSchema可以生成大型复杂模式，在时间关系方面提高了7.2%的F1，在层次关系方面提高了31.0%的F1。此外，与之前最先进的闭域模式归纳模型相比，人类评估者在将模式转化为连贯的故事时能够覆盖更多的事件（约10%），并且在可读性方面对我们的模式的评分高出1.3分（5分制）。

    **关键词**：事件模式；增量提示

47. ***[Hierarchical Verbalizer for Few-Shot Hierarchical Text Classification](https://aclanthology.org/2023.acl-long.164.pdf)***        

    **摘要**：在实际应用中，由于复杂的标签层次结构和高昂的标签成本，分层文本分类（HTC）的性能较差，尤其是在低资源或少标签的情况下。最近，在预训练语言模型（PLMs）上应用提示的趋势越来越明显，这在少量平面文本分类任务中表现出了有效性。然而，当训练数据极度匮乏时，在HTC问题中研究基于提示的学习范式的工作还很有限。在这项工作中，我们定义了基于路径的少量文本分类设置，并建立了严格的基于路径的评价指标，以进一步探索少量文本分类任务。为了解决这个问题，我们提出了分层动词化器（"HierVerb"），这是一个多动词化器框架，将HTC视为多层单标签或多标签分类问题，并将学习向量视为受分层结构和分层对比学习约束的动词化器。通过这种方式，HierVerb将标签层次结构知识融合到动词化器中，显著优于那些通过图编码器注入层次结构的动词化器，最大限度地发挥了PLM的优势。在少数人参与的设置下，在三个流行的HTC数据集上进行的广泛实验表明，使用HierVerb的提示可以显著提高HTC的性能，同时为弥合大型预训练模型与下游层次分类任务之间的差距提供了一种优雅的方法。

    **关键词**：分层文本分类

48. ***[HINT: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation](https://aclanthology.org/2023.acl-long.631.pdf)***        

    **摘要**：最近的NLP模型已经显示出卓越的能力，能够仅使用自然语言指令作为指导，有效地将zero-shot推广到新任务中。然而，由于依赖于将冗长的指令与每个输入示例连接起来，许多此类方法都存在计算成本高的问题，导致指令的重新处理成本高昂。为了避免这种情况，我们引入了用于指令微调的超网络（Hypernetworks for INstruction Tuning，HINT），它使用预训练的文本编码器将任务指令和示例转换为参数效率高的模块，插入到底层模型中，从而消除了在模型输入中包含指令的需要。HINT中的超网络也会产生编码指令，我们在解码过程中将其与编码输入串联，以进一步提高性能。在控制计算量（以FLOPs衡量）的情况下，HINT模型的性能超过最先进基线的10%。通过将指令转换为模块，HINT模型可以有效地忽略指令的长度和计算使用方面的少量示例输入。因此，HINT可以通过加入额外的少量数据将其性能提高25%，而计算用量仅增加5%。这结合了参数高效微调和上下文学习的优势。

    **关键词**：高效指令微调

49. ***[Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization](https://aclanthology.org/2023.acl-long.279.pdf)***        

    **摘要**：本文介绍了一种新的预训练语言模型Z-Code++，该模型针对抽象文本摘要进行了优化。该模型使用三种技术扩展了最先进的编码器-解码器模型。首先，我们使用两阶段预训练来提高模型在低资源摘要任务中的性能。首先使用文本语料库对模型进行语言理解预训练，然后继续使用摘要语料库对模型进行预训练，以生成基础文本。其次，我们用分离注意力层取代编码器中的自我注意力层，其中每个词使用两个向量表示，分别编码其内容和位置。第三，我们使用融合编码器，这是一种以分层方式对长序列进行编码的简单而有效的方法。Z-Code++在5种语言的13项文本总结任务中的9项创造了新的一流水平。我们的模型参数效率高，在XSum上优于600倍的PaLM540B，在SAMSum上优于200倍的GPT3175B。在zero-shot和few-shot设置中，我们的模型大大优于竞争模型。

    **关键词**：预训练模型；抽象摘要；分离注意力；融合编码器

50. ***[Can Large Language Models Be an Alternative to Human Evaluations?](https://aclanthology.org/2023.acl-long.870.pdf)***        

    **摘要**：要评估机器学习模型生成的文本或人类撰写的文本的质量，人工评估是不可或缺和不可避免的。然而，人工评估很难再现，而且其质量也不稳定，这阻碍了不同自然语言处理（NLP）模型和算法之间的公平比较。最近，大型语言模型（LLM）在只提供任务指令的情况下，在未见任务中表现出了卓越的性能。在本文中，我们探讨了LLMs的这种能力是否可以用来替代人工评估。我们向LLM提供与人类评估完全相同的指令、待评估样本和问题，然后要求LLM生成对这些问题的响应；我们将此称为LLM评估。我们使用人工评估和LLM评估来评估两个NLP任务中的文本：开放式故事生成和对抗性攻击。我们发现，LLM评价的结果与专家人工评价的结果一致：人工专家评价较高的文本，LLM的评价也较高。我们还发现，LLM的评价结果在不同格式的任务指令和用于生成答案的抽样算法中都是稳定的。我们首次展示了使用LLM评估文本质量的潜力，并讨论了LLM评估的局限性和伦理考虑。

    **关键词**：LLM代替人工评估

51. ***[Gradient Ascent Post-training Enhances Language Model Generalization](https://aclanthology.org/2023.acl-short.74.pdf)***        

    **摘要**：在这项工作中，我们通过实证研究表明，在随机、无标注的文本语料库上通过几步梯度上升后训练（GAP）更新预训练的LMs（350M, 1.3B, 2.7B），可以增强其在不同NLP任务中的zero-shot泛化能力。具体来说，我们发现，在12种不同的NLP任务中，GAP可以使LMs与2-3倍大的LMs相媲美。我们还表明，在非分布式语料库中应用GAP能够带来最可靠的性能改进。我们的研究结果表明，GAP是一种很有前途的方法，可以在不对特定任务进行微调的情况下提高LM的泛化能力。

    **关键词**：梯度上升训练；泛化能力增强

52. ***[Knowledgeable Parameter Efficient Tuning Network for Commonsense Question Answering](https://aclanthology.org/2023.acl-long.503.pdf)***        

    **摘要**：常识性问题解答对于日常事务的决策非常重要。尽管现有的常识性问题解答工作基于完全微调的PLM，已经取得了可喜的成果，但它们存在计算成本过高以及可解释性差的问题。一些作品通过精心设计的GNN模块（需要专业知识），结合知识来提供特定证据，从而改进了PLMs。在本文中，我们提出了一种简单的知识参数高效微调网络，将PLM与外部知识相结合，用于常识性问题解答。具体来说，我们设计了一个可训练的参数共享适配器，该适配器与参数冻结PLM相连，以较小的成本纳入知识。该适配器通过两个与知识相关的辅助任务（即跨度掩蔽和关系辨别）同时配备了与实体和问题相关的知识。为了使适配器专注于相关知识，我们设计了门控和关注机制，以分别过滤和融合来自PLM的查询信息。在两个基准数据集上进行的广泛实验表明，KPE具有参数效率高的特点，能够有效地整合知识以改进常识性问题解答。

    **关键词**：常识性问题解答；参数共享适配器；知识注入

53. ***[Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings](https://aclanthology.org/2023.acl-short.102.pdf)***        

    **摘要**：在transformer语言模型中使用位置嵌入已被广泛接受。然而，最近的研究对这种嵌入的必要性提出了质疑。我们通过证明一个随机初始化和冻结的transformer语言模型，在没有位置嵌入的情况下，通过自我注意方差的缩小，固有地编码了强大的位置信息，从而进一步扩展了这一研究。为了量化这种差异，我们推导出了transformer层中每一步的基本分布。通过使用完全预训练模型进行经验验证，我们发现在大量梯度更新之后，方差收缩效应仍然存在。我们的发现证明了舍弃位置嵌入的决定是正确的，从而促进了transformer语言模型更有效的预训练。

    **关键词**：位置嵌入舍弃

54. ***[Large Language Models Meet NL2Code: A Survey](https://aclanthology.org/2023.acl-long.411.pdf)***        

    **摘要**：根据自然语言描述生成代码（或称NL2Code）的任务被认为是代码智能领域的一项紧迫而重大的挑战。由于预训练技术的快速发展，针对代码的大型语言模型不断涌现，推动了NL2Code的发展。为了促进该领域的进一步研究和应用，我们在本文中对现有的27种NL2Code大型语言模型进行了全面调查，并回顾了基准和指标。我们在HumanEval基准上对所有现有模型进行了直观的比较。通过深入的观察和分析，我们得出结论，NL2Code大型语言模型成功的关键因素是 "大型、优质数据、专家调优"。此外，我们还讨论了模型与人类之间的差距所带来的挑战和机遇。我们还创建了一个网站<https://nl2code.github.io>，通过众包追踪最新进展。据我们所知，这是第一份关于NL2Code大型语言模型的调查报告，我们相信它将为该领域的持续发展做出贡献。

    **关键词**：综述；代码生成

55. ***[Rehearsal-free Continual Language Learning via Efficient Parameter Isolation](https://aclanthology.org/2023.acl-long.612.pdf)***        

    **摘要**：我们研究了在学习一系列语言处理任务时如何避免灾难性遗忘的问题。与之前的方法相比，我们强调了不缓存历史任务数据的重要性，这使得问题更具挑战性。我们提出的方法采用参数隔离策略。对于每个任务，它分配一小部分私有参数，并通过一个共享的预训练模型来学习这些参数。为了在测试时加载正确的参数，我们引入了一种简单而有效的非参数方法。在持续语言学习基准上的实验表明，我们的方法明显优于所有现有的无数据缓存方法，并且与使用历史数据的方法相当（甚至更好）。

    **关键词**：灾难性遗忘；参数隔离

56. ***[MISGENDERED: Limits of Large Language Models in Understanding Pronouns](https://aclanthology.org/2023.acl-long.293.pdf)***        

    **摘要**：语言技术中的性别偏见已被广泛研究，但研究大多局限于二元性别范式。考虑非二元性别身份也是至关重要的，因为将其排除在外可能会对已经被边缘化的群体造成进一步伤害。在本文中，我们全面评估了流行语言模型在正确使用英语中性代词（如单数they、them）和新代词（如ze、xe、thon）方面的能力，这些代词是那些性别身份不被二元代词所代表的个体所使用的。我们介绍了Misgendered，这是一个评估大型语言模型正确使用首选代词能力的框架，它包括（1）声明个人代词的实例，然后是一个缺少代词的句子，以及（2）使用统一方法评估屏蔽和自动回归语言模型的实验装置。当提示开箱即用时，语言模型在正确预测新代词（平均准确率为7.6%）和性别中性代词（平均准确率为31.0%）方面表现不佳。这种泛化能力的缺乏是由于在训练数据和记忆关联中缺乏对非二元代词的表示。在提示中使用明确示例进行少量适应性训练提高了性能，但对新代词的适应性仅为45.4%。我们在<https://tamannahossainkay.github.io/misgendered>上发布了完整的数据集、代码和演示。

    **关键词**：性别偏见；代词选择

57. ***[ALERT: Adapt Language Models to Reasoning Tasks](https://aclanthology.org/2023.acl-long.60.pdf)***        

    **摘要**：大型语言模型的最新进展使它们能够在复杂的任务中表现出色，这些任务需要通过few-shot学习进行逐步推理。然而，目前还不清楚这些模型是在应用它们在预训练中学习到的推理技能，还是它们只是在更精细的粒度上记忆它们的训练语料库，并且已经学会更好地理解它们的上下文。为了解决这个问题，我们引入了ALERT模型，一个评估语言模型推理能力的基准和分析套件。ALERT能够在需要推理能力的复杂任务上比较预训练模型和微调模型。我们的基准提供了一个测试平台来评估任何语言模型的细粒度推理技能，它跨越了20多个数据集，涵盖了10种不同的推理技能。通过使用ALERT，我们进一步研究了微调的作用。我们广泛的实证分析表明，与预训练阶段相比，语言模型在微调阶段学习到了更多的推理技能，如文本蕴涵、归纳推理和类比推理。然而，我们也发现，当语言模型被微调时，它们倾向于过度拟合提示模板，这损害了模型的鲁棒性，导致泛化问题。

    **关键词**：推理能力评估

58. ***[Revisiting Relation Extraction in the era of Large Language Models](https://aclanthology.org/2023.acl-long.868.pdf)***        

    **摘要**：关系提取（RE）是从文本中推断实体之间语义关系的NLP核心任务。标准的监督关系提取技术需要训练模块来标记包含实体跨度的标记词，然后预测它们之间的关系。最近的工作将这一问题作为序列到序列的任务来处理，将实体之间的关系线性化为目标字符串，并根据输入条件生成。在这里，我们挑战了这种方法的极限，使用了比之前的工作更大的语言模型（GPT-3和Flan-T5 large），并在不同的监督水平下评估了它们在标准RE任务中的性能。我们通过进行人工评估，而不是依赖精确匹配，解决了评估RE生成方法的固有问题。在这种完善的评估下，我们发现(1)使用GPT-3进行的少量提示实现了接近SOTA的性能，即与现有的完全监督模型大致相当；(2)Flan-T5在少量提示设置中的能力较弱，但是使用思维链（CoT）风格的解释（通过GPT-3生成）对其进行监督和微调可以获得SOTA的结果。我们将该模型作为RE任务的新基准发布。

    **关键词**：关系提取

59. ***[Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor](https://aclanthology.org/2023.acl-long.806.pdf)***        

    **摘要**：指令微调使预训练的语言模型能够根据推理时的自然语言描述执行新任务。这些方法依赖于大量以众包数据集或用户交互为形式的人工监督。在这项工作中，我们引入了“非自然指令”（Unnatural Instructions）：这是一个包含各种创造性指令的大型数据集，几乎不需要人工干预。我们收集了64,000个示例，通过向语言模型提示三个指令种子示例并诱发第四个示例。然后，通过提示模型重新表述每条指令来扩展该数据集，总共创建了约240,000个指令、输入和输出示例。实验表明，尽管Unnatural Instructions包含了相当数量的噪声，但它的训练效果可以与在开源人工合成数据集上的训练效果相媲美，在各种基准测试中超过了T0++和Tk-Instruct等模型的性能。这些结果证明了模型生成的数据作为众包数据集扩展和多样化的一种具有成本效益的替代方法的潜力。

    **关键词**：指令数据集；模型生成数据

60. ***[RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs](https://aclanthology.org/2023.acl-long.427.pdf)***        

    **摘要**：尽管语言模型取得了前所未有的成功，但即使是最大型的语言模型也会犯错误。与人类利用反馈进行学习和改进的方式类似，以前的工作也提出为语言模型提供自然语言反馈，以指导它们修复输出结果。由于获取人类生成的批评意见成本高昂，研究人员设计了学习型批评意见生成器来代替人类批评意见，同时假设人们可以训练下游模型来利用生成的反馈意见。然而，这种方法并不适用于黑盒或有限访问模型，如ChatGPT，因为它们无法进行微调。此外，在大型通用语言代理时代，微调既不具有计算效率，也不具有空间效率，因为它会导致网络的多个副本。在这项工作中，我们引入了RL4F（反馈强化学习），它是一个多代理协作框架，在该框架中，评论生成器被训练为最大化GPT-3的最终任务性能，GPT-3是一个固定模型，其大小是评论生成器的200多倍。RL4F产生的批评有助于GPT-3修正其输出。我们研究了行动规划、总结和字母化的三个数据集，结果表明，与其他基于学习、检索增强或提示的批判生成器相比，RL4F在多个文本相似度指标上的相对改进高达10%。

    **关键词**：反馈强化学习；评论生成器；修正输出

61. ***[From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding](https://aclanthology.org/2023.acl-long.200.pdf)***        

    **摘要**：目前最先进的自然语言理解模型需要一个预处理步骤，将原始文本转换为离散的标记。这一过程被称为标记化（tokenization），依赖于预先建立的单词或子单词语素词汇。这种固定词汇限制了模型对拼写错误的鲁棒性以及适应新领域的能力。在这项工作中，我们引入了一种新颖的开放词汇语言模型，该模型采用了分层的两级方法：一个是词级，另一个是序列级。具体来说，我们设计了一个词内模块，该模块使用浅层transformer架构从字符中学习词的表示，以及一个深层词间transformer模块，该模块通过关注整个词序列来上下文化每个词的表示。因此，我们的模型可直接在字符序列上进行操作，同时明确意识到单词的边界，但不偏重子单词或单词级词汇。各种下游任务的实验表明，我们的方法优于强大的基线方法。我们还证明，我们的分层模型对文本损坏和领域转移具有鲁棒性。

    **关键词**：开放词汇语言模型

62. ***[Few-shot In-context Learning on Knowledge Base Question Answering](https://aclanthology.org/2023.acl-long.385.pdf)***        

    **摘要**：知识库问题解答被认为是一个难题，因为它面临着对各种可能的自然语言问题进行泛化的挑战。此外，不同知识库之间知识库模式项的异质性往往要求对不同的知识库问题解答（KBQA）数据集进行专门的训练。为了用一个统一的免训练框架来处理不同知识库问题集上的问题，我们提出了KB-BINDER，它首次实现了知识库问题集任务的few-shot上下文学习。首先，KB-BINDER利用像Codex这样的大型语言模型，通过模仿一些示范来生成逻辑形式作为特定问题的草稿。其次，KB-BINDER以知识库为基础，通过BM25分数匹配将生成的草稿绑定到可执行的草稿上。在4个公开的异构KBQA数据集上的实验结果表明，KB-BINDER可以在仅有少量上下文演示的情况下实现很强的性能。特别是在GraphQA和3-hop MetaQA上，KB-BINDER的表现甚至超过了最先进的训练模型。在GrailQA和WebQSP上，我们的模型也与其他完全训练过的模型相当。我们相信KB-BINDER可以作为未来研究的重要基准。我们计划公布所有代码和数据。我们的代码在<https://github.com/ltl3A87/KB-BINDER>。

    **关键词**：知识库问题解答；上下文学习

63. ***[Revisiting Automated Prompting: Are We Actually Doing Better?](https://aclanthology.org/2023.acl-short.155.pdf)***        

    **摘要**：目前的文献表明，大型语言模型（LLM）是优秀的few-shot学习者，在few-shot学习环境中，提示可以显著提高它们在一系列下游任务中的性能。随后，我们尝试将人工提示自动化，并取得了一些进展。特别是，随后的工作表明，在某些K-shot学习场景中，自动化可以优于微调。在本文中，我们在六个不同的下游任务和更大范围的K-shot学习设置中重新研究了自动提示技术。我们发现，自动提示并不能始终优于简单的人工提示。我们的工作表明，除了微调之外，人工提示应作为该研究领域的基准。

    **关键词**：自动提示

64. ***[Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning](https://aclanthology.org/2023.acl-short.129.pdf)***        

    **摘要**：众所周知，大型语言模型（LLM）会记忆大量的训练数据。部分记忆内容已被证明可以通过简单查询模型提取出来，这就带来了隐私风险。我们提出了一种新颖的方法，利用提示调整来控制LLM中记忆内容的提取率。我们提出了提高和降低提取率的两种提示训练策略，分别对应于攻击和防御。我们使用GPT-Neo系列模型在一个公共基准上演示了我们技术的有效性。对于1.3B参数的GPT-Neo模型，与基线相比，我们的攻击使提取率提高了9.3个百分点。我们的防御可以通过用户指定的超参数进行调整，以实现不同的隐私-效用权衡。与基线相比，我们的提取率降低了97.7%，复杂度增加了16.9%。

    **关键词**：记忆提取

65. ***[DISCO: Distilling Counterfactuals with Large Language Models](https://aclanthology.org/2023.acl-long.302.pdf)***        

    **摘要**：使用反事实增强数据训练的模型可以学习任务因果结构的表征，从而实现稳健的泛化。然而，对于大多数任务而言，高质量的反事实数据非常稀缺，且不易大规模生成。如果是众包数据，这种数据的规模和多样性通常有限；如果是使用监督方法生成的数据，要扩展到新的反事实维度，计算成本很高。在这项工作中，我们介绍了DISCO（DIStilled COunterfactual Data），一种大规模自动生成高质量反事实数据的新方法。DISCO通过一个大型通用语言模型来生成短语扰动。然后，特定任务的教师模型对这些生成的数据进行过滤，从而提炼出高质量的反事实数据。虽然与任务无关，但我们将我们的管道应用于自然语言推理（NLI）任务，并发现在NLI压力测试等具有挑战性的评估中，与未经数据增强训练的模型相比，使用DISCO生成的反事实数据训练的相对较小的学生模型更加稳健（绝对值为6%），并且在不同分布之间的泛化效果更好（2%）。此外，DISCO增强模型在三个评估集上的反事实对之间的一致性提高了10%，这表明DISCO增强使模型能够更可靠地学习因果表征。我们的资料库可在以下网址获得：<https://github.com/eric11eca/disco>

    **关键词**：反事实数据；数据生成

66. ***[AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models](https://aclanthology.org/2023.acl-short.149.pdf)***        

    **摘要**：通过对话帮助用户收集信息的“信息搜索对话”近年来取得了长足的进步。然而，由于训练数据的匮乏，这项研究仍然受阻。为了解决这一问题，我们提出了AutoConv合成会话生成方法，该方法利用了大语言模型（LLM）的few-shot学习能力和生成能力。具体来说，我们将对话生成问题表述为一个语言建模任务，然后利用一些人类对话对LLM进行微调，以捕捉信息搜索过程的特征，并利用它生成高质量的合成对话。在两个常用数据集上的实验结果验证了AutoConv比强基线有很大改进，并减轻了对人类注释的依赖。此外，我们还提供了一些分析研究，以促进未来的研究。

    **关键词**：信息搜索对话

67. ***[NarrowBERT: Accelerating Masked Language Model Pretraining and Inference](https://aclanthology.org/2023.acl-short.146.pdf)***        

    **摘要**：大规模语言模型预训练是自然语言处理中一种非常成功的自监督学习形式，但随着模型和预训练语料库的不断扩大，其执行成本也越来越高。我们提出的NarrowBERT是一种改进的transformer编码器，可将掩码语言模型预训练的吞吐量提高2倍以上。NarrowBERT对transformer模型进行了稀疏化处理，使得自注意查询和前馈层在预训练过程中只对每个句子的屏蔽词组进行操作，而不是像通常的transformer编码器那样对所有词组进行操作。我们还表明，NarrowBERT将推理时的吞吐量提高了3.5倍之多，而在MNLI等句子编码任务上的性能下降却很小（或没有）。最后，我们检验了NarrowBERT在IMDB和Amazon评论分类以及CoNLL NER任务中的性能，结果表明它与标准BERT的性能相当。

    **关键词**：改进编码器；稀疏化

# Findings

1. ***[Recyclable Tuning for Continual Pre-training](https://aclanthology.org/2023.findings-acl.723.pdf)***        

    **摘要**：持续预训练是指预训练的语言模型（PLM）不断从增长的数据中获取新知识并逐步升级的模式。在升级后的PLM发布之前，我们可能已经针对各种任务对原始PLM进行了调整，并存储了调整后的权重。然而，在调整升级后的PLM时，这些过时的适应权重通常会被忽略和丢弃，造成潜在的资源浪费。我们将这一问题摆在突出位置，并认为应开发适当的算法来回收过时的适配权重。为此，我们制定了持续预训练的可回收调整任务。在试验研究中，我们发现在持续预训练后，升级后的PLM在一定程度上仍与过时的适配权重兼容。基于这一发现，我们从两个新的方面分析了持续预训练的PLM之间的联系，即模式连接性和功能相似性。在此基础上，我们提出了一种基于初始化的方法和一种基于蒸馏的方法。我们证明了这两种方法在提高收敛性和性能以调整升级后的PLM方面的可行性。我们还证明了将这两种方法结合起来可以获得更好的性能。

    **关键词**：持续预训练；权重回收

1. ***[Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models](https://aclanthology.org/2023.findings-acl.40.pdf)***        

    **摘要**：生成中间步骤或思维链（CoT）是显著提高语言模型（LM）多步骤推理能力的有效方法。然而，CoT长度会随着问题复杂度的增加而迅速增长，很容易超过最大上下文大小。与其增加上下文限制，我们探索了一个正交方向：使语言模型将问题划分为多个上下文。我们提出了一个新的推理框架，称为 "思维递归"（RoT），它引入了几个特殊标记，模型可以输出这些标记来触发与上下文相关的操作。包括GPT-3在内的多种架构的广泛实验表明，RoT显著提高了LMs的推理能力，以解决由成千上万个标记组成的问题。

    **关键词**：思维链；上下文长度限制；思维递归；

1. ***[Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors](https://aclanthology.org/2023.findings-acl.50.pdf)***        

    **摘要**：最近的研究表明，在大规模指令跟随数据集上对大型语言模型（LLMs）进行微调，可以大幅提高它们在各种NLP任务中的性能，尤其是在zero-shot场景下。然而，在关系提取（RE）这一基本的信息提取任务上，即使是先进的指令调整LLM仍然无法超越小型LM。我们假设，由于RE在指令微调数据集中的发生率较低，占所有任务的比例不到1%（Wang等，2022年），因此指令微调无法在LLM中激发强大的RE能力。为了解决这一局限性，我们提出了QA4RE，这是一个将RE与问题解答（QA）相结合的框架，问题解答是指令微调数据集中的主要任务。在四个数据集上使用两个系列的指令微调LLM（共六个LLM）进行了全面的zero-shot RE实验，结果表明我们的QA4RE框架持续提高了LLM的性能，有力地验证了我们的假设，并使LLM的性能大大超过了强大的zero-shot基线。此外，我们还提供了详尽的实验和讨论，以展示我们的QA4RE框架的鲁棒性、少射有效性和强大的可移植性。这项工作通过将LLM与更常见的指令微调任务（如QA）相结合，为LLM适应具有挑战性且代表性不足的任务提供了一种可行的方法。

    **关键词**：关系提取；RE与QA结合框架

1. ***[ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models](https://aclanthology.org/2023.findings-acl.218.pdf)***        

    **摘要**：在过去十年中，词级类比作为评估词嵌入方法（如word2vec）质量的内在指标发挥了重要作用。然而，现代的大语言模型（LLMs）主要是基于GLUE和SuperGLUE等基准进行外在评估，而对于LLMs是否能够在长文本之间进行类比的研究却寥寥无几。在本文中，我们提出了“类比”（ANALOGICAL）这一新的基准，通过长文本的类比分类法对LLM进行内在评估，该分类法具有六个复杂度级别——(i)词，(ii)词与句子，(iii)句法，(iv)否定，(v)引申，以及(vi)隐喻。利用13个数据集和三种不同的距离测量方法，我们评估了八种LLM在语义向量空间中识别类比对的能力。我们的评估发现，随着类比分类法的上升，LLM在识别类比方面面临的挑战越来越大。

    **关键词**：内在评估；类比基准

1. ***[Data-Efficient Finetuning Using Cross-Task Nearest Neighbors](https://aclanthology.org/2023.findings-acl.576.pdf)***        

    **摘要**：获取用于训练感兴趣任务模型的标记数据通常代价高昂。先前的研究表明，在多任务数据上进行训练并使用任务描述（提示）进行增强有效地将知识转移给新任务。为了高效构建任务特定的模型，我们假设可以访问少量（32-1000个）未标记的目标任务示例，并使用它们从一个大型的带有提示的多任务数据池中检索最相似的标记示例。与当前的均匀抽样提示多任务数据（例如：FLAN、T0）微调模型的方法相比，我们的跨任务最近邻微调方法在数据利用率方面显著更高。仅使用P3数据池中仅有的2%的数据且没有任何标记的目标任务数据，我们的模型在12个包括法律和科学文档问答在内的测试任务上，相较于在所有可用数据上训练的强基线模型，性能提升了3-30%。类似地，从SuperNaturalInstructions中使用跨任务最近邻训练的模型，仅占数据池的约5%，在该数据池的12个测试任务上获得了与最先进模型相媲美的性能。此外，我们方法产生的模型还为目标任务数据的少样本微调提供了更好的初始化，表现为相对于少样本微调的T0-3B模型，在8个数据集上相对改进了2-23%。

    **关键词**：数据效率；跨任务最近领；多任务学习；少样本学习

1. ***[Cost-effective Distillation of Large Language Models](https://aclanthology.org/2023.findings-acl.463.pdf)***        

    **摘要**：知识蒸馏（KD）包括训练一个小型“学生”模型来复制一个高容量“教师”模型的强大性能，从而在资源受限的环境中实现高效部署。性能最佳的方法往往针对特定任务或架构，缺乏通用性。现有的几种方法需要在特定任务数据集上对教师进行预训练，这对于大型数据集来说成本高昂，而对于小型数据集来说则不稳定。在这里，我们提出了一种通过与任务和模型架构无关的新型蒸馏损失来改进KD的方法。我们成功地将我们的方法应用于BERT基础的蒸馏，并在一系列GLUE任务中从蒸馏的学生中获得了极具竞争力的结果，特别是对于数据集较小的任务。

    **关键词**：知识蒸馏；通用性；任务、架构无关蒸馏损失

1. ***[Reasoning in Large Language Models Through Symbolic Math Word Problems](https://aclanthology.org/2023.findings-acl.364.pdf)***        

    **摘要**：大型语言模型(LLM)解决了几乎没有标注数据的下游任务，从而彻底改变了NLP。尽管LLM具有多种能力，但它们的推理能力这一更大的问题仍然没有得到很好的理解。本文通过研究数字问题的符号版本来解决数学文字问题（MWPs）中的推理问题，因为符号表达是数字答案的“简明解释”。我们创建并使用了SVAMP数据集的符号版本，并发现GPT-3的davinci-002模型在符号MWPs上也有很好的zero-shot精度。为了评估模型推理的准确性，我们不仅评估了准确性，还评估了最终答案和输出推理之间的一致性，这分别对应于MWPs的数字答案和符号答案。我们探索了一种自我提示的方法，以鼓励符号推理与数字答案保持一致，从而使LLM具备提供简明、可验证的推理的能力，并使其更具可解释性。令人惊讶的是，自我提示也提高了符号准确度，使其高于数字和符号准确度，从而产生了集合效应。SVAMP-Sym数据集将用于未来的符号数学问题研究。

    **关键词**：推理能力；符号推理；自我提示；可解释性

1. ***[How Well Do Large Language Models Perform on Faux Pas Tests?](https://aclanthology.org/2023.findings-acl.663.pdf)***        

    **摘要**：受大型语言模型在多大程度上“理解”社会智力这一问题的启发，我们研究了这些模型对涉及“假动作”情境描述的问题产生正确回答的能力。假动作测验是临床心理学中使用的一种测验，众所周知，它对儿童来说比单独的心智理论或社会智力测验更具挑战性。我们的研究结果表明，尽管模型有时似乎能提供正确的回答，但实际上他们在这项任务中很吃力，而且许多看似正确的回答可以归因于人类读者的过度解读（“ELIZA效应”）。另外一个被观察到的现象是大多数模型无法对预设问题做出正确的回答。最后，在一个实验中，模型的任务是生成新颖的假话故事，我们发现，虽然一些模型能够生成新颖的假话故事，但这些故事都是显性的，因为模型以隐性方式描述情况的能力有限。

    **关键词**：假动作测验

1. ***[Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models](https://aclanthology.org/2023.findings-acl.472.pdf)***        

    **摘要**：语言模型已被证明表现出正扩展性，即随着模型在规模、计算或数据方面的扩展，其性能也会提高。在这项工作中，我们介绍了NeQA，这是一个由带否定的问题组成的数据集，其中的语言模型并不表现出直接的正扩展。我们的研究表明，这项任务可以表现出反向缩放、U型缩放或正向缩放，而且随着我们使用更强大的提示方法或模型族，这三种缩放趋势会依次发生变化。我们假设NeQA的解决取决于两个子任务：问题解答（任务1）和否定理解（任务2）。我们发现，任务1是线性缩放，而任务2是带有一个出现的过渡点的sigmoid形缩放，将这两个缩放趋势组合起来就得到了NeQA的最终缩放趋势。我们的工作揭示并提供了一种分析语言模型复杂缩放趋势的方法。

    **关键词**：数据集；缩放趋势

1. ***[Towards Reasoning in Large Language Models: A Survey](https://aclanthology.org/2023.findings-acl.67.pdf)***        

    **摘要**：推理是人类智能的一个基本方面，在问题解决、决策制定和批判性思维等活动中发挥着至关重要的作用。近年来，大型语言模型（LLMs）在自然语言处理领域取得了重大进展，有观察表明，当这些模型足够大时，它们可能表现出推理能力。然而，LLM在多大程度上具有推理能力尚不清楚。本文全面概述了当前关于LLMs推理的知识状况，包括改进和激发这些模型推理的技术、评估推理能力的方法和基准、该领域先前研究的发现和意义以及对未来方向的建议。我们的目标是对这一主题进行详细和最新的综述，并激发有意义的讨论和未来的工作。

    **关键词**：综述；推理能力

1. ***[What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning](https://aclanthology.org/2023.findings-acl.527.pdf)***        

    **摘要**：大型语言模型（LLMs）利用上下文学习（ICL）来解决仅有少量演示的任务，但其机制尚未得到很好的理解。一些研究表明，LLMs只回忆预训练中已经学习过的概念，而另一些研究则暗示ICL对演示进行隐式学习。我们描述了ICL利用演示的两种方式。任务识别（TR）是指LLM能够通过演示（即使没有正确的标签）识别任务并应用其预先训练的前验的程度，而任务学习（TL）是指捕捉在预先训练中未见的新输入-标签映射的能力。利用广泛的分类数据集和三个LLM系列（GPT-3、LLaMA和OPT），我们设计了对照实验来区分TR和TL在ICL中的作用。我们发现：(1)模型仅靠TR就能获得非同一般的性能，而TR并不会随着模型的增大或演示的增多而进一步提高；(2)LLM会随着模型的扩展而获得TL，而TL的性能会随着上下文演示的增多而持续提高。我们的发现揭示了ICL背后的两种不同力量，由于它们的不同性质，我们主张在未来的ICL研究中对它们进行区分。

    **关键词**：上下文学习；任务识别；任务学习

1. ***[Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning](https://aclanthology.org/2023.findings-acl.668.pdf)***        

    **摘要**：使用大型语言模型的思维链（CoT）提示在许多自然语言处理任务中被证明是有效的，但是设计能够很好地泛化到不同问题类型的提示可能是具有挑战性的，特别是在数学单词问题求解的背景下。此外，通常会有大量的训练数据，这些数据具有较好的多样性覆盖，但却没有CoT注释，这限制了监督学习技术的使用。为了解决这些问题，我们研究了两种方法，以充分利用少量提示场景中的训练数据：*动态程序提示*和*程序提炼*。我们的方法在很大程度上受到了Gao等人(2022)的启发，他们提出了用程序代替CoT作为中间推理步骤。这种提示策略使我们能够在MWP求解中通过执行程序准确验证答案的正确性。我们的动态程序提示包括通过从大型语言模型中抽取正确的程序来注释训练数据，而程序提炼则包括根据程序注释的训练数据调整较小的模型。我们在三个标准MWP数据集上进行的实验证明了这些方法的有效性，在提示和微调方面比以前的基线方法有了显著改进。我们的结果表明，利用大量的训练数据可以提高提示的泛化能力，并提升微调后的较小模型在MWP求解中的性能。

    **关键词**：思维链；动态程序提示

1. ***[How does the task complexity of masked pretraining objectives affect downstream performance?](https://aclanthology.org/2023.findings-acl.669.pdf)***        

    **摘要**：掩码语言建模（MLM）是一种广泛使用的自监督预训练目标，其中模型需要预测被给定上下文的掩码替换的原始标记。尽管更简单且计算效率更高的预训练目标（例如预测屏蔽标记的第一个字符）最近显示出了与MLM相当的结果，但在下游任务中，没有任何具有屏蔽方案的目标实际优于MLM。我们假定复杂性的缺乏是导致性能下降的重要原因，在此基础上，我们验证了更复杂的掩码目标是否能够取得更好的结果，并研究了它们应该具有多少复杂性才能在性能上与MLM相媲美。我们使用GLUE、SQuAD和Universal Dependencies基准得出的结果表明，更复杂的目标往往会显示出更好的下游结果，至少需要MLM复杂度的一半才能实现与MLM相当的性能。最后，我们从任务复杂度的角度讨论了如何使用掩蔽目标对模型进行预训练。

    **关键词**：掩码语言建模

1. ***[Masked Latent Semantic Modeling: an Efficient Pre-training Alternative to Masked Language Modeling](https://aclanthology.org/2023.findings-acl.876.pdf)***        

    **摘要**：在本文中，我们提出了一种经典的掩码语言建模（MLM）预训练范式的替代方案，其目标从重建随机选择的掩蔽子词的确切身份转变为预测其潜在语义属性。我们将所提出的预训练技术命名为掩蔽潜在语义建模（简称MLSM）。为了能够根据上下文确定被屏蔽子词的潜在语义属性，我们采用了一种使用稀疏编码的无监督技术。我们的实验结果表明，我们通过MLSM预训练的模型的微调性能持续显著优于使用虚MLM预训练和其他强基线。

    **关键词**：掩蔽潜在语义建模；稀疏编码

1. ***[Large Language Models with Controllable Working Memory](https://aclanthology.org/2023.findings-acl.112.pdf)***        

    **摘要**：大型语言模型（LLMs）在自然语言处理（NLP）领域取得了一系列突破，部分原因在于它们在预训练期间记忆了大量的世界知识。尽管许多下游应用为模型提供了信息上下文以帮助其完成基本任务，但模型的世界知识如何与上下文中呈现的事实信息进行交互仍有待探索。作为一种理想的行为，当上下文中包含的任务相关信息与模型记忆的知识相冲突时，LLM应该优先考虑上下文。这使得模型预测能够以上下文为基础，从而便于更新特定的模型预测，而无需频繁地重新训练模型。与此相反，当上下文与任务无关时，模型应忽略上下文，并依赖于其内部知识。在本文中，我们首次联合研究了LLMs的上述两个特性，即可控性和鲁棒性。我们证明了最先进的T5和PaLM模型（包括预训练和微调模型）可能表现出较低的可控性和鲁棒性，并且不会随着模型规模的增加而改善。作为解决方案，我们提出了一种简单而有效的方法--知识感知微调（KAFT）--通过向标准监督数据集注入反事实和无关上下文来增强可控性和鲁棒性。我们的综合评估展示了KAFT在不同模型架构和规模下的实用性。

    **关键词**：可控性与鲁棒性；知识感知微调

1. ***[Scaling Laws for BERT in Low-Resource Settings](https://aclanthology.org/2023.findings-acl.492.pdf)***        

    **摘要**：大型语言模型是资源密集型的，无论是经济上还是环境上，并且需要大量的训练数据，这对于大多数NLP从业者来说是根本无法获得的。以前的工作已经研究了此类模型的扩展规律，但模型参数、数据集大小和计算成本的最佳比例主要集中在大规模上。相比之下，我们通过建立三个轻量级BERT模型（16M/51M/124M参数），并在一组小型语料（5M/25M/125M单词）上进行训练，分析了这些变量在受限环境下对语言模型性能的影响。我们在四种不同语言特点的语言（巴斯克语、西班牙语、斯瓦希里语和芬兰语）上进行了实验，并在MLM和若干NLU任务上对模型进行了评估。我们得出的结论是，低资源环境下的参数、数据和计算的幂律与之前推断的最佳比例律不同，对数据的要求应该更高。我们的见解在我们研究的所有语言以及MLM和下游任务中都是一致的。此外，我们通过实验确定了使用基于transformer的方法的成本何时值得承担，而不是倾向于其他计算量更小的解决方案。

    **关键词**：低资源计算

1. ***[The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code](https://aclanthology.org/2023.findings-acl.574.pdf)***        

    **摘要**：因果推理，即识别因果关系的能力，在人类思维中至关重要。尽管大型语言模型（LLMs）在许多NLP任务中取得了成功，但它们在进行复杂的因果推理（如归纳推理和反事实推理）时仍然面临挑战。鉴于编程代码可以通过 "if "等条件语句更频繁、更明确地表达因果关系，我们希望探索代码大型语言模型是否能够获得更好的因果推理能力。我们的实验表明，与纯文本LLMs相比，有代码提示的Code-LLMs是更好的因果推理者。我们进一步从不同方面对提示进行干预，发现关键点在于编程结构。代码和数据可在<https://github.com/xxxiaol/magic-if>。

    **关键词**：因果推理

1. ***[Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale.](https://aclanthology.org/2023.findings-acl.326.pdf)***        

    **摘要**：近年来，语言模型的规模急剧扩大，这些模型的能力也随着规模的扩大而提高。最近的缩放规律研究大多集中在高计算量高参数量的设置上，对于这些能力何时开始显现的问题基本上没有答案。在本文中，我们研究了当问题规模缩小时，是否可以观察到预训练的效果，即模拟更小、词汇量更少的语言。我们展示了在小到1.25M参数的模型中使用掩码语言建模（MLM）目标进行预训练的好处，并在预训练的困惑度和下游性能（GLUE基准）之间建立了很强的相关性。我们研究了降尺度效应，将尺度法则扩展到小到约1M参数的模型。在这种规模下，我们观察到计算最优模型的幂律被打破，并表明MLM损失在低于$2.2 \times 10^{15} FLOPs$时不会随计算成本（FLOPs）平滑扩展。我们还发现，增加层数并不总是有利于下游性能。

    **关键词**：最优模型幂律

1. ***[Attribute Controlled Dialogue Prompting](https://aclanthology.org/2023.findings-acl.150.pdf)***        

    **摘要**：提示微调（Prompt-tuning）已成为一种日益流行的参数高效方法，用于将大型预训练语言模型适应下游任务。然而，离散提示和连续提示都假设一个任务中的所有数据样本都有固定的提示，而忽略了在一些任务（如开放域对话生成）中输入变化很大的事实。在本文中，我们为对话生成提出了一种新颖的、针对特定实例的提示微调算法。具体来说，我们基于实例级控制代码而非对话历史生成提示，以探索其对可控对话生成的影响。在流行的开放域对话数据集上进行的实验表明，我们的方法优于提示基线，并可与仅占总参数5%-6%的微调相媲美。

    **关键词**：提示微调；实例级控制代码

1. ***[Rethinking Semi-supervised Learning with Language Models](https://aclanthology.org/2023.findings-acl.347.pdf)***        

    **摘要**：半监督学习（SSL）是一种流行的学习方法，旨在有效利用无标签数据来提高下游自然语言处理（NLP）任务中的模型性能。目前，有两种流行的方法来利用无标签数据：自我训练（ST）和任务自适应预训练（TAPT）。自我训练（ST）使用教师模型为无标签数据分配伪标签，而任务自适应预训练（TAPT）则在微调之前继续对无标签数据进行预训练。据我们所知，TAPT在SSL任务中的有效性还没有得到系统的研究，以前的工作也没有直接比较过TAPT和ST在利用未标记数据池方面的能力。在本文中，我们进行了广泛的实证研究，比较了五种最先进的ST方法和TAPT在各种NLP任务和数据规模（包括域内和域外设置）中的应用。令人惊讶的是，我们发现，与更复杂的ST方法相比，TAPT是一种更强大、更稳健的SSL学习器，即使只使用几百个未标记样本或存在领域偏移时也是如此，而且在SSL中往往比在完全监督设置中带来更大的改进。我们的进一步分析表明，当标记或非标记数据规模较小或存在领域偏移时，使用ST方法存在风险，并强调TAPT是一种潜在的解决方案。

    **关键词**：半监督学习；自我训练；任务自适应预训练

1. ***[Code Execution with Pre-trained Language Models](https://aclanthology.org/2023.findings-acl.308.pdf)***        

    **摘要**：代码执行是编程语言语义的一个基本方面，它反映了代码的确切行为。然而，大多数用于代码智能的预训练模型忽略了执行跟踪，而仅仅依赖于源代码和语法结构。在本文中，我们研究了预训练模型理解和执行代码的能力。我们开发了一种基于突变的数据增强技术，以创建一个大规模、真实的Python数据集和代码执行任务，这对Codex等现有模型提出了挑战。然后，我们介绍了CodeExecutor，这是一个Transformer模型，它利用代码执行预训练和课程学习来增强其语义理解能力。我们对CodeExecutor的代码执行进行了评估，并展示了其良好的性能和局限性。我们还展示了它在代码智能任务中的潜在优势，如zero-shot代码到代码搜索和文本到代码生成。我们的分析为代码执行预训练模型的学习和泛化能力提供了见解。

    **关键词**：代码执行任务

1. ***[Large Language Models are Built-in Autoregressive Search Engines](https://aclanthology.org/2023.findings-acl.167.pdf)***        

    **摘要**：文档检索是标准网络搜索引擎的关键阶段。现有的双编码器密集检索器独立获取问题和文档的表示，只允许它们之间的浅层交互。为了克服这一限制，最近的自回归搜索引擎取代了双编码器架构，直接生成候选池中相关文档的标识符。然而，随着候选文档数量的增加，这种自回归搜索引擎的训练成本急剧上升。在本文中，我们发现大语言模型（LLM）可以按照人类的指令直接生成用于文档检索的URL。令人惊讶的是，当提供一些{Query-URL}对作为上下文演示时，LLMs可以生成Web URL，其中近90%的对应文档包含开放域问题的正确答案。通过这种方式，LLM可以被视为内置的搜索引擎，因为它们没有经过明确的训练来将问题映射到文档标识符。实验证明，在三个开放域问题解答基准测试中，我们的方法可以在zero-shot和few-shot访问设置下持续获得比现有检索方法更好的检索性能。这项工作的代码可以在<https://github.com/Ziems/llm-url>上找到。

    **关键词**：文档检索

1. ***[Pre-training Language Model as a Multi-perspective Course Learner](https://aclanthology.org/2023.findings-acl.9.pdf)***        

    **摘要**：ELECTRA作为生成器-判别器预训练框架，在各种下游任务中取得了令人瞩目的语义构建能力。尽管ELECTRA的表现令人信服，但它仍然面临着训练单调和交互不足的挑战。生成器仅具有遮蔽语言建模（MLM），导致鉴别器的学习偏差和标签不平衡，降低了学习效率；鉴别器和生成器之间没有明确的反馈环路，导致这两个组件之间存在鸿沟，未能充分利用课程学习。本研究提出了一种多视角课程学习（MCL）方法，以获取多视角和多视度的样本进行高效的预训练，并充分利用生成器和判别器之间的关系。具体来说，设计了三个自监督课程，以缓解MLM的固有缺陷，并以多角度的方式平衡标签。此外，我们还提出了两个自我修正课程，通过创建一个用于二次监督的 "修正笔记本 "来弥合两个编码器之间的鸿沟。此外，还进行了课程汤试验，以解决MCL的 "拉锯战 "动力学问题，进化出更强的预训练模型。实验结果表明，在GLUE和SQuAD 2.0基准上，我们的方法将ELECTRA的平均性能分别显著提高了2.8%和3.2%个绝对点，并在相同设置下超越了近期先进的ELECTRA式模型。预训练的MCL模型开源在<https://huggingface.co/McmanusChen/MCL-base>。

    **关键词**：生成器；判别器；多视角课程学习

1. ***[Better Zero-Shot Reasoning with Self-Adaptive Prompting](https://aclanthology.org/2023.findings-acl.216.pdf)***        

    **摘要**：现代大型语言模型(LLM)已经在复杂的任务中表现出令人印象深刻的能力，通常是通过类似于人类的逐步推理。这得益于大型语言模型强大的few-shot和zero-shot推理能力--它们可以有效地从少数几个手工制作的完整回答（"上下文示例"）中学习，或者通过专门设计的触发器自发地进行推理。然而，我们也发现了一些局限性。首先，在 "少量实例 "设置中的性能对实例的选择非常敏感，而实例的设计需要大量的人力。此外，考虑到LLM的下游任务多种多样，手工制作每个任务的标签可能比较困难或费力。其次，虽然 "zero-shot "设置不需要手工制作，但由于缺乏对LLM的指导，其性能受到限制。为了解决这些局限性，我们提出了基于一致性的自适应提示（COSP），这是一种新颖的LLM提示设计方法。COSP既不需要手工制作的回答，也不需要正确标签，而是通过精心设计的标准，结合一致性、多样性和重复性，从LLM的zero-shot输出中选择和建立示例集。在针对三种不同LLM的zero-shot设置中，我们发现，与zero-shot基线相比，仅使用LLM预测，COSP就能显著提高性能达15%，并且在一系列推理任务中，COSP的性能都能达到或超过少次拍摄基线。

    **关键词**：自适应提示

1. ***[I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors](https://aclanthology.org/2023.findings-acl.465.pdf)***        

    **摘要**：视觉隐喻是一种强大的修辞手段，用于通过图像说服或传达创造性思想。与语言隐喻类似，视觉隐喻通过象征和并置符号隐含地传达意义。我们提出了从语言隐喻生成视觉隐喻的新任务。对于基于扩散的文本到图像模型（如DALL$\cdot$E 2）来说，这是一项具有挑战性的任务，因为它要求能够对隐含意义和构成性进行建模。我们建议通过大语言模型（LLM）和扩散模型之间的协作来解决这一任务：GPT-3（davinci-002）通过 "思维链"（Chain-of-Thought）提示生成文本，该文本代表了对包含隐含意义和相关对象的语言隐喻的可视化阐述，然后将其作为基于扩散的文本到图像模型的输入。利用人机协作框架（人与LLM和性能最佳的扩散模型进行交互），我们创建了一个高质量的数据集，其中包含1,540个语言隐喻的6,476个视觉隐喻及其相关的视觉阐述。为了评估我们的人类-人工智能协作框架的实用性和数据集的质量，我们进行了基于人类的内在评估和使用视觉引申作为下游任务的外在评估。

    **关键词**：视觉隐喻；语言隐喻；数据集

1. ***[Recipes for Sequential Pre-training of Multilingual Encoder and Seq2Seq Models](https://aclanthology.org/2023.findings-acl.598.pdf)***        

    **摘要**：预训练的纯编码器模型和序列到序列（seq2seq）模型各有优势，但从头开始训练这两种类型的模型计算成本高昂。我们探讨了通过从一个模型初始化另一个模型来提高预训练效率的方法。(1)从seq2seq模型中提取编码器，我们发现它的性能低于掩码语言建模(MLM)编码器，特别是在序列标记任务中。在seq2seq训练期间掩码的变化、减小解码器大小以及继续少量的MLM训练并不能缩小差距。(2)相反，使用编码器来热启动seq2seq训练，我们表明，通过在训练中途解冻编码器，我们可以与从头开始的seq2seq模型的任务性能相匹配。总之，这种两阶段的方法是获得多语言编码器和seq2seq模型的有效方法，与从头开始训练每个模型的性能相匹配，同时将总计算成本降低了27%。

    **关键词**：热启动训练

1. ***[Membership Inference Attacks against Language Models via Neighbourhood Comparison](https://aclanthology.org/2023.findings-acl.719.pdf)***        

    **摘要**：成员推理攻击（MIAs）旨在预测一个数据样本是否存在于机器学习模型的训练数据中，被广泛用于评估语言模型的隐私风险。现有的大多数攻击依赖于这样一种观察：模型倾向于为其训练样本赋予比非训练点更高的概率。然而，孤立地对模型得分进行简单的阈值化处理往往会导致较高的假阳性率，因为它没有考虑到样本的内在复杂性。最近的工作表明，基于参考的攻击将模型得分与在类似数据上训练的参考模型获得的得分进行比较，可以大大提高MIA的性能。然而，为了训练参考模型，这类攻击提出了一个强烈的、可以说是不切实际的假设，即对手能够获得与原始训练数据非常相似的样本。因此，我们研究了它们在更现实场景中的性能，发现它们在与用于训练参考模型的数据分布相关的情况下非常脆弱。为了研究这种脆弱性是否提供了一层安全保障，我们提出并评估了邻域攻击，这种攻击将给定样本的模型得分与合成生成的邻域文本得分进行比较，因此无需访问训练数据分布。我们的研究表明，我们的攻击不仅能够与完全了解训练数据分布的基于参考的攻击相抗衡，而且明显优于现有的无参考攻击以及基于参考的不完全了解攻击，这表明有必要重新评估对抗攻击的威胁模型。

    **关键词**：成员推理攻击；隐私风险；领域攻击

1. ***[Evaluating the Factual Consistency of Large Language Models Through News Summarization](https://aclanthology.org/2023.findings-acl.322.pdf)***        

    **摘要**：虽然大型语言模型（LLMs）已被证明能有效完成各种任务，但它们也会产生信息幻觉。为了衡量 LLM 是否更喜欢对其输入信息进行事实一致的延续，我们提出了一个名为 FIB（事实不一致基准）的新基准，该基准侧重于总结任务。具体来说，我们的基准是比较 LLM 对输入新闻文章的事实一致摘要和事实不一致摘要的评分。对于事实一致的摘要，我们使用人工编写的参考摘要，并将其手动验证为事实一致。要生成与事实不符的摘要，我们会从一套人工标注为与事实不符的摘要模型中生成摘要。模型的事实一致性是根据其准确性来衡量的，也就是说，它给事实一致性摘要打分较高的文档所占的比例。为了验证FIB的实用性，我们评估了包括BLOOM和OPT在内的6个不同模型系列的23个大型语言模型，其参数从1B到176B不等。我们发现，与事实不一致的摘要相比，现有的 LLM 通常会给事实一致的摘要更高的分数。但是，如果事实不一致的摘要在文档中逐字出现，那么 LLM 对这些事实不一致摘要的评分就会高于事实一致摘要。我们验证了基准中的设计选择，包括评分方法和分心摘要的来源。

    **关键词**：信息幻觉；基准；事实不一致

2. ***[Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model](https://aclanthology.org/2023.findings-acl.35.pdf)***        

    **摘要**：预训练语言模型在各种自然语言处理任务中取得了显著的成功。然而，预训练最近转向了更大的模型和更大的数据，这导致了巨大的计算和能源成本。在本文中，我们提出了语言模型的影响子集选择（ISS），它明确地利用终端任务知识来选择预训练语料库中的一个微小子集。具体来说，ISS选择的样本将对最终任务的表现产生最积极的影响。此外，我们还设计了一种基于梯度匹配的影响估计方法，该方法可以大大减少影响的计算时间。ISS只需0.45%的数据，计算成本却降低了3个数量级，在涵盖4个领域的8个数据集上的表现优于预训练模型（如RoBERTa）。

    **关键词**：影响子集选择；终端任务知识

3. ***[Do Large Language Models Know What They Don't Know?](https://aclanthology.org/2023.findings-acl.551.pdf)***        

    **摘要**：大型语言模型(LLM)拥有丰富的知识，使其在各种自然语言处理(NLP)任务中表现出色。目前的研究重点是在其现有知识范围内提高其性能。尽管LLM拥有丰富的知识，但它们能够容纳和理解的信息量仍然有限。因此，了解自身在未知领域的局限性（即自知之明）的能力至关重要。本研究旨在通过评估LLM识别无法回答或不可知问题的能力来评估他们的自知之明。我们引入了一种自动方法来检测这些模型回答中的不确定性，从而为他们的自知之明提供了一种新的衡量标准。我们进一步介绍了一个独特的数据集SelfAware，该数据集由五个不同类别的无法回答的问题及其可回答的问题组成。我们对包括GPT-3、InstructGPT和LLaMA在内的20个LLM进行了广泛的分析，发现了这些模型内在的自知能力。此外，我们还证明了情境学习和指令调整可以进一步增强这种自知能力。尽管这种见解很有前途，但我们的发现也凸显了这些模型的能力与人类识别其知识局限性的能力之间存在相当大的差距。

    **关键词**：LLM的自知之明；数据集

4. ***[The Larger they are, the Harder they Fail: Language Models do not Recognize Identifier Swaps in Python](https://aclanthology.org/2023.findings-acl.19.pdf)***        

    **摘要**：大型语言模型（LLMs）已成功应用于代码生成任务，这就提出了这些模型如何理解编程的问题。典型的编程语言在语义上具有人类程序员能够直观理解和利用的不变性和等价性，例如标识符重命名的（近似）不变性。我们发现，当默认函数名被调换时，LLMs不仅不能正确生成正确的Python代码，而且其中一些LLMs甚至会随着模型规模的增大而对其错误预测更加自信，这就是最近发现的反向缩放现象（Inverse Scaling），与通常观察到的预测质量随模型规模增大而提高的趋势背道而驰。我们的研究结果表明，尽管LLMs具有惊人的典型案例性能，但它们仍然缺乏对所处理内容的深刻、抽象的理解，这使得它们不适合处理在统计上偏离训练数据的任务，而且仅仅是扩展还不足以实现这种能力。

    **关键词**：代码生成；反向缩放

5. ***[HuaSLIM: Human Attention Motivated Shortcut Learning Identification and Mitigation for Large Language models](https://aclanthology.org/2023.findings-acl.781.pdf)***        

    **摘要**：大型语言模型在各种NLP任务中取得了显著进展。然而，人们发现，这些模型往往依赖于与标签虚假相关的捷径特征进行预测，这削弱了它们在分布外样本上的泛化能力。在本文中，我们提出了一种以人类注意力为导向的方法来识别和减少捷径学习，从而鼓励基于LLM的目标模型学习相关特征。我们定义了一种基于注意力的测量方法来捕捉模型和数据偏差，并通过探索人类和神经注意力来识别捷径标记。在自蒸馏框架中，我们根据检测到的捷径标记和估计的捷径程度动态调整蒸馏温度，从而减轻捷径学习的影响。此外，我们利用人类注意力作为监督信号，约束大型语言模型更多地关注相关标记。在多个NLP任务上的实验结果表明，我们提出的方法可以有效地识别快捷词，并显著提高大型语言模型在OOD样本上的鲁棒性，同时不会降低在IID数据上的性能。

    **关键词**：人类注意力；捷径学习；自蒸馏

6. ***[HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models](https://aclanthology.org/2023.findings-acl.751.pdf)***        

    **摘要**：控制语言模型生成的文本和定制内容一直是一个长期的挑战。为提供控制而提出的现有提示技术都是针对特定任务的，缺乏通用性；这为非专业用户找到适合其任务的方法提供了大量选择。与这些技术相关的工作，如编写示例、解释、说明等，进一步限制了它们在非专业用户中的应用。在本文中，我们提出了一种简单的提示策略Help Me Think，我们鼓励大型语言模型（如GPT3和ChatGPT）通过提出一系列相关问题来帮助非专业用户，并利用用户的回答来执行任务。我们在各种任务中展示了我们的技术Help Me Think的功效。具体而言，我们将重点放在对普通人而言难度较大、需要大量思考才能完成的任务上。我们希望我们的工作能够鼓励开发利用大型语言模型的非传统方法。

    **关键词**：提示策略；非专业用户

7. ***[Nonparametric Masked Language Modeling](https://aclanthology.org/2023.findings-acl.132.pdf)***        

    **摘要**：现有的语言模型（LMs）在有限的词汇量上用软最大值（softmax）预测词组，这使得预测罕见词组或短语变得困难。我们引入了NPM，它是第一个非参数掩码语言模型，用参考语料库中每个短语的非参数分布取代了软最大值。NPM仅通过从文本语料库中检索标记来填充[MASK]。我们证明，NPM可以通过对比目标和批量近似全语料库检索进行高效训练。在16项任务（包括分类、事实探查和问题解答）上进行的zero-shot评估表明，无论是采用还是不采用检索-生成方法，NPM都明显优于更大的参数模型。它在处理罕见模式（词义或事实）和预测罕见或几乎未见过的词（例如非拉丁字母）方面尤其出色。我们在<github.com/facebookresearch/NPM>上发布了该模型和代码。

    **关键词**：非参数掩码语言模型；罕见短语

8. ***[Residual Prompt Tuning: improving prompt tuning with residual reparameterization](https://aclanthology.org/2023.findings-acl.421.pdf)***        

    **摘要**：提示微调是对预训练语言模型进行参数高效调优的成功方法之一。尽管它可以说是参数效率最高的方法（微调后的软提示占总参数的<0.1%），但它的性能通常比其他高效的调整方法差，并且对超参数相当敏感。在这项工作中，我们引入了残差提示调优——一种简单高效的方法，可显著提高提示微调的性能和稳定性。我们建议使用具有残差连接的浅层网络对软提示嵌入进行重新参数化。我们的实验表明，在T5-Large、T5-Base和BERT-Base模型中，残差提示微调明显优于提示微调。值得注意的是，在T5-Base模型的SuperGLUE基准上，我们的方法比提示微调方法提高了7个百分点，并且可以将提示长度减少10倍而不影响性能。此外，我们还证明了我们的方法对学习率和提示语初始化的选择具有鲁棒性，并且在少数几个提示语的设置中是有效的。

    **关键词**：残差提示微调

9. ***[Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming](https://aclanthology.org/2023.findings-acl.191.pdf)***        

    **摘要**：尽管在规模和组合性方面取得了进步，但预先训练的大型语言模型（LMs）仍难以可靠地执行逻辑推理。在这项工作中，我们从符号编程的角度来应对这一挑战。我们提出了DSR-LM，这是一个可微分的符号推理框架，其中预训练的LM管理事实知识的感知，而符号模块执行演绎推理。与那些依赖于手工创建逻辑规则的作品相比，我们的可微分符号推理框架能够有效地学习加权规则，并应用语义损失来进一步改进LMs。DSR-LM具有可扩展性、可解释性，并允许轻松集成先验知识，从而支持广泛的符号编程，以稳健地推导出逻辑结论。我们的实验结果表明，DSR-LM提高了预训练语言模型的逻辑推理能力，使演绎推理基准的准确率大幅提高了20%以上。此外，当序列长度发生系统性变化时，DSR-LM的表现优于各种竞争基线。

    **关键词**：逻辑推理；可微分符号推理

10. ***[AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing](https://aclanthology.org/2023.findings-acl.181.pdf)***        

    **摘要**：开发单语大型预训练语言模型（PLM）在处理自然语言处理（NLP）中的不同任务时非常成功。在这项工作中，我们介绍了AraMUS，这是最大的阿拉伯语PLM，在529GB的高质量阿拉伯语文本数据上训练了11B个参数。AraMUS在一系列阿拉伯语分类和生成任务中取得了最先进的性能。此外，与现有最好的阿拉伯语PLM相比，AraMUS显示出令人印象深刻的少量学习能力。

    **关键词**：阿拉伯语预训练模型

11. ***[Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text](https://aclanthology.org/2023.findings-acl.321.pdf)***        

    **摘要**：虽然大型语言模型（LLMs），如GPT-3，似乎是稳健和通用的，但它们的推理能力却无法与为特定自然语言推理问题训练的最佳模型相媲美。在本研究中，我们发现大型语言模型可以作为高效的少量语义解析器。它可以将自然语言句子转换成逻辑形式，作为答案集程序的输入，答案集程序是一种基于逻辑的声明式知识表示形式。这种组合产生了一个强大而通用的系统，它可以处理多个问题解答任务，而无需为每个新任务进行重新训练。它只需要一些例子来指导LLM适应特定任务，以及可重复使用的ASP知识模块，这些知识模块可应用于多个任务。我们证明了该方法在多个NLP基准上取得了最先进的性能，包括bAbI、StepGame、CLUTRR和gSCAN。此外，它还成功地解决了单靠LLM无法解决的机器人规划任务。

    **关键词**：通用推理；少量语义解析器；答案集程序

12. ***[Complementary Explanations for Effective In-Context Learning](https://aclanthology.org/2023.findings-acl.273.pdf)***        

    **摘要**：大型语言模型（LLMs）在从提示中的解释中学习方面表现出了非凡的能力，但是人们对这些解释是如何起作用的或者它们为什么有效的了解还很有限。这项工作旨在更好地理解解释用于语境学习的机制。我们首先研究了两个不同因素对带有解释的提示性能的影响：计算轨迹（解的分解方式）和用于表达提示的自然语言。通过在三个受控任务中对解释进行测试，我们发现这两个因素都有助于提高解释的有效性。我们进一步研究了如何为解决给定的测试查询形成最有效的解释集。我们发现，LLM可以从解释集的互补性中获益：不同示例所展示的多样化推理技能可以带来更好的表现。因此，我们提出了一种基于最大边际相关性的示例选择方法，用于构建既相关又互补的示例集，该方法成功地提高了多个LLMs在三个现实任务中的上下文学习性能。

    **关键词**：解释学习

13. ***[Numeric Magnitude Comparison Effects in Large Language Models](https://aclanthology.org/2023.findings-acl.383.pdf)***        

    **摘要**：大语言模型(LLM)并不能对文本中普遍存在的数字进行差异化表示。与此相反，神经科学研究发现了数字和文字的不同神经表征。在这项工作中，我们研究了流行的LLMs从行为角度捕捉数字大小（例如4<5）的能力。先前关于LLM表征能力的研究评估了它们是否表现出人类水平的性能，例如，在标准基准上的高总体准确性。在这里，我们从认知科学的角度提出了一个不同的问题：LLM的数字表征与人类语言使用者的数字表征有多接近？我们依赖于一个链接假设，将数词和数字模型嵌入之间的相似性映射到人类的反应时间上。结果显示，尽管在人脑中没有直接支持这些表征的神经回路，但不同架构的语言模型之间却存在着惊人的类人表征。这项研究显示了利用行为基准理解LLMs的实用性，并为未来研究LLMs的数字表征及其认知合理性指明了方向。

    **关键词**：数字表征

# Industry

1. ***[A Static Evaluation of Code Completion by Large Language Models](https://aclanthology.org/2023.acl-industry.34.pdf)***        

    **摘要**：在代码上训练的大型语言模型在提高软件开发人员的工作效率方面显示出巨大的潜力。已经提出了几种基于执行的基准来评估模型生成的代码在简单编程问题上的功能正确性。然而，考虑到执行成本，在复杂的实际项目中执行相同的评估是昂贵的。另一方面，静态分析工具（如衬砌器）可以在不运行程序的情况下检测错误，但还没有被很好地用于评估代码生成模型。在这项工作中，我们提出了一个静态评估框架，利用抽象语法树量化Python代码完成中的静态错误。与基于执行的评估相比，我们的方法不仅更加高效，而且适用于野生代码。在实验中，我们从开源仓库中收集代码上下文，使用公共模型生成一百万个函数体。我们的静态分析表明，未定义名称（Undefined Name）和未使用变量（Unused Variable）是语言模型最常见的错误。通过广泛的研究，我们还展示了采样温度、模型大小和上下文对代码完成中静态错误的影响。

    **关键词**：静态评估框架

1. ***[MathPrompter: Mathematical Reasoning using Large Language Models](https://aclanthology.org/2023.acl-industry.4.pdf)***        

    **摘要**：大语言模型（LLM）在解决算术推理任务时性能有限，经常提供错误答案。与自然语言理解不同，数学问题通常只有一个正确答案，这使得生成准确答案的任务对大型语言模型来说更具挑战性。据我们所知，我们还没有发现任何LLM能够表明他们对自己的回答的信任程度，这加剧了这些模型的信任缺失，阻碍了它们的应用。为了解决这一缺陷，我们提出了 "MathPrompter"，这是一种能够提高LLM在算术问题上的性能并增加预测可信度的技术。MathPrompter使用zero-shot思维链提示技术生成多个代数表达式或python函数，以不同的方式解决相同的数学问题，从而提高输出结果的置信度。这与其他基于提示的CoT方法形成了鲜明对比，后者不检查所遵循的中间步骤的有效性。在基于175B参数GPT的LLM评估中，我们的技术在 "MultiArith "数据集上的表现优于最先进的技术（78.7%→92.5%）。

    **关键词**：算术推理；信任缺失

1. ***[KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications](https://aclanthology.org/2023.acl-industry.21.pdf)***        

    **摘要**：大语言模型（LLM）不仅可以学习自然文本生成能力，还可以从现实世界的数据中学习针对不同人口群体的社会偏见。在部署基于LLM的应用时，这会带来严重的风险。由于语言和文化的差异，现有的研究和资源并不适用于韩国，而语言和文化的差异会对偏见和目标人群产生重大影响。这种限制需要本地化的社会偏见数据集，以确保安全有效地部署LLM。为此，我们提出了KosBi，这是一个新的社会偏见数据集，包含34k对韩语语境和句子，涵盖15个类别的72个人口群体。我们发现，通过基于过滤的调节，HyperClova（30B和82B）和GPT-3生成内容中的社会偏见平均可减少16.47%。

    **关键词**：社会偏见；数据集

# Workshop

1. ***[Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values](https://aclanthology.org/2023.acl-srw.37.pdf)***        

    **摘要**：本文提出了一种基于采样链的方法，使Shapley值在大型语言模型的数据评估和选择中具有计算上的可行性。

1. ***[Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity](https://aclanthology.org/2023.acl-srw.40.pdf)***        

    **摘要**：本文利用道德基础理论的工具，研究了基于政治意识形态的LLM道德偏好是否重复了社会科学研究中的已知结果。

# Demo

1. ***[Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation](https://aclanthology.org/2023.acl-demo.27.pdf)***        

    **摘要**：信息提取系统通常会产生成百上千条关于特定主题的字符串。我们提出了一种方法，可以帮助用户更好地利用这些字符串，在这种情况下，用户既希望获得可用信息的总体概览，又希望有机会深入了解某些方面的信息。该系统的工作原理是将相似的条目分组，并将剩余的条目排列成一个可分层浏览的DAG结构。我们将该方法应用于医学信息提取。

    **关键词**：信息提取；医学

1. ***[The ROOTS Search Tool: Data Transparency for LLMs](https://aclanthology.org/2023.acl-demo.29.pdf)***        

    **摘要**：ROOTS是一个1.6TB的多语言文本语料库，用于训练BLOOM，是目前最大的语言模型。作为这些努力的延续，我们推出了ROOTS搜索工具：一个在整个ROOTS语料库上提供模糊和精确搜索功能的搜索引擎。ROOTS是迄今为止能够以这种方式进行研究的最大的语料库。ROOTS搜索工具开源于Hugging Face Spaces：<https://huggingface.co/spaces/bigscience-data/roots-search>。我们将描述我们的实现和我们工具的可能用例。

    **关键词**：语料库；搜索工具

1. ***[Petals: Collaborative Inference and Fine-tuning of Large Models](https://aclanthology.org/2023.acl-demo.54.pdf)***        

    **摘要**：许多NLP任务都受益于使用大型语言模型（LLM），这些模型通常拥有超过1000亿个参数。随着BLOOM-176B和OPT-175B的发布，每个人都可以下载这种规模的预训练模型。不过，使用这些模型需要高端硬件，而许多研究人员都无法获得这些硬件。在某些情况下，可以通过RAM卸载或托管API以更低廉的价格使用LLM。然而，这些技术有其固有的局限性：卸载对于交互式推理来说太慢，而API对于需要访问权重、注意力或对数的研究来说不够灵活。在这项工作中，我们提出了Petals--一个通过联合多方资源协同推断和微调大型模型的系统。我们证明了这一策略优于超大模型的卸载，在消费级GPU上以每秒≈1步的速度运行BLOOM-176B的推理，这对于许多交互式LLM应用来说已经足够了。与大多数推理API不同，Petals还原生公开了已服务模型的隐藏状态，允许基于高效微调方法训练和共享自定义模型扩展。该系统、其源代码和文档可在以下网址获得 https://petals.ml 视频（2分钟）：https://youtu.be/F4muLI-0hTE

    **关键词**：推理与微调加速

1. ***[Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models](https://aclanthology.org/2023.acl-demo.4.pdf)***        

    **摘要**：基于transformer的预训练语言模型在各种基准测试中表现优异，因此越来越受欢迎。然而，人们仍然担心这些模型中存在隐藏的偏见，这会导致歧视性结果并强化有害的刻板印象。为了解决这个问题，我们提出了Finspector，这是一种以人为中心的可视化检查工具，旨在通过语言模型生成的对数似然得分检测不同类别中的偏见。该工具的目标是使研究人员能够通过可视化分析轻松识别潜在的偏见，最终促进这些模型在学术和工业环境中更公平、更公正的部署。Finspector代码仓库在<https://github.com/IBM/finspector>。

    **关键词**：可视化检查工具；偏见识别

1. ***[LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models](https://aclanthology.org/2023.acl-demo.11.pdf)***        

    **摘要**：支持用户自动创建可视化的系统必须解决几个子任务--理解数据语义、列举相关可视化目标和生成可视化规范。在这项工作中，我们将可视化生成视为一个多阶段生成问题，并认为基于大型语言模型（LLM）和图像生成模型（IGM）的协调良好的管道适合于解决这些任务。我们介绍了LIDA，一种用于生成语法无关的可视化和信息图表的新型工具。LIDA由4个模块组成--SUMMARIZER（将数据转换为丰富而紧凑的自然语言
    **摘要）、GOAL EXPLORER（根据数据列举可视化目标）、VISGENERATOR（生成、完善、执行和过滤可视化代码）和INFOGRAPHER（使用IGM生成忠实于数据的风格化图形）。LIDA为交互式图表、信息图表和数据故事生成提供了一个python api和一个混合用户界面（直接操作和多语言自然语言）。代码和演示可从以下网址获得 - https://microsoft.github.io/lida/

    **关键词**：可视化生成；新型工具

1. ***[OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models](https://aclanthology.org/2023.acl-demo.26.pdf)***        

    **摘要**：由于与全参数微调相关的高优化开销和存储成本，大型预训练模型（PTM）的规模给适应下游任务带来了巨大挑战。为了解决这个问题，许多研究探索了参数效率微调方法，在Ding等人（2022年）的研究中也被称为 "delta微调"，这种方法只更新一小部分参数，称为 "delta模块"，同时保持骨干模型的参数固定。然而，由于现有的实现直接修改骨干PTM的代码，并为每个PTM硬编码特定的delta微调方法，delta微调的实用性和灵活性受到了限制。在本文中，我们介绍了OpenDelta，这是一个开源库，通过提供各种delta微调方法的即插即用实现来克服这些限制。我们的新技术消除了修改骨干PTM代码的需要，使得OpenDelta与不同的甚至新的PTM兼容。OpenDelta被设计成简单、模块化和可扩展的，为研究人员和从业人员高效地微调大型PTM提供了一个全面的平台。

    **关键词**：高效微调；即插即用；开源库
