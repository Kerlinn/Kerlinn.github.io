# 正确认识ChatGPT

## ChatGPT原理剖析

![image-20230904153422331](机器学习【2023春】李宏毅.assets/image-20230904153422331.png)

![image-20230904153531874](机器学习【2023春】李宏毅.assets/image-20230904153531874.png)

![image-20230904153623030](机器学习【2023春】李宏毅.assets/image-20230904153623030.png)

![image-20230904153725708](机器学习【2023春】李宏毅.assets/image-20230904153725708.png)

![image-20230904155711453](机器学习【2023春】李宏毅.assets/image-20230904155711453.png)

![image-20230904161026297](机器学习【2023春】李宏毅.assets/image-20230904161026297.png)

![image-20230904161221573](机器学习【2023春】李宏毅.assets/image-20230904161221573.png)

![image-20230904161528570](机器学习【2023春】李宏毅.assets/image-20230904161528570.png)

![image-20230904162355514](机器学习【2023春】李宏毅.assets/image-20230904162355514.png)

![image-20230904162457230](机器学习【2023春】李宏毅.assets/image-20230904162457230.png)

![image-20230904162526833](机器学习【2023春】李宏毅.assets/image-20230904162526833.png)

## ChatGPT带来的新方向

![image-20230904162742929](机器学习【2023春】李宏毅.assets/image-20230904162742929.png)

![image-20230904162807957](机器学习【2023春】李宏毅.assets/image-20230904162807957.png)

![image-20230904163033642](机器学习【2023春】李宏毅.assets/image-20230904163033642.png)

![image-20230904163358692](机器学习【2023春】李宏毅.assets/image-20230904163358692.png)

![image-20230904163741843](机器学习【2023春】李宏毅.assets/image-20230904163741843.png)

## Extra Material

### ChatGPT是怎么炼成的

![image-20230904164156013](机器学习【2023春】李宏毅.assets/image-20230904164156013.png)

#### 1. 学习文字接龙

![image-20230904164236313](机器学习【2023春】李宏毅.assets/image-20230904164236313.png)

![image-20230904164354025](机器学习【2023春】李宏毅.assets/image-20230904164354025.png)

![image-20230904164405464](机器学习【2023春】李宏毅.assets/image-20230904164405464.png)

![image-20230904164450972](机器学习【2023春】李宏毅.assets/image-20230904164450972.png)

#### 2. 人类来引导文字接龙的方向

![image-20230904164610139](机器学习【2023春】李宏毅.assets/image-20230904164610139.png)

#### 3. 模仿人类老师的喜好

![image-20230904164757084](机器学习【2023春】李宏毅.assets/image-20230904164757084.png)

#### 4. 用增强式学习向模拟老师学习

![image-20230904164913891](机器学习【2023春】李宏毅.assets/image-20230904164913891.png)

![image-20230904164923872](机器学习【2023春】李宏毅.assets/image-20230904164923872.png)

---

![image-20230904165050353](机器学习【2023春】李宏毅.assets/image-20230904165050353.png)

## Optional Material

### 回归

![image-20230904173314639](机器学习【2023春】李宏毅.assets/image-20230904173314639.png)

![image-20230904210351975](机器学习【2023春】李宏毅.assets/image-20230904210351975.png)

####  Step1：模型

![image-20230904210619236](机器学习【2023春】李宏毅.assets/image-20230904210619236.png)

> 线性模型：形如上面这种形式
>
> 特征：输入的属性
>
> 权重：
>
> 偏置：

#### Step2：函数的好坏——损失函数

![image-20230904211908748](机器学习【2023春】李宏毅.assets/image-20230904211908748.png)

>   y hat表示一个正确的值，实际观察到的该有的数值

![image-20230904212505009](机器学习【2023春】李宏毅.assets/image-20230904212505009.png)

![image-20230904213059189](机器学习【2023春】李宏毅.assets/image-20230904213059189.png)

![image-20230904213319225](机器学习【2023春】李宏毅.assets/image-20230904213319225.png)

#### Step3：找到最好的函数——梯度下降

![image-20230904213845947](机器学习【2023春】李宏毅.assets/image-20230904213845947.png)

![image-20230905132524137](机器学习【2023春】李宏毅.assets/image-20230905132524137.png)

![image-20230905133027329](机器学习【2023春】李宏毅.assets/image-20230905133027329.png)

> 学习率η

![image-20230905133239249](机器学习【2023春】李宏毅.assets/image-20230905133239249.png)

![image-20230905133526475](机器学习【2023春】李宏毅.assets/image-20230905133526475.png)

![image-20230905133743969](机器学习【2023春】李宏毅.assets/image-20230905133743969.png)

![image-20230905134016099](机器学习【2023春】李宏毅.assets/image-20230905134016099.png)

![image-20230905134135581](机器学习【2023春】李宏毅.assets/image-20230905134135581.png)

#### Result

![image-20230905134333177](机器学习【2023春】李宏毅.assets/image-20230905134333177.png)

![image-20230905134435025](机器学习【2023春】李宏毅.assets/image-20230905134435025.png)

#### 再加参数

![image-20230905144822523](机器学习【2023春】李宏毅.assets/image-20230905144822523.png)

![image-20230905144904189](机器学习【2023春】李宏毅.assets/image-20230905144904189.png)

![image-20230905145022261](机器学习【2023春】李宏毅.assets/image-20230905145022261.png)

![image-20230905145111359](机器学习【2023春】李宏毅.assets/image-20230905145111359.png)

![image-20230905145240240](机器学习【2023春】李宏毅.assets/image-20230905145240240.png)

> 如果梯度下降真的能够帮你找到一个最佳函数，那么复杂模型会让**训练集**上的误差变低

![image-20230905145754782](机器学习【2023春】李宏毅.assets/image-20230905145754782.png)

> 过拟合

#### 一些额外需要考虑的因素

![image-20230905145830877](机器学习【2023春】李宏毅.assets/image-20230905145830877.png)

- 不同物种要用不同的线性回归函数

![image-20230905145946476](机器学习【2023春】李宏毅.assets/image-20230905145946476.png)

![image-20230905150132388](机器学习【2023春】李宏毅.assets/image-20230905150132388.png)

![image-20230905150332900](机器学习【2023春】李宏毅.assets/image-20230905150332900.png)

![image-20230905150505831](机器学习【2023春】李宏毅.assets/image-20230905150505831.png)

![image-20230905150525914](机器学习【2023春】李宏毅.assets/image-20230905150525914.png)

- 重新设计更平滑的损失函数

![image-20230905150652152](机器学习【2023春】李宏毅.assets/image-20230905150652152.png)

![image-20230905151822382](机器学习【2023春】李宏毅.assets/image-20230905151822382.png)

![image-20230905151308629](机器学习【2023春】李宏毅.assets/image-20230905151308629.png)

![image-20230905151838808](机器学习【2023春】李宏毅.assets/image-20230905151838808.png)

### 分类

![image-20230905210240796](机器学习【2023春】李宏毅.assets/image-20230905210240796.png)

![image-20230905210355272](机器学习【2023春】李宏毅.assets/image-20230905210355272.png)

#### 用回归方法做分类

![image-20230905211525647](机器学习【2023春】李宏毅.assets/image-20230905211525647.png)

![image-20230905211836078](机器学习【2023春】李宏毅.assets/image-20230905211836078.png)

![image-20230905212119363](机器学习【2023春】李宏毅.assets/image-20230905212119363.png)

 ![image-20230905213020120](机器学习【2023春】李宏毅.assets/image-20230905213020120.png)

![image-20230905213300894](机器学习【2023春】李宏毅.assets/image-20230905213300894.png)

![image-20230905213603671](机器学习【2023春】李宏毅.assets/image-20230905213603671.png)

![image-20230905213824868](机器学习【2023春】李宏毅.assets/image-20230905213824868.png)

![image-20230905214044792](机器学习【2023春】李宏毅.assets/image-20230905214044792.png)![image-20230905214116978](机器学习【2023春】李宏毅.assets/image-20230905214116978.png)

#### 高斯分布

![image-20230905214302334](机器学习【2023春】李宏毅.assets/image-20230905214302334.png)

![image-20230905214532519](机器学习【2023春】李宏毅.assets/image-20230905214532519.png)

![image-20230905215032030](机器学习【2023春】李宏毅.assets/image-20230905215032030.png)

![image-20230905215232328](机器学习【2023春】李宏毅.assets/image-20230905215232328.png)

#### 36:27

# 机器如何生成图像

## 速览图像生成常见模型

![image-20230909204315594](机器学习【2023春】李宏毅.assets/image-20230909204315594.png)

![image-20230909204523189](机器学习【2023春】李宏毅.assets/image-20230909204523189.png)

![image-20230909204618131](机器学习【2023春】李宏毅.assets/image-20230909204618131.png)

![image-20230909204809617](机器学习【2023春】李宏毅.assets/image-20230909204809617.png)

![image-20230909205632566](机器学习【2023春】李宏毅.assets/image-20230909205632566.png)

> P(x|y)不是一个高斯分布，不是一个人脑可以想象出来的模型。因此一个策略是，把Normal Distribution里面sample出来的vector都对应到P(x|y)中的每一个x。就是说，如果要画一只奔跑的狗，有这么多样子。那么这个影像生成模型就是在做这么一个对应关系，把Normal Distribution里面sample出来的东西对应到狗在奔跑的图片。那么接下来难点在于，怎么把Normal Distribution做一些扭曲，把它扭成P(x|y)的样子。这就是所有影像生成模型都在尝试解决的问题，因此这些影像生成模型背后想要攻克的问题都是一样的，只是解法不一样。

![image-20230909205652200](机器学习【2023春】李宏毅.assets/image-20230909205652200.png)

### VAE

![image-20230909205843631](机器学习【2023春】李宏毅.assets/image-20230909205843631.png)

> 一张图片输入Encoder变成一个向量，这个向量就可以Decoder还原成一张图片，encoder和decoder是一起训练的，要让输入和输出越接近越好。但是只训练是不够的，要加一个额外的限制，强迫中间的向量是normal distribution

### Flow-based Model

![image-20230909211647237](机器学习【2023春】李宏毅.assets/image-20230909211647237.png)

>  跟VAE相反，训练一个encoder，吃一张图片，输出就是一个向量，这个向量的分布，希望他是normal distribution，如果可以做到，接下来强迫这个encoder是一个invertible function，你就可以把encoder当成decoder来用。
>
> 训练的时候，他是识别图片输出向量，这个向量的分布是normal distribution，实际上你要画图时，要把这个encoder反过来用，让他可以输入一个向量，然后输出一张图片。
>
> 那么，你怎么知道encoder的inverse长什么样呢？这就是flow-based神奇的地方，它有刻意限制neural network的架构，让你训完之后，马上可以知道你的encoder的inverse长什么样

### Diffusion Model

![image-20230909211758246](机器学习【2023春】李宏毅.assets/image-20230909211758246.png)

### GAN

![image-20230909212749692](机器学习【2023春】李宏毅.assets/image-20230909212749692.png)

> 只训练decoder
>
> 一开始，你给它一大堆从normal distribution里sample出来的向量，一开始这个decoder没有经过训练，所以它不知道怎么画一张图，只能输出一些乱七八糟的东西。接下来会训练一个discriminator，它的作用是分辨输入的图片是decoder生成的假的图片，还是真实的图片，discriminator训练过程中的loss就代表了**P(x)和P'(x)的相似程度**，如果discriminator无法分辨来自两个分布时，代表这两个分布的image非常的接近。如果你的discriminator的performance是很差的，它的分类错误率是很高的，loss是很高的，就说明图片很像。
>
> **Decoder要做的事情就是想办法调整参数，让discriminator做的越差越好**
>
> 这里的decoder也叫generator

### 区别

![image-20230909212726354](机器学习【2023春】李宏毅.assets/image-20230909212726354.png)

> 共同点，都有encoder机制：
>
> - VAE中，encoder和decoder都很复杂，两个都需要训练
> - Flow-based中，将encoder做了些手脚，使其逆转之后变成decoder，只需要训练encoder
> - Diffusion中，反过来，加noise其实可以看成是无须训练的encoder，denoise可以看成是只训练decoder。因此VAE跟diffusion有很多共同点

**【备注】为什么没有GAN**

- 因为你完全可以在这些训练好的decoder之后加一个discriminator，让你的decoder变得更好。

![image-20230909213441763](机器学习【2023春】李宏毅.assets/image-20230909213441763.png)

- GAN可以看成是一个外挂，可以加到现有的生成模型后面

## 浅谈图像生成模型Diffusion Model

### how it works？

![image-20230909213711708](机器学习【2023春】李宏毅.assets/image-20230909213711708.png)

![image-20230909215114634](机器学习【2023春】李宏毅.assets/image-20230909215114634.png)

“雕塑本来就在大理石里面，雕刻的过程只是把不要的东西去掉”，Diffusion Model要做的东西是一样的，本来图片就在噪音里面，只是把不需要的噪音去掉

![image-20230909215322544](机器学习【2023春】李宏毅.assets/image-20230909215322544.png)

这些denoise是同一个，但在吃图片的同时还会吃一个输入，代表当前的noise严重程度（现在是在denoise的哪一步）

### how inside works?

![image-20230909215827849](机器学习【2023春】李宏毅.assets/image-20230909215827849.png)

noise predicter预测当前输入的图像中的噪音长什么样，把产生的噪音图像减去输入（当前要被denoise的图片），就产生了输出（下一步需要被denoise的图片）

### how to train?

![image-20230909230242369](机器学习【2023春】李宏毅.assets/image-20230909230242369.png)

![image-20230909230452785](机器学习【2023春】李宏毅.assets/image-20230909230452785.png)

做完diffusion process之后，就有noise predicter的训练资料了

![image-20230909230517862](机器学习【2023春】李宏毅.assets/image-20230909230517862.png)

### how to take text into account

![image-20230909231529230](机器学习【2023春】李宏毅.assets/image-20230909231529230.png)

![image-20230909231716366](机器学习【2023春】李宏毅.assets/image-20230909231716366.png)

![image-20230909231723063](机器学习【2023春】李宏毅.assets/image-20230909231723063.png)

![image-20230909231802470](机器学习【2023春】李宏毅.assets/image-20230909231802470.png)

![image-20230909231817802](机器学习【2023春】李宏毅.assets/image-20230909231817802.png)

## Stable Diffusion、DALL-E、Imagen背后共同的套路

![image-20230910102156967](机器学习【2023春】李宏毅.assets/image-20230910102156967.png)

> 主要有三个部分：
>
> 1. Text Encoder：把一段文字变成一块一块的向量，
> 2. Generation Model：将向量和noise整合成一个中间产物，这个中间产物是图片被压缩以后的结果
> 3. Decoder：将压缩后的中间产物还原为原始图片
>
> 通常三个部分分开训练，再把他们组合起来

![image-20230910102143799](机器学习【2023春】李宏毅.assets/image-20230910102143799.png)

![image-20230910102446216](机器学习【2023春】李宏毅.assets/image-20230910102446216.png)

> DALL-E有两个生成模型，一个是Autoregressive，Autoregressive直接生成完整图片计算量很大，但如果只生成压缩的版本，还行；另一个是Diffusion

![image-20230910102535214](机器学习【2023春】李宏毅.assets/image-20230910102535214.png)

### 1. Text Encoder

![image-20230910103750528](机器学习【2023春】李宏毅.assets/image-20230910103750528.png)

![image-20230910103323384](机器学习【2023春】李宏毅.assets/image-20230910103323384.png)

![image-20230910104014246](机器学习【2023春】李宏毅.assets/image-20230910104014246.png)



### 2. Generation Model

![image-20230910112920547](机器学习【2023春】李宏毅.assets/image-20230910112920547.png)

encoder吃一张图片，产生一个latent representation，sample一个noise，加在latent representation上。noise的维度跟latent representation是一样的，然后不断迭代着加noise。

![image-20230910112910026](机器学习【2023春】李宏毅.assets/image-20230910112910026.png)

![image-20230910113109606](机器学习【2023春】李宏毅.assets/image-20230910113109606.png)

> mid journey会把中间步骤也decoder给你看，所以你看到的是一张张模糊图片的轮廓

### 3. Decoder

![image-20230910105537210](机器学习【2023春】李宏毅.assets/image-20230910105537210.png)

![image-20230910105705157](机器学习【2023春】李宏毅.assets/image-20230910105705157.png)

> imagen 

![image-20230910110022912](机器学习【2023春】李宏毅.assets/image-20230910110022912.png)

> stable diffusion，dall-e





# Diffusion Model原理剖析

![image-20230910140047399](机器学习【2023春】李宏毅.assets/image-20230910140047399.png)

![image-20230910140038320](机器学习【2023春】李宏毅.assets/image-20230910140038320.png)

## DDPM

![image-20230910140207360](机器学习【2023春】李宏毅.assets/image-20230910140207360.png)

### Training

![image-20230910142151847](机器学习【2023春】李宏毅.assets/image-20230910142151847.png)

![image-20230910140757644](机器学习【2023春】李宏毅.assets/image-20230910140757644.png)

![image-20230910142635106](机器学习【2023春】李宏毅.assets/image-20230910142635106.png)

#### 回到起点

![image-20230910145213564](机器学习【2023春】李宏毅.assets/image-20230910145213564.png)

> 每次从高斯分布中一个sample一个vector(z)出来都能通过神经网络(G)生成一张图片(x)。哪怕sample的是很简单的z，通过G也能产生复杂的x。我们期待的是，能够训练一个神经网络G，它生成的x与真实的图片的distribution越接近越好

![image-20230910145421598](机器学习【2023春】李宏毅.assets/image-20230910145421598.png)

如今的生成的模型不再这么简单，它会要求你输入一段文字，产生一张图片。这段文字通常称为【condition】，因此下面的推导会暂时不考虑condition，不会影响原理的学习。（后面加上就好）

### 如何衡量【越接近越好】

![image-20230910154318542](机器学习【2023春】李宏毅.assets/image-20230910154318542.png)

![image-20230910154330195](机器学习【2023春】李宏毅.assets/image-20230910154330195.png)

![image-20230910154816755](机器学习【2023春】李宏毅.assets/image-20230910154816755.png)

![image-20230910171450431](机器学习【2023春】李宏毅.assets/image-20230910171450431.png)

![image-20230910171629034](机器学习【2023春】李宏毅.assets/image-20230910171629034.png)

![image-20230910172311070](机器学习【2023春】李宏毅.assets/image-20230910172311070.png)
