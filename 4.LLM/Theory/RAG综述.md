

# Start





# Retrieval-Augmented Generation for Large Language Models: A Survey

![image-20231220125817908](./RAG综述.assets/image-20231220125817908.png)

【链接】：https://arxiv.org/abs/2312.10997v2

## 0. 摘要

大型语言模型(LLM)展示了强大的功能，但是它们在实际应用中仍然面临挑战，例如幻觉、缓慢的知识更新和答案缺乏透明性。Retrieve-Augmented Generation (RAG)是指**在使用大语言模型回答问题之前，从外部知识库中检索相关信息的过程**。RAG已被证明可以*显著提高答案的准确性，减少模型幻觉，特别是对于知识密集型任务*。通过引用来源，用户可以验证答案的准确性，增加模型输出的信任度。它还促进了知识更新和特定领域知识的引入。RAG有效地将大语言模型的参数化知识与非参数化的外部知识库结合起来，使其成为实现大型语言模型的最重要的方法之一。本文概述了大语言模型时代RAG的发展范式，归纳出三种范式：朴素RAG、高级RAG和模块化RAG。然后，本文对RAG的三个主要组件进行了总结和组织：检索器、生成器和增强方法，以及每个组件中的关键技术。讨论了如何对RAG模型的有效性进行评价，介绍了两种RAG模型的评价方法，强调了评价的关键指标和能力，并提出了最新的自动评价框架。最后，从垂直优化、水平可扩展性、RAG的技术栈和生态系统三个方面介绍了未来可能的研究方向。

## 1. 介绍

大型语言模型(LLM)比我们以前在自然语言处理(NLP)中看到的任何东西都更强大。GPT系列模型[Brown et al.， 2020, OpenAI, 2023]、LLama系列模型[Touvron et al.， 2023]、Gemini[谷歌，2023]和其他大型语言模型显示出令人印象深刻的语言和知识掌握，在多个评估基准上超过了人类基准水平[Wang et al.， 2019, Hendrycks et al.， 2020, Srivastava et al.， 2022]。

然而，大型语言模型也有许多缺点。他们经常捏造事实[Zhang et al.， 2023b]，在处理特定领域或高度专业化的查询时缺乏知识[Kandpal et al.， 2023]。例如，当寻求的信息超出模型的培训数据或需要最新数据时，大语言模型可能无法提供准确的答案。这一限制为在现实生产环境中部署生成式人工智能带来了挑战，因为盲目地使用黑盒大语言模型可能不够。

传统上，神经网络通过**微调模型**以**参数化知识**来适应特定的领域或专有信息。虽然这种技术产生了显著的结果，但它**需要大量的计算资源，产生较高的成本，并需要专门的技术专长**，使其难以适应不断变化的信息环境。**【参数知识】**和**【非参数知识】**发挥着不同的作用。**<u>参数化知识通过训练大语言模型获得，并存储在神经网络权值中，代表模型对训练数据的理解和泛化，形成生成响应的基础</u>**。另一方面，**<u>非参数知识存在于外部的知识来源中，如向量数据库，不直接编码到模型中，而是作为可更新的补充信息。非参数知识使大语言模型能够访问和利用最新的或领域特定的信息，提高响应的准确性和相关性。</u>**

纯参数化语言模型（大语言模型）将从大量语料库中获取的知识存储在模型的参数中。然而，这样的模型有其局限性。**首先，很难从训练语料库中保留所有的知识，特别是对于不太常见但比较具体的知识。其次，由于模型参数不能动态更新，参数知识容易随着时间的推移而过时。最后，参数的扩展导致训练和推理的计算费用增加。**为了解决纯参数化模型的局限性，语言模型可以通过集成非参数化语料库数据库和参数化模型来采用半参数化方法。这种方法被称为检索增强生成(RAG)。

**检索增强生成**(RAG)一词是由[Lewis et al.， 2020]首先提出的。**<u>它将一个经过训练的检索器与一个经过训练的seq2seq模型(生成器)相结合，并进行端到端微调，以一种更易于解释和模块化的方式获取知识。</u>**在大型模型出现之前，RAG主要关注端到端模型的直接优化。检索侧的密集检索，例如使用基于向量的密集通道检索(DPR)[Karpukhin等人，2020]，以及在生成侧训练更小的模型是常见的做法。由于整体参数尺寸较小，检索器和生成器都经常进行端到端同步训练或微调[Izacard等人，2022]。

在像ChatGPT这样的LLM出现后，生成语言模型成为主流，在各种语言任务中展示了令人印象深刻的性能[Bai et al.， 2022, OpenAI, 2023, Touvron et al.， 2023，谷歌，2023]。**然而，大语言模型仍然面临着幻觉[Yao et al.， 2023, Bang et al.， 2023]、知识更新和数据相关问题等挑战。这影响了大语言模型的可靠性，使它们在某些严肃的任务场景中难以发挥作用，特别是在需要访问大量知识的知识密集型任务中，如开放领域的问题回答[Chen and Yih, 2020, Reddy et al.， 2019, Kwiatkowski et al.， 2019]和常识推理[Clark et al.， 2019, Bisk et al.，[2020]。参数内的隐式知识可能是不完整和不足的。**

后续研究发现，在大型模型的上下文学习(In-Context Learning, ICL)中引入RAG可以缓解上述问题，且效果显著且易于实施。在推理过程中，RAG从外部信息源动态检索信息，并将检索到的数据作为参考来组织答案。这大大提高了响应的准确性和相关性，有效地解决了大语言模型中出现的幻觉问题。这种技术在大语言模型出现后很快得到了广泛应用，并且已经成为最热门的技术之一，用于改进聊天机器人并使大语言模型更实用。RAG通过将事实知识从大语言模型的训练参数中分离出来，巧妙地将生成模型的强大功能与检索模块的灵活性结合起来，有效地解决了纯参数化模型中固有的知识不完整和不足的问题。

本文系统地回顾和分析了当前RAG的研究方法和未来的发展路径，将其归纳为三种主要的研究范式：朴素RAG、高级RAG和模块化RAG。随后，本文对检索(Retrieval)、增强(Augmented)和生成(Generation)三个核心组件进行了综合总结，重点阐述了RAG的改进方向和当前技术特点。在增强方法部分，将当前的工作分为三个方面：RAG的增强阶段、增强数据源和增强过程。此外，本文还总结了RAG的评价体系、适用场景和其他相关内容。通过本文，读者对大型模型和检索-增强生成有了更全面、系统的了解。熟悉知识检索增强的演进路径和关键技术，识别不同技术的优缺点，识别适用场景，并在实践中探索当前的典型应用案例。值得注意的是，在之前的工作中，Feng el al.[2023b]系统地回顾了将大模型与知识相结合的方法、应用和未来趋势，主要集中在知识编辑和检索增强方法上。Zhu等人[2023]介绍了大型语言模型增强检索系统的最新进展，并特别关注检索系统。同时，Asai等人[2023a]关注“什么”、“何时”、“如何”等问题，分析并阐明了基于检索的语言模型中的关键过程。与之相比，本文系统地概述了检索增强生成(retrieve - augmented Generation, RAG)的整个过程，并重点研究了如何通过知识检索来增强大型语言模型的生成。

RAG算法和模型的发展如图1所示。从时间轴上看，大部分与RAG相关的研究出现在2020年之后，一个重要的转折点是2022年12月ChatGPT的发布。自ChatGPT发布以来，自然语言处理领域的研究已经进入了大模型时代。朴素RAG技术迅速得到重视，导致相关研究的数量迅速增加。在增强策略方面，自RAG概念提出以来，关于训练前强化和监督微调阶段强化的研究一直在进行。然而，关于推理阶段强化的研究大多出现在大语言模型时代。这主要是由于与高性能大型模型相关的高训练成本。研究人员试图通过在推理阶段加入RAG模块，以经济有效的方式整合外部知识，从而增强模型生成。关于扩充数据的使用，早期RAG主要关注非结构化数据的应用，特别是在开放领域的问题回答上下文中。随后，可检索的知识来源范围扩大，使用高质量的数据作为知识来源，有效地解决了诸如错误知识内化和大模型中的幻觉等问题。这包括结构化的知识，知识图谱就是一个典型的例子。最近，对self-retrieval的关注越来越多，这涉及到挖掘大语言模型本身的知识，以提高它们的性能。

![image-20231220144027805](./RAG综述.assets/image-20231220144027805.png)

本文接下来的章节结构如下：第2章介绍了RAG的背景。第三章介绍了RAG的主流范式。第四章分析了RAG中的检索器。第五章重点介绍RAG中的生成器。第六章重点介绍了RAG中的增强方法。第七章介绍了RAG的评价体系。第八章展望了RAG未来的发展趋势。最后，在第九章中，我们总结了本综述的主要内容。

## 2. 背景

在本章中，我们将介绍RAG的定义，以及与其他模型优化技术(如微调)的比较。

### 2.1 定义

RAG的含义随着技术的发展而扩展。在大语言模型时代，RAG的具体定义是指模型在回答问题或生成文本时，首先从大量的语料库中检索相关信息。随后，它利用这些检索到的信息来生成响应或文本，从而提高预测的质量。RAG方法允许开发人员避免为每个特定的任务重新训练整个大型模型的需要。相反，他们可以附加一个知识库，为模型提供额外的信息输入，并提高其响应的准确性。RAG方法特别适用于知识密集型任务。综上所述，RAG系统包括两个关键阶段：

1. 利用编码模型基于问题检索相关文档，如BM25、DPR、ColBERT和类似方法[Robertson et al.， 2009, Karpukhin et al.， 2020, Khattab和Zaharia, 2020]。
2. 生成阶段：使用检索到的上下文作为条件，系统生成文本。

![image-20231220175851784](./RAG综述.assets/image-20231220175851784.png)

### 2.2 RAG vs 微调

在大型语言模型(大语言模型)的优化中，除了RAG之外，另一个重要的优化技术是微调。

**RAG类似于为模型提供一本教科书，允许它根据特定的查询检索信息。**这种方法适用于模型需要回答特定查询或处理特定信息检索任务的场景。然而，RAG并不适合让模型理解广泛的领域或学习新的语言、格式或风格。

**微调类似于让学生通过广泛学习内化知识。当模型需要复制特定的结构、样式或格式时，这种方法非常有用。**微调可以增强非微调模型的性能，并使交互更高效。它特别适合于<u>【强调基础模型中的现有知识，修改或定制模型的输出，以及为模型提供复杂的指令】</u>。然而，微调并不适合将新知识合并到模型中，也不适合需要对新用例进行快速迭代的情况。

**微调类似于让学生通过长期学习内化知识。当模型需要复制特定的结构、样式或格式时，此方法适用。**微调可以实现比非微调模型更好的性能，交互也更高效。微调特别适合于强调基础模型中的现有知识，修改或自定义模型的输出，以及用复杂的指令来指示模型。然而，微调并不适合向模型中添加新知识，也不适合需要对新用例进行快速迭代的场景。RAG与微调的具体比较见表1。

![image-20231220171050724](./RAG综述.assets/image-20231220171050724.png)

RAG和微调不是相互排斥的，而是可以互补的，在不同的级别上增强模型的能力。在某些情况下，结合这两种技术可以获得最优的模型性能。使用RAG和微调的整个优化过程可能需要多次迭代才能获得满意的结果。

已有研究表明，与其他优化大型语言模型的方法相比，检索增强生成具有显著优势[Shuster et al.， 2021, Yasunaga et al.， 2022, Wang et al.， 2023c, Borgeaud et al.， 2022]：

- RAG通过将答案与外部知识联系起来，减少语言模型中的幻觉问题，并使**生成的回答更加准确和可靠**，从而提高了准确性。
- 使用检索技术可以识别最新的信息。与单纯依赖训练数据的传统语言模型相比，**RAG保持了响应的及时性和准确性**。
- 透明度是RAG的一个优势。通过引用来源，用户可以**验证答案的准确性**，从而增加对模型输出的信任。
- RAG具有定制功能。通过对相关文本语料库进行索引，可以将模型裁剪到不同的领域，**为特定领域提供知识支持**。
- 在安全和隐私管理方面，**RAG通过其在数据库中内置的角色和安全控制，可以更好地控制数据使用**。相比之下，精细调优的模型可能缺乏对谁可以访问哪些数据的清晰管理。
- RAG**更具可扩展性**。它可以处理大规模的数据集，而不需要更新所有参数和创建训练集，使其更经济有效。
- 最后，RAG的结果更值得信赖。RAG从最新数据中选取确定性的结果，而经过微调的模型在处理动态数据时可能会出现幻觉和不准确性，缺乏透明度和可信度。

## 3. RAG框架

RAG的研究范式在不断演变。本章主要介绍RAG研究范式的演变。我们把它分为三种类型：朴素RAG、高级RAG和模块化RAG。虽然早期的RAG是成本效益和表现比原生的大语言模型更好，它仍然面临许多缺点。先进RAG和模块化RAG的出现旨在解决朴素RAG的具体缺陷。

### 3.1 Naive RAG

Naive RAG研究范式代表了在ChatGPT被广泛采用后不久获得声望的最早的方法论。朴素的RAG涉及传统的过程：索引、检索和生成。Naive RAG也被总结为“Retrieve”-“Read”框架[Ma等人，2023a]。

#### Indexing

从数据源获取数据并为其建立索引的管道通常发生在离线状态。具体来说，数据索引的构建包括以下步骤：

1. **Data Indexing**数据索引：这涉及到清理和提取原始数据，将不同的文件格式(如PDF、HTML、Word、Markdown等)转换为纯文本。
2. **Chunking**分块：将加载的文本分成更小的块。这是必要的，因为语言模型通常限制了它们可以处理的上下文的数量，所以有必要创建尽可能小的文本块。
3. **Embedding and Creating Index**嵌入和创建索引：这是通过语言模型将文本编码成向量的过程。得到的向量将用于后续的检索过程中计算向量与问题向量之间的相似度。该嵌入模型要求推理速度快。由于在用户提问时需要对大量的语料库进行编码，并对问题进行实时编码，所以模型的参数大小不宜过大。生成嵌入后，下一步是创建索引，存储原始语料库块，并以键值对的形式嵌入，以便将来快速频繁的搜索。

#### Retrieve

给定用户的输入，将使用与第一阶段相同的编码模型将查询转换为向量。计算问题嵌入与文献块嵌入在语料库中的相似度。根据相似程度选择前K个文档块作为当前问题的增强上下文信息。

#### Generation

给定的问题和相关文档被组合成一个新的提示符。然后，大型语言模型将根据所提供的信息回答问题。根据不同任务的需要，可以决定是允许大型模型使用其知识，还是只根据给定的信息进行回答。如果有历史对话信息，也可以合并到多轮对话提示中。

#### Naive RAG的缺点

Naive RAG在三个领域面临主要挑战：检索质量、响应生成质量和增强过程。关于检索质量，问题是多方面的。主要的问题是低精度，检索集中的所有块都与查询相关，导致潜在的幻觉和半空中下降问题。第二个问题是低召回率，当没有检索到所有相关块时，就会出现低召回率，从而阻止大语言模型获得足够的上下文来合成答案。此外，过时的信息带来了另一个挑战，其中数据冗余或过时的数据可能导致不准确的检索结果。在响应生成质量方面，问题也同样多样。幻觉是一个突出的问题，因为模型编造了一个并不存在于背景中的答案。当模型生成的答案不能处理查询时，不相关性是另一个问题。此外，毒性或偏见，即模型产生有害或冒犯性的反应，是另一个问题。

最后，增强过程也面临着一些挑战。至关重要的是，从检索的段落中有效地整合上下文与当前的生成任务是至关重要的。如果处理不当，输出可能会显得不连贯或脱节。冗余和重复是另一个问题，特别是当多个检索的段落包含类似的信息时，会导致生成步骤中的内容重复。此外，确定多个检索到的文章对生成任务的重要性或相关性是一项挑战，而增强过程需要适当地平衡每个文章的价值。检索到的内容也可能来自不同的写作风格或语调，增强过程需要调和这些差异，以确保输出的一致性。最后，生成模型可能过度依赖于增强的信息，导致输出仅仅重复检索到的内容，而没有提供新的价值或合成的信息。

### 3.2 Advanced RAG

Advanced RAG已经针对Naive RAG的不足进行了针对性的改进。在检索生成质量方面，Advanced RAG结合了检索前和检索后的方法。为了解决Naive RAG遇到的索引问题，Advanced RAG通过滑动窗口、细粒度分割和元数据等方法优化了索引。同时，提出了各种优化检索过程的方法。就具体实现而言，Advanced RAG可以通过管道或端到端方式进行调整。

#### Pre-Retrieval Process 

**Optimizing Data Indexing**

优化数据索引的目的是提高索引内容的质量。目前，有五种主要策略用于此目的：增加索引数据的粒度、优化索引结构、添加元数据、对齐优化和混合检索。

1. **Enhancing Data Granularity** 提高数据粒度：索引前优化的目标是提高文本的标准化、一致性，并确保事实的准确性和上下文的丰富性，以保证RAG系统的性能。文本标准化包括去除不相关信息和特殊字符，以提高检索效率。在一致性方面，首要任务是消除实体和术语的歧义，同时消除重复或冗余的信息，以简化检索者的焦点。确保事实的准确性是至关重要的，只要有可能，应该核实每条数据的准确性。为了适应现实世界中的系统交互上下文，上下文保持可以通过添加另一层带有特定领域注释的上下文来实现，并通过用户反馈循环进行持续更新。时间敏感性是重要的上下文信息，应该设计机制来刷新过时的文档。总之，优化索引数据的重点应该是清晰、上下文和正确性，以使系统高效和可靠。下面介绍了最佳实践。

2. **Optimizing Index Structures** 优化索引结构：这可以通过调整块的大小、改变索引路径和合并图结构信息来实现。调整块(从小块到大块)的方法包括收集尽可能多的相关上下文和最小化噪音。在构建RAG系统时，块大小是一个关键参数。有不同的评估框架比较单个块的大小。LlamaIndex2使用GPT4来评估保真度和相关度，而LLaMA[Touvron等人，2023]指标对不同的分块方法具有自动评估特征。跨多个索引路径的查询方法与以前的元数据过滤和分组方法密切相关，可能涉及同时跨不同索引进行查询。可以使用标准索引查询特定的查询，也可以使用独立索引根据元数据关键字(如特定的“日期”索引)进行搜索或过滤。

   引入图结构涉及到将实体转换为节点，将它们的关系转换为关系。这可以通过利用节点之间的关系来提高准确性，特别是对于多跳问题。使用图形数据索引可以增加检索的相关性。

3. **Adding Metadata Information** 添加元数据信息：这里的重点是将引用的元数据嵌入到块中，比如用于过滤的日期和目的。添加元数据(如引用的章节和子部分)也有助于改进检索。当我们将索引划分为多个块时，检索效率就成了一个问题。首先过滤元数据可以提高效率和相关性。
4. **Alignment Optimization** 对齐优化：此策略主要处理对齐问题和文档之间的差异。对齐概念包括引入假设的问题，创建适合于每个文档回答的问题，并将这些问题嵌入(或替换)到文档中。这有助于解决文档之间的对齐问题和差异。
5. **Mixed Retrieval** 混合检索：这种策略的优势在于利用不同检索技术的优势。智能结合各种技术，包括基于关键词的搜索、语义搜索和向量搜索，适应不同的查询类型和信息需求，确保最相关和上下文丰富的信息的一致检索。混合检索可以作为检索策略的有力补充，提高RAG管道的整体性能。

#### Embedding 

- **Fine-turning Embedding** 微调嵌入：嵌入模型的微调直接影响RAG的有效性。微调的目的是增强检索内容和查询之间的相关性。微调嵌入的作用类似于生成语音前的耳朵调整，优化检索内容对生成输出的影响。一般来说，微调嵌入的方法分为在特定领域上下文中调整嵌入和优化检索步骤两大类。特别是在处理演化词或稀有词的专业领域，这些自定义嵌入方法可以提高检索的相关性。BGE[BAAI, 2023]嵌入模型是一种微调和高性能的嵌入模型，如BAAI 3开发的BGE-large- en。要创建用于微调BGE模型的训练数据，首先使用gpt-3.5-turbo等大语言模型基于文档块制定问题，其中问题和答案(文档块)形成微调对，用于微调过程。
- **Dynamic Embedding**动态嵌入：动态嵌入根据单词出现的上下文进行调整，与静态嵌入不同，静态嵌入对每个单词使用单个向量。例如，在像BERT这样的变压器模型中，同一个单词可以根据周围单词的不同有不同的嵌入方式。有证据表明，在OpenAI的文本嵌入ada-002模型4中，出现了意想不到的高余弦相似度结果，特别是当文本长度小于5个标记时。理想情况下，嵌入应该包含尽可能多的背景，以确保“健康”的结果。OpenAI的embedded -ada-02是建立在大型语言模型(如GPT)的原则之上的，它比静态嵌入模型更加复杂，能够捕获特定级别的上下文。虽然它在上下文理解方面很出色，但它可能不会表现出与最新的全尺寸语言模型(如GPT4)一样的上下文敏感性。

#### Post-Retrieval Process 

从数据库中检索有价值的上下文后，将它与用于输入的查询合并到大语言模型中会带来挑战。立即将所有相关文档呈现给大语言模型可能会超出上下文窗口限制。将大量文档连接起来形成一个冗长的检索提示符是无效的，这会带来噪音并妨碍大语言模型对关键信息的关注。为了解决这些问题，需要对检索到的内容进行额外的处理。

- **ReRank**重排：重新排序，将最相关的信息移到提示的边缘是一个简单的想法。这个概念已经在诸如LlamaIndex、LangChain和HayStack等框架中得到了实现[Blagojevi, 2023]。例如，Diversity Ranker会根据文档多样性优先排序，而LostInTheMiddleRanker会将最好的文档放在上下文窗口的开头和结尾。同时，为了解决解释基于向量的模拟搜索语义相似度的难题，cohereAI rerank [Cohere, 2023]、bgererank5或LongLLMLingua [Jiang et al.， 2023a]等方法重新计算了相关文本和查询之间的语义相似度。
- **Prompt Compression**提示压缩：研究表明，检索文档中的噪声会对RAG性能产生不利影响。在后处理中，重点在于压缩不相关的上下文，突出关键段落，减少整体上下文长度。选择性上下文[Litman et al.， 2020]和LLMLingua [Anderson et al.， 2022]等方法利用小的语言模型计算提示互信息或perplexity，估计元素的重要性。然而，这些方法在RAG或长上下文场景中可能会丢失关键信息。Recomp [Xu等人，2023a]通过在不同粒度上训练压缩机来解决这一问题。长上下文[Xu et al.， 2023b]在处理广泛上下文时进行分解和压缩，而“在记忆迷宫中行走”[Chen et al.， 2023a]设计了一个分层的摘要树来增强大语言模型的关键信息感知。

#### RAG Pipeline Optimization 

检索过程的优化旨在提高RAG系统的效率和信息质量，目前的研究主要集中在智能结合各种检索技术，优化检索步骤，引入认知回溯的概念，灵活应用多种查询策略，利用嵌入相似性。这些努力共同致力于在RAG检索中的上下文信息的效率和丰富性之间取得平衡。

- **Exploring Hybrid Search**探索混合搜索：通过智能地混合各种技术，如基于关键字的搜索、语义搜索和向量搜索，RAG系统可以利用每种方法的优势。这种方法使RAG系统能够适应不同的查询类型和信息需求，确保一致地检索最相关和上下文丰富的信息。混合搜索作为检索策略的有力补充，提高了RAG管道的整体性能。
- **Recursive Retrieval and Query Engine**递归检索和查询引擎：在RAG系统中优化检索的另一种强大的方法涉及到实现递归检索和一个复杂的查询引擎。递归检索需要在初始检索阶段获取更小的文档块，以捕获关键语义。在此过程的后期，将向语言模型(LM)提供具有更多上下文信息的较大块。这种两步检索方法有助于在效率和上下文丰富的响应之间取得平衡。
- **StepBack-prompt**：StepBack-prompt method [Zheng et al.， 2023]将其集成到RAG process中，鼓励大语言模型从特定的实例中后退一步，并参与到对底层通用概念或原则的推理中。实验结果表明，在各种具有挑战性的推理密集型任务中，使用后向提示可以显著提高性能，显示了它对RAG的自然适应性。检索增强步骤可以应用于对向后提示的答案的生成和最终的问题回答过程。
- **Subqueries**子查询：可以在不同的场景中使用各种查询策略，包括使用LlamaIndex等框架提供的查询引擎、使用树查询、使用向量查询或使用最基本的块顺序查询。
- **HyDE**：这种方法基于一个假设，即生成的答案可能比直接查询更接近嵌入空间。利用大语言模型，HyDE生成一个响应查询的假设文档(答案)，嵌入文档，并使用这种嵌入来检索与假设文档类似的真实文档。与基于查询的嵌入相似度搜索方法不同，该方法强调从答案到答案的嵌入相似度。但是，它可能不会始终产生良好的结果，特别是在语言模型不熟悉所讨论的主题的情况下，这可能会导致生成更多容易出错的实例。

### 3.3 Modular RAG

模块化的RAG结构打破了传统的朴素RAG的索引、检索和生成框架，在整个过程中提供了更大的多样性和灵活性。一方面，它整合了多种方法来扩展功能模块，如在相似度检索中加入搜索模块，在检索器中应用微调方法[Lin et al.， 2023]。此外，具体问题导致了重构RAG模块[Yu等人，2022]和迭代方法[Shao等人，2023]的出现。模块化RAG范式正在成为RAG领域的主流，它允许跨多个模块的序列化管道或端到端培训方法。三种RAG范式的比较如图3所示。

![image-20231220180930637](./RAG综述.assets/image-20231220180930637.png)

#### New Modules

- **Search Module**搜索模块：新的模块搜索模块不同于在Naive/Advanced RAG中查询和语料库之间的相似性检索，该搜索模块针对特定场景进行了剪裁，在过程中使用llm生成的代码、查询语言(如SQL、Cypher)或其他自定义工具对(其他)语料库进行直接搜索。搜索的数据源包括搜索引擎、文本数据、表格数据或知识图谱[Wang et al.， 2023c]。
- **Memory Module**内存模块：利用大语言模型本身的内存功能来指导检索，该原则涉及到查找与当前输入最相似的内存。Self-mem [Cheng等人，2023b]将“原始问题”和“双重问题”结合起来，迭代地使用一个增强检索的生成器来创建一个无界内存池。一个检索增强的生成模型可以使用自己的输出来增强自己，使得文本更接近推理过程中的数据分布，使用模型自己的输出，而不是训练数据[Wang et al.， 2022a]。
- **Extra Generation Module**额外生成模块：在检索的内容中，冗余和噪声是常见的问题。与直接从数据源检索不同，额外的生成模块利用大语言模型来生成所需的上下文[Yu et al.， 2022]。与直接检索相比，由大语言模型生成的内容更可能包含相关信息。
- **Task Adaptable Module**任务适应性模块：UPRISE[Cheng et al.， 2023a]专注于改造RAG以适应各种下游任务，从预先构造的数据池中自动检索给定零机会任务输入的提示，增强了任务和模型之间的通用性。提词器[Dai et al.， 2022]利用大语言模型作为少量镜头的查询生成器，并基于生成的数据创建特定任务的检索器。利用大语言模型的泛化功能，PROMPTAGATOR仅通过几个示例就可以创建特定任务的端到端检索器。
- **Alignment Module**校准模块：查询和文本之间的对齐一直是影响RAG有效性的关键问题。在模块化RAG时代，研究人员发现，为寻回器添加可训练的适配器模块可以有效地缓解对齐问题。PRCA[Yang et al.， 2023b]利用强化学习训练由大语言模型奖励驱动的上下文适配器，定位在寻回器和生成器之间。在标记自回归策略的强化学习阶段，通过奖励最大化来优化检索到的信息。AAR[Yu et al.， 2023b]提出了一种通用插件，该插件从已知源大语言模型中学习LM偏好，以帮助未知或非共同finetuned大语言模型。RRR[Ma et al.， 2023a]设计了一个基于强化学习重写查询的模块，以将查询与语料库中的文档对齐。
- **Validation Module**验证模块：在实际场景中，并不总是能保证检索到的信息是可靠的。在大语言模型中，检索不相关的数据可能会导致错觉的发生。因此，可以在检索文档之后引入一个额外的验证模块，以评估检索文档和查询之间的相关性。这增强了RAG的鲁棒性[Yu et al.， 2023a]。

#### New Pattern

模块化RAG的组织方法是灵活的，允许基于特定的问题上下文在RAG过程中替换或重新配置模块。对于由检索和生成两个模块(文献中称为阅读或合成)组成的Naive RAG，该框架具有很强的适应性和丰完全性。目前的研究主要探讨了两种组织范式，包括模块的添加或替换，以及模块之间的组织流调整。

- **Adding or Replacing Modules**增加/更换模块

  添加或替换模块的策略需要维护retrieve - read的结构，同时引入额外的模块来增强特定的功能。RRR[Ma等人，2023a]提出了RewriteRetrieve-Read过程，利用大语言模型性能作为对重写器模块强化学习的奖励。这允许重写器调整检索查询，提高阅读器的下游任务性能。类似地，可以在Generate-Read[Yu et al.， 2022]等方法中选择性地替换模块，其中大语言模型生成模块替换检索模块。Recite-Read [Sun et al.， 2022]将外部检索转换为从模型权值的检索，最初让大语言模型记忆任务相关的信息，并生成输出来处理知识密集型的自然语言处理任务。

- **Adjusting the Flow between Modules**调整模块间的流程

  在模块间流的调节方面，语言模型和检索模型之间的交互性受到了重视。DSP[Khattab等人，2022]引入了演示-搜索-预测框架，将上下文学习系统视为一个明确的程序，而不是一个终端任务提示来处理知识密集型任务。ITER-RETGEN [Shao et al.， 2023]利用生成的内容来指导检索，在Retrieve-ReadRetrieve-Read流程中迭代执行“检索增强生成”和“生成增强检索”。Self-RAG[Asai等人，2023b]遵循decision -retrieve-reflect-read过程，引入了一个主动判断模块。这种自适应和多样化的方法允许模块化RAG框架内的模块的动态组织。

## 4. Retriever

在RAG上下文中，“R”代表检索，在RAG管道中起着从庞大的知识库中检索top-k相关文档的作用。然而，打造高质量的检索器是一项重要的任务。在本章中，我们围绕三个关键问题展开讨论：1)如何获得准确的语义表示?2)如何匹配查询和文档的语义空间?3)如何使检索器的输出与大型语言模型的偏好保持一致?

### 4.1如何获得准确的语义表示?

在RAG中，语义空间是查询和文档映射的多维空间。当我们执行检索时，它是在语义空间中测量的。如果语义表达不准确，那么它对RAG的影响是致命的，本节将介绍两种方法来帮助我们构建准确的语义空间。

#### Chunk optimization

处理外部文档时，第一步是分组以获得细粒度的特性。然后块被嵌入。然而，嵌入过大或过小的文本块可能都不会取得良好的效果。因此，为语料库中文档找到最优的块大小是保证搜索结果准确性和相关性的关键。

在选择分块策略时，重要的考虑因素包括：被索引的内容的特征、使用的嵌入模型及其最佳块大小、用户查询的预期长度和复杂性，以及检索结果在特定应用程序中的使用方式。例如，对于较长的或较短的内容，应该选择不同的组块模型。此外，不同的嵌入模型在不同的块大小下表现不同;例如，sentence-transformer更适合单个句子，而text-embedded-ada-002更适合包含256或512个标记的块。此外，用户输入问题文本的长度和复杂性，以及您的应用程序的特定需求，如语义搜索或Q&A，都将影响组块策略的选择。这可能与您选择的大语言模型的令牌限制直接相关，并且可能需要您调整块大小。实际上，该算法通过自适应地应用几种分块策略来获得准确的查询结果;没有最好的，只有最适合的。

目前RAG研究中采用了多种块优化方法来提高检索效率和准确性。滑动窗口等技术通过多次检索聚合全局相关信息，实现分层检索。Small2big技术在搜索过程中利用较小的文本块，并为语言模型提供较大的附属文本块进行处理。摘要嵌入技术对文档摘要进行Top K检索，提供完整的文档上下文。元数据过滤技术利用文档元数据进行过滤。图索引技术将实体和关系转换为节点和连接，显著增强了多跳问题上下文中的相关性。这些方法的合并提高了检索结果，提高了检索性能。

#### Fine-tuning Embedding Models

在获得合适的语块大小后，我们需要通过嵌入模型将语块嵌入并查询到语义空间中，因此嵌入能否有效地表示语料库至关重要。目前，已经出现了一些优秀的嵌入模型，如[UAE[AngIE, 2023]， Voyage[VoyageAI, 2023]， BGE[BAAI, 2023]等，它们在大规模语料库上进行了预训练，但在应用于特定领域时，可能不能准确地表示特定领域的语料库信息。此外，特定于任务的嵌入模型微调对于确保模型理解与内容相关性相关的用户查询至关重要，而未进行微调的模型可能无法满足特定任务的需求。因此，对于下游应用程序来说，微调嵌入模型至关重要。在嵌入微调方法中有两种基本的模式。

- **Domain Knowledge Fine-tuning领域知识微调**：为了使嵌入模型正确地理解特定领域的信息，我们需要构造特定领域的数据集来微调嵌入模型。然而，微调嵌入模型不同于普通的语言模型，主要是因为所使用的数据集不同。在目前主要的嵌入模型微调方法中，所使用的数据集由查询、语料库和相关文档三部分组成。嵌入模型根据查询结果在语料库中查找相关文档，然后以查询的相关文档命中与否作为模型的度量指标。

  在数据集的构造、微调模型和评估过程中，这三个组件中的每一个都可能面临许多挑战。在LlamaIndex [Liu, 2023]中，针对嵌入模型的微调过程，引入了一系列关键类和函数，大大简化了这一过程。通过准备一个领域知识语料库，并利用它提供的方法，我们可以很容易地获得适合于我们想要的领域的专门的嵌入模型。

- **Fine-tuning of downstream tasks下游任务的微调**：使嵌入模型适应下游任务同样重要。当在下游任务中使用RAG时，有些工作通过使用大语言模型的功能对嵌入模型进行了微调。PROMPTAGATOR[Dai et al.， 2022]利用了大型语言模型(Large Language Model，大语言模型)作为一个少量的查询生成器，并基于生成的数据创建特定于任务的检索程序，并缓解了由于数据稀缺而在某些领域难以监管的微调问题。LLM- embedder [Zhang et al.， 2023a]使用大型语言模型输出来自多个下游任务的数据的奖励值，通过对数据集的硬标记和来自大语言模型的软奖励，对具有两个不同监督信号的检索器进行微调。

通过领域知识注入和下游任务微调，这在一定程度上改善了语义表示。然而，这种方法训练的检索器在直觉上对大型语言模型并没有帮助，因此已经做了一些工作来监督通过来自大语言模型的反馈信号直接对嵌入模型进行微调。(这一节将在4.4中介绍)

### 4.2 如何匹配查询和文档的语义空间

在RAG应用程序中，一些检索器使用相同的嵌入模型对查询和文档进行编码，而另一些检索器使用两个模型分别对查询和文档进行编码。此外，用户的原始查询可能存在表达能力差、语义信息缺乏等问题。因此，对齐用户查询和文档的语义空间是非常必要的。本节将介绍实现这一目标的两项关键技术。

#### Query Rewrite

使查询和文档的语义保持一致的最直观的方法是重写查询。如Query2Doc[Wang et al.， 2023b]和ITERRETGEN[Shao et al.， 2023]所述，利用大型语言模型固有的能力，通过引导生成一个伪文档，然后将原始查询与该伪文档合并，形成新的查询。在HyDE[Gao et al.， 2022]中，查询向量是通过使用文本指示器来建立的，使用这些指示器来生成一个“假设的”文档，该文档是相关的，但可能并不真正存在，它只需要捕获相关的模式。RRR[Ma et al.， 2023a]引入了一个新的框架，将检索和读取的顺序颠倒，专注于查询重写。该方法首先使用大型语言模型生成查询，然后使用网络搜索引擎检索上下文，最后使用小型语言模型作为训练重写器来服务于冻结的大型语言模型。step - backprompt [Zheng et al.， 2023]方法可以使大型语言模型进行抽象推理，提取高级概念和原理，并在此基础上进行检索。最后，多查询检索的方法是使用大型语言模型生成多个查询，这些查询可以并行执行，检索结果可以一起输入，对于依赖于多个子问题的单个问题非常有用

#### Embedding Transformation

如果存在像重写查询这样的粗粒度方法，那么也应该有特定于嵌入操作的细粒度实现。在LlamaIndex[Liu, 2023]中，可以在查询编码器之后连接适配器，并对适配器进行微调，以优化查询嵌入的表示，将其映射到更适合于特定任务的潜在空间。当查询和外部文档的数据结构不同时，例如非结构化查询和结构化外部文档，使查询与文档保持一致是非常重要的。SANTA[Li et al.， 2023d]提出了两种训练前方法使检索器感知结构化信息1)利用结构化数据与非结构化数据之间的自然对齐关系进行对比学习，进行结构感知前训练。2)掩蔽实体预测，设计一种面向实体的掩蔽策略，要求语言模型填充掩蔽实体。

### 4.3 如何调整检索器的输出和大语言模型的偏好

在RAG pipeline中，即使我们使用上述技术来提高检索命中率，也不一定能提高RAG的最终效果，因为被检索到的文档可能不是大语言模型所需要的。因此，本节介绍两种方法来调整检索器的输出和大语言模型的首选项。

#### LLM supervised training

许多工作利用了从大型语言模型到微调嵌入模型的各种反馈信号。AAR[Yu等人，2023b]通过一个编码器解码器体系结构LM为一只预先训练过的检索器提供监控信号。通过FiD交叉注意力评分确定LM的首选文档，然后使用硬负抽样和标准交叉熵损失对检索器进行微调。最终，微调的检索器可以直接用于增强不可见的目标LMs，从而更好地执行目标任务。检索器的训练损失：
$$
\zeta=\sum_{q}\sum_{d^+\in D^{a^+}}\sum_{d^-\in D^-}l\left(f\left(q,d^+\right),f\left(q,d^-\right)\right)
$$
其中$D^{a^+}$是检索集中的大语言模型首选的文档，而$D^{a^-}$不是首选的文档。$l$ 是标准的交叉熵损失。最后，我们认为大语言模型可能更倾向于关注可读的文档，而不是信息丰富的文档

REPLUG[Shi et al.， 2023]使用检索器和大语言模型来计算检索到的文档的概率分布，然后通过计算KL散度来执行监督训练。这种简单有效的训练方法通过使用LM作为监督信号来提高检索模型的性能，消除了任何特定的交叉注意机制的需要。检索器的训练损失如下：

$$
\zeta=\frac{1}{|D|}\sum_{x\in D}KL\left(P_{R}\left(d|x\right)||Q_{LM}\left(d|x,y\right)\right)
$$
其中$D$是输入上下文的集合，$P_R$是检索似然，$Q_{LM}$是每个文档的LM似然。

UPRISE [Cheng等人，2023a]也使用了冻结的大型语言模型来微调Prompt检索器。但是语言模型和检索器都将提示输入对作为输入，然后使用大型语言模型给出的分数来监督检索器的训练，这相当于使用大型语言模型来标记数据集。Atlas[Izacard et al.， 2022]提出了四种精细调优监督嵌入模型的方法，其中注意蒸馏(Attention精馏)使用语言模型在输出过程中产生的交叉注意分数进行蒸馏。EMDR2使用期望最大化算法以检索到的文档作为潜在变量进行训练。Perplexity蒸馏直接使用模型生成的令牌的困惑度作为指标进行训练。LOOP基于文档删除对LM预测的影响，引入了新的损耗函数，为模型更好地适应特定任务提供了有效的训练策略。

#### Plug in an adapter

然而，由于使用API实现嵌入功能或本地计算资源不足等因素，对嵌入模型进行微调可能具有挑战性。因此，有些作品选择外部连接适配器进行对齐。PRCA[Yang et al.， 2023b]通过上下文提取阶段和奖励驱动阶段训练适配器，并基于基于令牌的自回归策略优化检索器的输出。TokenFiltering[Berchansky et al.， 2023]方法计算交叉注意分数，选择得分最高的输入令牌来有效地过滤令牌。RECOMP[Xu et al.， 2023a]提出了提取式生成式压缩器，该压缩器通过选择相关句子或综合文档信息生成摘要，实现多文档查询焦点摘要。此外，一种新的方法PKG[Luo et al.， 2023]通过指令微调将知识注入白箱模型，并直接替换检索器模块，该模块用于根据查询直接输出相关文档。

## 5. Generator

RAG中的另一个核心组件是生成器，它负责将检索到的信息转换成自然流畅的文本。其设计灵感来自传统的语言模型，但与传统的生成模型相比，RAG的生成器通过利用检索到的信息，提高了准确性和相关性。在RAG中，生成器的输入不仅包括传统的上下文信息，还包括通过检索器获得的相关文本片段。这使得生成器能够更好地理解问题背后的上下文，并生成信息更丰富的响应。此外，生成器由检索到的文本指导，以确保生成的内容和检索到的信息之间的一致性。正是由于输入数据的多样性，在生成阶段进行了一系列有针对性的工作，所有这些工作都旨在更好地使大型模型适应来自查询和文档的输入数据。我们将通过检索后处理和微调等方面深入介绍生成器。

### 5.1 如何通过检索后处理增强检索结果?

对于没有微调过的大型语言模型，大多数研究依赖于公认的大型语言模型，如GPT4[OpenAI, 2023]，利用其健壮的内部知识来全面检索文档知识。然而，这些大型模型的固有问题，例如上下文长度限制和对冗余信息的脆弱性，仍然存在。为了缓解这些问题，一些研究在检索后处理方面做出了努力。后检索处理是指检索器对大型文档数据库中检索到的相关信息进行进一步处理、筛选或优化的过程。其主要目的是提高检索结果的质量，以更好地满足用户需求或后续任务。它可以理解为对在检索阶段获得的文档进行再处理的过程。后检索处理的操作通常包括信息压缩和结果重新排序。

#### Information Compression

虽然检索器可以从海量的知识库中获取相关的信息，但是在检索文档中处理大量的信息仍然是我们面临的挑战。现有的一些研究试图通过增加大型语言模型的上下文长度来解决这一问题，但目前的大型模型仍然面临上下文的限制。因此，在某些情况下，信息凝聚是必要的。总之，信息凝聚的重要性主要体现在以下几个方面：降低噪声、应对上下文长度限制、增强生成效果。

PRCA [Yang et al.， 2023b]通过训练一个信息提取器来解决这个问题。在上下文提取阶段，给定一个输入文本$S_{input}$，它可以生成一个输出序列$C_{extraction}$，该序列表示从输入文档中压缩的上下文。训练过程的目标是尽可能减少$C_{extraction}$与真实文本$C_{truth}$之间的差异。他们采用的损失函数如下：
$$
minL(\theta)=-\frac1N\sum_{i=1}^NC_{truth}^{(i)}log(f_.(S_{input}^{(i)};\theta))
$$
其中$f$为信息提取器，$θ$为提取器的参数。RECOMP[Xu et al.， 2023a]类似地通过利用对比学习来训练信息冷凝器。对于每个训练数据点，存在1个正样本和5个负样本。在这个过程中，使用对比损耗来训练编码器[Karpukhin等人，2020]。具体优化目标如下：
$$
-log\frac{e^{sim(x_i,p_i)}}{sim(x_i,p_i)+\sum_{n_j\in N_i}e^{sim(x_i,p_i)}}
$$


其中$x_i$为训练数据，$p_i$为正样本，$n_j$为负样本，$sim(x,y)$计算$x$和$y$之间的相似度。另一项研究选择进一步精简文档的数量，通过减少检索文档的数量来提高模型的回答精度。[Ma et al.， 2023b]提出了“Filter-Ranker”范式，该范式综合了大型语言模型(大语言模型)和小型语言模型(SLMs)的优点。在这个范例中，SLMs充当过滤器，而大语言模型充当重新排序代理。通过促使大语言模型重新排列SLMs识别出的困难样本的部分，研究结果表明，在各种信息提取(IE)任务中都有显著的改进。

#### Rerank

重新排序模型的关键作用在于优化从检索器检索的文档集。当添加了额外的上下文时，大语言模型的性能会随着回溯性能而下降，而重新排序提供了解决这个问题的有效解决方案。其核心思想包括重新安排文档记录，将最相关的项目放在顶部，从而将文档的总数减少到一个固定的数量。这不仅解决了检索过程中可能遇到的上下文窗口扩展问题，而且有助于提高检索效率和响应性[Zhuang等，2023]。

将上下文压缩作为重排序的一部分，目的是仅根据给定的查询上下文返回相关信息。该方法的双重意义在于通过减少单个文档的内容和过滤整个文档，将最相关的信息集中显示在检索结果中。因此，重新排序模型在整个信息检索过程中起到了优化和细化的作用，为后续的大语言模型处理提供了更有效和准确的输入。

### 5.2 如何优化生成器以适应输入数据?

在RAG模型中，生成器的优化是架构的重要组成部分。生成器的任务是获取检索到的信息并生成相关文本，从而提供模型的最终输出。优化生成器的目标是确保生成的文本既自然又有效地利用检索到的文档，以便更好地满足用户的查询需求。

在典型的大型语言模型(大语言模型)生成任务中，输入通常是一个查询。在RAG中，主要的区别在于输入不仅包括一个查询，还包括检索器检索的各种文档(结构化的/非结构化的)。附加信息的引入可能会对模型的理解产生重大影响，特别是对于较小的模型。在这种场景中，微调模型以适应查询+检索文档的输入变得尤为重要。具体来说，在向微调模型提供输入之前，通常需要对检索器检索的文档进行检索后处理。需要注意的是，在RAG上对生成器进行微调的方法本质上类似于大语言模型的一般微调方法。这里，我们将简要介绍一些有代表性的工作，包括数据(格式化/非格式化)和优化函数。

#### General Optimization Process

指的是包含(input, output)对的训练数据，目的是训练模型在给定输入x的情况下产生输出y的能力。Self-mem[Cheng et al.， 2023b]的工作中采用了一种比较经典的训练过程。给定输入x，检索相关文档z(选择本文中的top1)，对(x, z)进行积分后，模型产生输出y。本文采用了两种常用的微调范式，即Joint-Encoder [Arora et al.， 2023, Wang et al.， 2022b, Lewis et al.， 2020]和Dual-Encoder [Xia et al.， 2019, Cai et al.， 2021, Cheng et al.，202]。Joint-Encoder采用了一种基于编码器-解码器的标准模型，编码器对输入进行初始编码，解码器通过注意机制将编码后的结果进行组合，自回归生成令牌：

$$
H=Encoder(x[SEP]m)
$$

$$
h^i=Decoder(CrossAttn(H),y<i)
$$

$$
P_{G_\xi}(.|x,y<i)=Softmax(h^i)
$$

对于Dual-Encoder，系统建立了两个独立的编码器，分别负责对输入(查询、上下文)和文档进行编码。然后由解码器依次对输出进行双向交叉注意处理。作者选择使用Transformer [Vaswani et al.， 2017]作为这两种架构的构建块，并优化$G_ξ$负对数似然(NLL)损耗。
$$
H_{x}=SourceEncoder(x)H_{m}=MemoryEncoder(x)
$$

$$
h^i=Decoder(CrossAttn(H_x,H_m),y<i)
$$

$$
\mathcal{L}_{nll}=-\sum_{t=1}^{|y|}logP_{G_{\xi}}(y_{t}|x,m,y<t)
$$

#### Utilizing Contrastive Learning

在准备训练数据的阶段，通常产生的是输入和输出之间的交互对。在这种情况下，模型只能访问一个唯一的真实输出，这可能会导致“暴露偏差”问题[Ranzato et al.， 2015]：在训练阶段，模型只暴露于一个单一的真实反馈，而不访问任何其他生成的令牌。这可能会削弱模型在应用中的性能，因为它可能会过度适合于训练数据中的特定反馈，而没有有效地推广到其他场景。因此，SURGE提出了一种图文对比学习方法[Kang等人，2023]。对于任意给定的输入输出交互对，这种对比学习方法的目标可以定义为：

$$
\mathcal{L}_{cont}=\frac{1}{2}log\frac{e^{sim(\zeta(z),\xi(h))/\iota}}{\sum_{h^{\prime}}e^{sim(\zeta(z),\xi(h^{\prime}))/\iota}}+\frac{1}{2}log\frac{e^{sim(\zeta(z),\xi(h))/\iota}}{\sum_{z^{\prime}}e^{sim(\zeta(z^{\prime}),\xi(h))/\iota}}
$$

其中$ζ$，$ξ$是可学习的线性投影层。$z$是来自Encoder的图的平均表示，$h$是解码器表示的平均值。$Z'$，$h '$分别表示对应的负样本。在给定的文本中，'$h'$'和'$z'$'代表负样本。通过引入对比学习目标，模型可以学习更好地生成多样化和合理的回答，而不仅仅是在训练数据中看到的回答。这有助于降低过拟合的风险，并提高模型在现实场景中的泛化能力。

在处理涉及结构化数据的检索任务时，SANTA[Li et al.， 2023d]的工作采用了三个阶段的训练过程，充分理解结构和语义信息。具体来说，在检索器的训练阶段采用了对比学习，其主要目标是优化查询和文档的嵌入表示。具体优化目标如下：
$$
\mathcal{L}_{DR}=-log\frac{e^{sim(q,d^+)}}{e^{f(q,d^+)}+\sum_{d^-\in D^-}e^{sim(q,d^-)}}
$$

其中$q$和$d$是由编码器编码的查询和文档。$d^−$，$d^+$分别代表负样本和正样本。在生成器的初始训练阶段，我们利用对比学习对结构化数据和相应的非结构化数据的文档描述进行对齐。优化目标如上所述。

此外，在生成器的后期训练阶段，受文献[Sciavolino et al.， 2021, Zhang et al.， 2019]的启发，我们认识到实体语义在检索过程中学习文本数据表示方面的显著有效性。因此，我们首先在结构化数据中执行实体标识，然后对生成器的训练数据的输入部分中的实体应用掩码，使生成器能够预测这些掩码。以下的优化目标是：
$$
\begin{aligned}\mathfrak{L}_{MEP}&=\sum_{j=1}^k-logP(Y_d(t_j)|X_d^{mask},Y_d(t_1,...,j-1))\end{aligned}
$$

其中$Y_d(t_j)$表示序列$Y_d$中的第$j$个记号，$Yd = < mask >_1, ent_1，…， < mask >_n, ent_n$为包含掩码实体的ground truth序列。在整个训练过程中，我们通过从上下文中获取必要的信息来恢复屏蔽实体，理解文本数据的结构语义，并在结构化数据中对齐相关实体。我们优化了语言模型，以填充隐藏的跨度，并更好地理解实体语义[Ye等人，2020]。



## 6. Augmentation in RAG

本章主要从**增强的阶段**、**增强数据源**和**增强的过程**三个维度来阐述RAG开发中的关键技术。RAG核心组件的分类如图4所示。

![image-20240106124631660](./RAG综述.assets/image-20240106124631660.png)

### 6.1 增强阶段

RAG作为一项知识密集型任务，在语言模型训练的**训练前阶段**、**微调阶段**和**推理阶段**采用了不同的技术方法。

#### Pre-training Stage

自预训练模型出现以来，研究者们通过预训练阶段的检索方法来提高预训练语言模型(Pre-training Language model, PLMs)在开放领域问答中的性能。在预先训练的模型中识别和扩展内隐知识是很有挑战性的。REALM[Arora等人，2023]介绍了一种更模块化和可解释的知识嵌入方法。遵循遮罩语言模型(MLM)范式，REALM将预训练和微调建模为一个检索然后预测的过程，其中语言模型通过基于masked sentences $x$预测masked tokens $y$，建模$P(x|y)$进行预训练。

RETRO[Borgeaud等人，2022]利用检索增强对自回归语言模型进行预训练，通过检索大量标记数据并显著减少模型参数，实现了大规模的预训练。RETRO与GPT模型共享主干结构，并引入了额外的RETRO编码器，对从外部知识库检索到的相邻实体的特征进行编码。此外，RETRO在其解码器变压器结构中集成了按块交叉注意层，以有效地集成来自RETRO编码器的检索信息。复古达到较低的困惑标准GPT模型。此外，它提供了通过更新检索数据库来更新存储在语言模型中的知识的灵活性，而不需要重新训练语言模型[Petroni等人，2019]。

Atla[Izacard et al.， 2022]采用了类似的方法，在训练前和微调阶段结合了使用T5架构的检索机制[rafael et al.， 2020]。在预训练之前，它使用预训练的T5初始化编码-解码器LM主干，并使用预训练的设计器初始化稠密检索器。在预训练过程中，它每1000步刷新一次异步索引。

COG [Vaze等人，2021]是一个文本生成模型，它通过从现有的文本集合中逐渐复制文本片段(如单词或短语)来形式化其生成过程。与按顺序选择单词的传统文本生成模型不同，COG利用高效的向量搜索工具来计算文本片段的有意义的上下文表示，并对它们进行索引。因此，文本生成任务被分解为一系列的复制和粘贴操作，在每个时间步骤中，相关的文本片段是从文本集合中寻找，而不是从一个独立的词汇表中选择。COG在各个方面展示了比RETRO更好的性能，包括问答、领域适应和扩展短语索引。

另一方面，随着标度规律的发现，模型参数迅速增加，使自回归模型成为主流。研究人员还在探索是否可以使用RAG方法对更大的模型进行预训练。RETRO++[Wang et al.， 2023a]是RETRO的扩展，增加了模型的参数比例。研究发现，在文本生成质量、事实准确性、低毒性和下游任务的准确性方面，尤其是在知识密集型任务，如开放领域的问题回答方面，都有持续的改善。这些研究结果为训练前的自回归语言模型与未来基础模型的检索提供了重要的方向。

总之，增强前训练的优势和局限性是显而易见的。从积极的方面来看，这种方法提供了一个更强大的基础模型，在复杂性、文本生成质量和下游任务性能方面优于标准GPT模型。此外，与纯预训练模型相比，该模型通过使用更少的参数实现更高的效率。它尤其擅长处理知识密集型任务，允许通过对特定领域语料库的训练来创建特定领域的模型。然而，也有缺点，包括需要大量的训练前数据和更大的训练资源，以及更新速度较慢的问题。特别是随着模型尺寸的增大，增强检索训练的成本也相对较高。尽管存在这些局限性，但该方法在模型鲁棒性方面具有显著的特点。经过训练后，基于纯训练前的检索增强模型消除了外部库依赖的需要，提高了生成速度和操作效率。

#### Fine-tuning Stage

在下游微调阶段，研究人员采用了各种方法来微调检索器和生成器，以改进信息检索，主要是在opendomain问答任务中。对于检索器finetuning, REPlUG[Shi et al.， 2023]将语言模型(LM)视为一个黑箱，并通过可调的检索模型对其进行了增强。repug通过有监督信号获取黑匣子语言模型的反馈，对初始检索模型进行了改进。另一方面，UPRISE[Cheng et al.， 2023a]通过在不同的任务集上进行微调，创造出一种轻量级且多才多艺的检索器，从而对猎犬进行微调。这种检索器可以为零镜头任务自动提供检索提示，展示了它的通用性和跨任务和模型的性能改进。

同时，微调生成器的方法包括Self-Mem[Cheng等人，2023b]，它通过示例内存池对生成器进行微调，Self-RAG[Asai等人，2023b]，通过生成反射令牌满足主动检索需求。RADIT[Lin et al.， 2023]方法通过最大化给定一个检索增强指令的swers的正确概率，对生成器和检索器进行微调。它更新了生成器和检索器，以最小化文档和查询之间的语义相似性，有效地利用了相关的背景知识。

此外，SUGRE[Kang等人，2023]引入了对比学习的概念。它对检索器和生成器进行端到端微调，确保生成非常详细的文本和检索到的子图。使用基于图形神经网络(GNN)的上下文感知子图检索器，SURGE从与正在进行的对话对应的知识图谱中提取相关知识。这确保生成的响应忠实地反映检索到的知识。SURGE为此使用了一个不变但有效的图形编码器和一个图形-文本对比学习目标。

总之，在微调阶段的增强方法表现出几个特点。首先，对大语言模型和检索器进行微调可以更好地适应特定任务，提供同时微调其中一项或两项任务的灵活性，如RePlug[Shi et al.， 2023]和RA-DIT[Lin et al.， 2023]等方法所示。其次，这种微调的好处延伸到适应不同的下游任务，正如UPRISE所证明的[Cheng等人，2023a]，使模型更加灵活。此外，微调使模型能够更好地适应各种语料库中的不同数据结构，这对于图形结构语料库尤其有利，SUGRE方法强调了这一点。

但是，这个阶段的微调也有一些限制，比如需要专门为RAG微调准备的数据集，以及与推断阶段的RAG相比，需要大量的计算资源。总的来说，在微调过程中，研究人员可以根据特定的需求和数据格式灵活地定制模型，与训练前阶段相比减少了资源消耗，同时保留了调整模型输出风格的能力。

#### Inference Stage

将RAG 方法与大语言模型相结合已经成为推理阶段的一个流行研究方向。值得注意的是，朴素RAG的研究范式依赖于推理阶段对检索内容的整合。

为了克服朴素RAG的局限性，研究者在推理阶段的RAG中引入了更丰富的上下文。DSP[Khattab等人，2022]框架依赖于一个复杂的管道，涉及在冻结的语言模型(LM)和检索模型(RM)之间传递自然语言文本，为模型提供更多信息的上下文，以提高生成质量。PKG为大语言模型配备了知识引导模块，允许在不改变大语言模型参数的情况下访问相关知识，使模型能够执行更复杂的任务。此外，CREA-ICL[Li et al.， 2023b]利用跨语言知识的同步检索来帮助获取额外信息，同时通过从大语言模型中取样一个或多个段落来背诵表单上下文。

在推理阶段，优化RAG过程有助于适应更有挑战性的任务。例如，ITRG[Feng et al.， 2023a]通过迭代检索和搜索正确的推理路径，提高了对需要多步推理任务的适应性。ITERRETGEN[Shao et al.， 2023]采用迭代方法来合并检索和生成，实现了“检索增强生成”和“生成增强检索”的交替过程。

另一方面，IRCOT[Trivedi et al.， 2022]融合了RAG和CoT的概念[Wei et al.， 2022]，采用交替CoT引导的检索，并利用检索结果来改进CoT。这种方法显著提高了GPT-3跨各种QA任务的性能，突出了集成检索和生成的潜在优势。

总而言之，推理阶段增强方法具有轻量级、经济、不需要额外训练和使用强大的预训练模型的优点。其主要优点在于在调优过程中冻结大语言模型的参数，重点在于提供更适合需求的上下文，具有快速和低成本的特点。然而，这种方法也有一些限制，包括需要额外的数据处理和流程优化，同时受限于基础模型的功能。通常，这种方法通常与过程优化技术相结合，如逐步推理、迭代推理和自适应检索，以更好地满足不同任务的需求。

### 6.2 增强数据源

数据来源是影响RAG有效性的关键因素。不同的数据源提供不同的知识粒度和维度，需要不同的处理方法。它们主要分为三类：非结构化数据、结构化数据和由大语言模型生成的内容。

#### 用非结构化数据扩充

非结构化数据主要包括文本数据，通常派生自纯文本语料库。此外，其他文本数据也可以作为检索来源，如用于大模型微调的Prompt数据[Cheng等人，2023a]和跨语言数据[Li等人，2023b]。

在文本粒度方面，除了常见的块(包括句子)，检索单元可以是标记(例如kNN-LM[Khandelwal等人，2019])、短语(例如NPM[Lee等人，2020]、COG[Vaze等人，2021])和文档段落。细粒度的检索单元通常可以更好地处理罕见的模式和域外场景，但会增加检索成本。

在词层面，FLARE采用主动检索策略，只有当LM生成低概率词时才进行检索。该方法首先生成一个临时的下一个句子，用于相关文档的检索，然后在检索到的文档条件下重新生成下一个句子，以预测后续的句子。

在块级，RETRO使用前一个块检索最近的相邻块，并将此信息与前一个块的上下文信息集成起来，以指导下一个块的生成。RETRO通过从检索数据库中检索最近的相邻块$N(C_{i−1})$，然后融合前一个块的上下文信息$(C_1，…， C_{i−1})$和$N(C_{i−1})$检索信息，通过交叉注意引导下一个块$C_i$的生成。为了保持因果关系，第$i$块$C_i$的自回归生成只能使用前一个块$N(C_{i−1})$的最近邻居，而不是$N(C_i)$。

#### 用结构化数据扩充

像知识图谱(KG)这样的结构化数据源逐渐集成到RAG的范式中。经过验证的KGs可以提供更高质量的背景，减少模特产生幻觉的可能性。

RET-LLM [Modarressi et al.， 2023]通过从过去的对话中提取关系三元组以供将来使用，构建了个性化的知识图谱内存。SUGRE[Kang et al.， 2023]使用图神经网络(GNN)嵌入从知识图谱中检索到的相关子图，以防止模型生成与上下文无关的回复。SUGRE[Kang等人，2023]采用了一种图编码方法，将图结构反映到PTMs的表示空间中，并在图文本模式之间利用多模态对比学习目标，以确保检索到的事实和生成的文本之间的一致性。KnowledgeGPT[Wang et al.， 2023c]以代码格式生成知识库(知识库)的搜索查询，并包含预定义的知识库操作函数。除了检索之外，KnowledgeGPT还提供了在个性化知识库中存储知识的功能，以满足不同用户的需求。这些结构化数据源为RAG提供了更丰富的知识和上下文，有助于改进模型性能。

#### 大语言模型生成的内容RAG

由于注意到RAG回忆的辅助信息并不总是有效的，甚至有可能产生负面影响，一些研究通过深入探究大语言模型的内部知识，扩展了RAG的研究范式。这种方法利用大语言模型本身生成的内容进行检索，目的是提高下游任务的性能。以下概述了这一类别内值得注意的研究：

SKR[Wang et al.， 2023d]使用一个标记的训练集，将模型可以直接回答的问题归类为已知问题，而需要检索增强的问题归类为未知问题。该模型被训练来辨别一个问题是否已知，只对未知输入应用检索增强，而直接回答其他输入

GenRead[Yu et al.， 2022]用大语言模型生成器替代检索器。实验结果表明，生成的上下文文档包含正确答案的情况比Naive RAG检索的情况更普遍。生成的答案也显示了优越的质量。作者将此归因于生成文档级上下文的任务与因果语言建模的训练前目标之间的一致性，从而允许更好地利用存储在模型参数中的世界知识。

Selfmem[Cheng等人，2023b]迭代地使用检索增强生成器来创建一个无界内存池。内存选择器用于选择一个输出作为后续代的内存。这个输出作为原问题的对偶问题。通过结合原始问题和双重问题，检索增强生成模型可以利用自己的输出来增强自己。

这些不同的方法展示了RAG检索增强的创新策略，旨在提高模型的性能和有效性。

### 6.3 增强的过程

大多数RAG研究通常只执行一个检索和生成过程。然而，单个检索可能包含冗余信息，导致“中间丢失”现象[Liu et al.， 2023]。这种冗余信息会模糊关键信息或包含与真实答案相反的信息，对生成效应产生负面影响[Yoran et al.， 2023]。此外，在需要多步骤推理的问题中，从单一检索获得的信息是有限的。

目前优化检索过程的方法主要有**迭代检索**和**自适应检索**。这允许模型在检索过程中进行多次迭代，或者自适应地调整检索过程以更好地适应不同的任务和场景。

#### Iterative Retrieval 迭代检索

定期收集基于原始查询和生成文本的文档可以为大语言模型提供额外的资料[Borgeaud et al.， 2022, Arora et al.， 2023]。在多次迭代检索中提供额外的引用提高了后续答案生成的鲁棒性。但是，这种方法在语义上可能是不连续的，并可能导致收集大量无用的信息，因为它主要依赖于n个标记序列来分离生成的和检索到的文档。递归检索和多跳检索用于特定的数据场景。

递归检索可以首先通过结构化索引处理数据，然后逐级检索。当检索层次结构丰富的文档时，可以对整个文档或长PDF中的每个部分进行摘要。然后根据摘要执行检索。确定文档后，对内部块进行第二次检索，实现递归检索。多跳检索通常用于进一步挖掘图结构数据源中的信息[Li等人，2023c]。

有些方法迭代检索和生成的步骤。ITER-RETGEN [Shao et al.， 2023]协同使用“检索增强生成”和“生成增强检索”来完成需要信息再现的任务。也就是说，模型使用完成任务所需的内容来响应输入任务，这些目标内容作为检索更多相关知识的信息上下文。这有助于在另一个迭代中生成更好的响应。

IRCoT[Trivedi等人，2022]也探索了为每个生成的句子检索文档，并在思想链的每一步引入检索。该算法利用语义相关度来指导检索，并利用检索结果来提高语义相关度，保证语义的完整性。

#### Adaptive Retrieval 自适应检索

实际上，前两部分中描述的RAG方法遵循的是一种被动的方法，其中检索是优先的。这种方法涉及到查询相关文档并根据上下文将其输入到大语言模型中，可能会导致效率问题。Flare[Jiang et al.， 2023b]和SelfRAG[Asai et al.， 2023b]等引入的自适应检索方法对RAG的检索过程进行了优化，使大语言模型能够积极判断检索的时机和内容。这有助于提高检索信息的效率和相关性。

事实上，大语言模型积极使用工具和做出判断的方式并非源于RAG，而是被广泛应用于大型模型的代理中[Yang et al.， 2023c, Schick et al.， 2023, Zhang, 2023]。Graph-Toolformer[Zhang, 2023]的检索步骤大致分为：大语言模型主动使用检索器，Self-Ask和DSP[Khattab et al.， 2022]尽量使用少量的shot提示触发大语言模型搜索查询。当大语言模型认为有必要时，他们可以决定搜索相关的查询来收集必要的资料，类似于代理的工具调用。

WebGPT[Nakano等人，2021]使用了一个强化学习框架来自动训练GPT-3模型，以使用搜索引擎生成文本。它使用特殊的令牌来执行操作，包括在搜索引擎上查询、滚动排名和引用引用。这允许GPT-3利用搜索引擎生成文本。另一方面，

Flare[Jiang等人，2023b]则自动进行检索，并基于生成文本的概率来解决周期性文档检索的成本问题。在生成过程中，它使用概率作为衡量lms信心的指标。当一个词的概率低于一个预定义的阈值时，信息检索系统将检索引用并删除概率较低的词。这种方法设计用于处理大语言模型可能需要额外知识的情况。

Self-RAG[Asai等人，2023b]引入了一项名为Reflection token的重要创新。生成这些特殊标记来检查输出，有两种类型：Retrieve和Critic。该模型可以自主决定何时检索段落或使用设置的阈值触发检索。当需要检索时，产生器同时处理多个段落，执行片段级波束搜索以获得最佳序列。使用批评家分数更新每个细分的分数，并且可以在推理过程中调整这些权重，以定制模型的行为。Self-RAG框架还允许大语言模型自主决定是否需要召回，避免培训额外的分类器或依赖NLI模型。这增强了模型自主判断输入并生成准确答案的能力。

## 7. RAG评估

在探索RAG的开发和优化过程中，如何有效地评估RAG的性能已成为一个核心问题。本章主要讨论了评估方法、RAG的关键指标、应具备的能力以及一些主流的评估框架。

### 7.1 评估方法

评估RAG有效性的方法主要有两种：独立评估和端到端评估[Liu, 2023]。

#### 独立评估

独立评估包括评估检索模块和生成(读/合成)模块。

1. 检索模块

   一套度量系统(如搜索引擎、推荐系统或信息检索系统)根据查询或任务对项目进行排序的有效性的度量标准，通常用于评估RAG检索模块的性能。例如命中率、MRR、NDCG、精度等。

2. 生成模块

   这里的生成模块指的是通过将检索到的文档补充到查询中而形成的增强的或合成的输入，不同于最终的答案/响应生成，后者通常是端到端评估的。生成模块的评估指标主要关注上下文相关性，衡量检索到的文档与查询问题的相关性。

#### 端到端评估

端到端评估考察由RAG模型为给定输入生成的最终响应，包括模型生成的答案与输入查询的相关性和对齐。从内容生成目标来看，评价可以分为无标签内容和有标签内容。未标记的内容评价指标包括回答的保真度、回答的相关性、无害性等，而标记的内容评价指标包括准确性和EM。另外，从评价方法的角度来看，端到端的评价可以分为使用大语言模型的人工评价和自动评价。以上总结了RAG端到端评估的一般情况。此外，基于RAG在特定领域的应用，采用了特定的评价指标，如问答任务中的EM [Borgeaud等人，2022,Izacard等人，2022]，摘要任务中的UniEval和E-F1 [Jiang等人，2023b]，机器翻译中的BLEU [Zhong等人，2022]。这些度量有助于理解RAG在各种特定应用场景中的性能。

### 7.2 关键指标和能力

现有的研究往往缺乏对检索增强生成对不同大语言模型的影响的严格评估。在大多数情况下，评估RAG应用于不同的下游任务和不同的猎犬可能产生不同的结果。然而，一些学术和工程实践已经把重点放在了RAG的一般评价指标和有效使用RAG所需的能力上。本节主要介绍评估RAG有效性的关键指标和评估其性能的基本能力。

#### 关键指标

最近的OpenAI报告[Jarvis和Allard, 2023]提到了优化大型语言模型(大语言模型)的各种技术，包括RAG及其评估指标。此外，最新的评估框架，如RAGAS[Es et al.， 2023]和ARES[Saad-Falcon et al.， 2023]也涉及RAG评估指标。总结这些工作，主要关注三个核心指标：答案的忠实性、答案相关性和上下文相关性。

1. **Faithfulness**真实性

   这个度量强调模型生成的答案必须保持对给定上下文的真实，确保答案与上下文信息一致，不偏离或抵触它。评估的这一方面对于解决大型模型中的错觉至关重要。

2. **Answer Relevance**答案相关性

   这个度量强调生成的答案需要与所提出的问题直接相关。

3. **Context Relevance**上下文相关性

   这个指标要求检索到的上下文信息尽可能准确和有针对性，避免不相关的内容。毕竟，处理长文本对大语言模型来说代价高昂，而且过多的无关信息会降低大语言模型利用上下文的效率。

   OpenAI报告还提到“上下文回忆”作为补充指标，衡量模型检索回答问题所需的所有相关信息的能力。这个指标反映了RAG检索模块的搜索优化级别。低召回率表明可能需要优化搜索功能，例如引入重新排名机制或微调嵌入，以确保更相关的内容检索。

#### 关键能力

RGB的工作[Chen等人，2023b]根据RAG所需的四种基本能力，包括噪声鲁棒性、负排斥、信息集成和反事实鲁棒性，分析了不同大型语言模型的性能，为检索增强生成建立了一个基准。RGB专注于以下四项功能：

1. **Noise Robustness**噪声鲁棒性

   这种能力衡量模型处理噪声文档的效率，这些噪声文档与问题相关，但不包含有用的信息。

2. **Negative Rejection**消极拒绝

   当模型检索的文档缺乏回答问题所需的知识时，模型应该正确地拒绝响应。在消极拒绝的测试设置中，外部文件只包含噪音。理想情况下，大语言模型应该发出“缺乏信息”或类似的拒绝信号。

3. **Information Integration**信息集成

   这种能力评估模型是否能够集成来自多个文档的信息以回答更复杂的问题。

4. **Counterfactual Robustness**反事实鲁棒性

   该测试的目的是评估当接收到关于检索信息中潜在风险的指示时，模型是否能够识别和处理文档中已知的错误信息。反事实健壮性测试包括大语言模型可以直接回答的问题，但是相关的外部文档包含事实错误。

### 7.3 评估框架

最近，大语言模型社区一直在探索使用“大语言模型as judge”进行自动评估，许多人利用强大的大语言模型(如GPT-4)来评估他们自己的大语言模型应用程序的输出。Databricks使用GPT-3.5和GPT-4作为大语言模型法官来评估他们的聊天机器人应用程序的实践表明，使用大语言模型作为自动评估工具是有效的[Leng et al.， 2023]。他们相信这种方法也可以高效且经济地评估基于RAG的应用。

在RAG评价框架领域，RAGAS和ARES相对较新。这些评估的核心焦点是三个主要指标：答案的忠实性、答案相关性和上下文相关性。此外，业界提出的开源库truulens也提供了类似的评估模式。这些框架都使用大语言模型作为评判标准。由于TruLens与RAGAS类似，本章将专门介绍RAGAS和ARES。

#### RAGAS

该框架考虑了检索系统识别相关段落和关键上下文段落的能力、大语言模型忠实地使用这些段落的能力以及生成这些段落本身的质量。RAGAS是一个基于简单手写提示符的评估框架，它使用这些提示符以完全自动化的方式度量质量的三个方面——答案忠实度、答案相关性和上下文相关性。在这个框架的实现和实验中，所有的提示都使用gpt3.5-turbo-16k模型进行评估，该模型可以通过OpenAI API获得[Es et al.， 2023]。

**算法原理**

1. 评估答案的真实性：使用大语言模型将答案分解成单独的语句，并验证每个语句是否与上下文一致。最后，通过比较支持的语句数量和语句总数，可以计算出“忠实分数”。
2. 评估答案的相关性：使用大语言模型生成潜在的问题，并计算这些问题与原始问题之间的相似性。答案相关性得分是通过计算所有生成的问题与原始问题的平均相似度而得出的。
3. 评估上下文相关性：使用大语言模型提取与问题直接相关的句子，并使用这些句子与上下文中句子总数的比例作为上下文相关性得分。

#### ARES

ARES旨在从三个方面自动评估RAG系统的性能：上下文相关性、答案忠实性和答案相关性。这些评估指标类似于RAGAS中的那些。然而，作为一种基于简单手写提示的较新的评估框架，RAGAS对新的RAG评估设置的适应性有限，而这正是ARES工作的意义之一。此外，正如评估显示的那样，ARES的表现明显低于RAGAS。

ARES通过使用少量的人工标注数据和合成数据，降低了评估成本，并利用预测驱动推理(Predictive-Driven Reasoning, PDR)提供统计置信区间，提高了评估的准确性[Saad-Falcon et al.， 2023]。

**算法原理**

1. 生成合成数据集：ARES最初使用语言模型从目标语料库中的文档生成合成问题和答案，以创建正样本和负样本。
2. 准备大语言模型评委：下一步，ARES将使用合成数据集对轻量级语言模型进行微调，以训练它们评估上下文相关性、答案可信度和答案相关性。
3. 使用置信区间对RAG系统进行排序：最后，ARES将这些判断模型应用于RAG系统评分，并将其与使用PPI方法手工注释的验证集相结合，生成置信区间，从而可靠地估计RAG系统的性能。

## 8. 未来展望

在本章中，我们探讨了RAG的三种未来前景，即垂直优化、水平扩张和RAG的生态系统。

### 8.1 RAG的垂直优化

尽管在过去的一年中RAG技术取得了快速的进步，但在其垂直领域中仍有几个领域需要进一步研究。

首先，RAG中的长上下文问题是一个重大的挑战。如文献[Xu et al.， 2023c]所述，RAG的生成阶段受到大语言模型上下文窗口的约束。如果窗口太短，可能包含的相关信息不够;如果太长，可能会导致信息丢失。目前，扩展大语言模型的上下文窗口，甚至扩展到无限上下文，是大语言模型发展的一个关键方向。然而，一旦移除上下文窗口约束，RAG应该如何适应仍然是一个值得注意的问题。

其次，RAG的鲁棒性是另一个重要的研究热点。如果检索过程中出现不相关的噪声，或者检索内容与事实相矛盾，则会显著影响RAG的有效性。这种情况被比喻为“向一个有毒的蘑菇打开一本书”。因此，增强rag的鲁棒性越来越受到研究者的关注，如研究[Yu et al.， 2023a, Glass et al.， 2021, Baek et al.， 2023]。

第三，RAG和微调的协同问题也是一个主要的研究重点。Hybrid逐渐成为RAG的主流方法之一，以RADIT为例[Lin et al.， 2023]。如何协调两者之间的关系，同时获得参数化和非参数化的优点是一个需要解决的问题。

最后，RAG的工程实践是一个值得关注的重要领域。实现的简单性以及与公司工程需求的一致，为RAG的崛起做出了贡献。然而，在工程实践中，如何提高大规模知识库场景中的检索效率和文档召回率，如何确保企业数据安全，如防止大语言模型被诱导公开文档的源、元数据或其他信息，是需要解决的关键问题[Alon等人，2022]。

### 8.2 RAG的水平扩展

在水平方向上，RAG的研究得到了迅速的发展。RAG的思想从最初的文本问答领域开始，逐渐应用到更多的模态数据，如图像、代码、结构化知识、音频和视频等。这方面的工作已经有很多了。

在图像领域，BLIP2的propozhiyosal [Li et al.， 2023a]使用冻结图像编码器和大规模语言模型进行视觉语言预训练，降低了模型训练的成本。此外，该模型可以从零样本生成图像到文本的转换。在文本生成领域，使用VBR[Zhu et al.， 2022]方法生成图像来指导语言模型的文本生成，在开放文本生成任务中效果显著。

在代码领域，RBPS[Nashid等人，2023]用于与代码相关的小规模学习。通过编码或频率分析，可以自动检索与开发人员任务类似的代码示例。这种技术已经在测试断言生成和程序修复任务中证明了它的有效性。在结构化知识领域，CoK[Li et al.， 2023c]提示等方法首先从知识图谱中检索与输入问题相关的事实，然后将这些事实以提示的形式添加到输入中。该方法在知识图谱问答任务中取得了较好的效果。

在音频和视频领域，GSS[Zhao et al.， 2022]方法从口语词汇库中检索并连接音频剪辑，立即将MT数据转换为ST数据。UEOP[Chan等人，2023]通过引入语音到文本映射的外部离线策略，实现了端到端自动语音识别的新突破。文本-语音方法生成的音频嵌入和语义文本嵌入可以通过基于knn的注意融合对ASR进行偏置，有效缩短了域适应时间。Vid2Seq[Yang et al.， 2023a]架构通过引入特殊的时间标记来增强语言模型，使其能够在相同的输出序列中无缝地预测事件边界和文本描述。

### 8.3 RAG生态系统

#### 下游任务与评价

通过整合来自广泛知识库的相关信息，RAG在增强语言模型处理复杂查询和生成信息丰富的响应的能力方面显示出了巨大的潜力。大量研究表明RAG在各种下游任务中表现良好，如开放式问题回答和事实验证。RAG模型不仅提高了下游应用中信息的准确性和相关性，还增加了响应的多样性和深度。

鉴于RAG的成功，探索模型在多领域应用中的适应性和通用性将是未来工作的一部分。这包括它在专业领域知识问答中的应用，如医学、法律和教育。在下游任务(如专业领域知识问答)的应用中，RAG可能比微调提供更低的培训成本和更好的性能收益。

同时，完善RAG的评价体系，对其在不同下游任务中的应用进行评价和优化，对于模型在具体任务中的效率和效益至关重要。这包括为不同的下游任务开发更准确的评估指标和框架，例如上下文相关性、内容创造性和无害性等。

此外，通过RAG增强模型的可解释性，让用户更好地理解模型如何以及为什么做出特定的响应，也是一项有意义的任务。

#### 技术栈

在RAG生态系统中，相关技术栈的开发起到了推动作用。比如，随着ChatGPT的流行，LangChain和LLamaIndex很快就被大家所熟知。它们都提供了一套丰富的与rag相关的api，逐渐成为大模型时代不可或缺的技术之一。同时，新型技术栈也在不断开发。虽然它们没有像LangChain和LLamaIndex那样提供那么多的功能，但它们更专注于自己的独特特性。例如，Flowise AI6强调低代码，允许用户实现各种以RAG为代表的AI应用程序，而无需编写代码，只需通过拖放即可。其他新兴技术包括HayStack、Meltno和Cohere Coral。

除了人工智能原生框架，传统软件或云服务提供商也扩大了他们的服务范围。例如，由矢量数据库公司Weaviate提供的Verba7专注于个人助理。亚马逊为用户提供了基于RAG思维的智能企业搜索服务工具Kendra。用户可以通过内置连接器在不同的内容存储库中进行搜索。

技术堆栈和RAG的发展是相辅相成的。新技术对现有技术栈提出了更高的要求，而技术栈功能的优化进一步推动了RAG技术的发展。总的来说，RAG工具链的技术堆栈已经初步形成，许多企业级应用已经逐渐出现，但一个一体化的平台仍然需要进一步完善。

## 9. 结论

本文深入探讨了检索增强生成(retrieve - augmented Generation, RAG)，这是一种利用外部知识库来补充大型语言模型(Large Language model，大语言模型)上下文并生成响应的技术。值得注意的是，RAG结合了来自大语言模型的参数化知识和非参数化的外部知识，缓解了幻觉问题，通过检索技术及时识别信息，提高了响应的准确性。此外，通过引用来源，RAG增加了模型输出的透明度和用户信任。RAG还可以通过索引相关的文本语料库来根据特定的领域进行定制。将RAG的发展和特点归纳为三种范式：朴素式RAG、高级式RAG和模块化RAG，每种范式都有各自的模型、方法和缺点。Naive RAG主要涉及“检索-阅读”过程。Advanced RAG使用更精细的数据处理，优化知识库索引，并引入多次或迭代检索。随着研究的深入，RAG集成了诸如微调等其他技术，导致了模块化RAG范式的出现，模块化RAG范式用新的模块丰富了RAG过程，并提供了更多的灵活性。

在后面的章节中，我们将进一步详细分析RAG的三个关键部分。第四章介绍了RAG的检索器，如何处理语料库以获得更好的语义表示，如何缓解查询与文档之间的语义鸿沟，如何调整检索器以适应生成器。第5章说明了生成器如何通过对检索到的文档进行后处理以获得更好的生成结果，避免了“中间丢失”的问题，以及调整生成器以适合检索器的方法。随后，在第六章中，我们从检索阶段、检索数据源和检索过程三个方面对当前的检索增强方法进行了综述。第七章阐述了如何评价现有的RAG方法，包括评价指标、关键指标和现有的评价框架，最后展望了RAG未来可能的研究方向。RAG作为一种检索与生成相结合的方法，在未来的研究中有许多潜在的发展方向。通过不断改进技术，扩大应用范围，可以进一步提高RAG的性能和实用性。

# Retrieval-Augmented Generation for AI-Generated Content: A Survey

![image-20240304150254344](./RAG综述.assets/image-20240304150254344.png)

【链接】：https://arxiv.org/abs/2402.19473

## 3. 方法

在本节中，我们首先介绍AIGC背景下的RAG基础。随后，我们概述了进一步提高RAG有效性的增强方法。

![image-20240304140715262](./RAG综述.assets/image-20240304140715262.png)

> RAG基础范式的分类

### 3.1 RAG基础

根据检索器增强生成器的方式，我们将RAG基础范式分为4个不同的类别，如图6所示。

#### 1) 基于查询的RAG

基于查询的RAG也称为提示增强（prompt augmentation）。它将用户的查询与检索过程中获取的文档的见解集成到语言模型输入的初始阶段。这种范例在RAG的应用程序中被广泛采用。检索文档后，将其内容与原始用户查询合并，以创建复合输入序列。这个增强的序列随后被输入到一个预先训练好的语言模型中，以产生响应。

REALM[31]采用双bert框架简化知识检索和集成，将预训练模型与知识提取器结合在一起。最初的BERT模块与文档一起处理输入问题以方便检索，利用MIPS选择概率最高的前k个文档并定期更新索引。然后将获得的文档片段与查询集成，并提供给第二个BERT模块以生成多个输出，这些输出聚合为一个单一的综合响应。RAG[32]将预训练的语言模型与知识检索机制协同起来，利用DPR和BART结构来完成检索增强生成任务。DPR作为检索组件，从庞大的文档数据库中获取相关信息，而BART使用这些信息生成文本。RAG-Token和RAG-Sequence的检索时间不同，前者在每个令牌生成时检索信息，后者对整个序列进行单次检索。KILT[125]侧重于通过将维基百科页面与特定快照对齐，并通过BLEU评分评估确定最相关的文本范围，从而确保信息的准确性和可靠性。它过滤掉低质量的数据，以保持高标准的信息映射，提供各种检索系统选项，如Tf-idf、DPR、RAG和BLINK + flair，以支持基于证据的预测或根据任务要求引用。SELF-RAG[126]通过整合检索和评论策略来提高响应的准确性和相关性。最初，该模型使用检索器来搜索与输入问题密切相关的信息段落。随后，评论模型评估这些段落，以确定它们的相关性和对检索文本的支持程度，评估它们对生成响应的影响。最后，生成器模型根据这些信息构建响应，并通过评论标记评估这些响应的质量。

最近，有人提出了一些不修改语言模型体系结构就可以实现RAG的方法，这种方法特别适合通过api访问语言模型的场景。REPLUG[127]通过将语言模型视为“黑盒”来说明这种方法，利用Contriever将相关的外部文档无缝地合并到查询中。REPLUG LSR是LMSupervised Retrieval的一个变体，它通过语言模型监督下的洞察来优化检索，从而进一步完善了这一过程，旨在通过丰富其上下文理解来降低困惑分数并提高模型性能。RALM[128]使用BM25算法进行文档检索和预测性重新排序，选择相关文档进行输入整合。

在当代多模态应用研究中，将检索到的内容集成到输入中已被证明是提高各种任务性能的有效方法。该策略适用于几个关键领域，包括代码生成、音频生成和知识库问答(KBQA)。

对于文本到代码的任务，APICoder[129]和docprompt[40]演示了如何有效地将检索到的信息集成到语言模型中，从而提高生成代码的准确性和相关性。在自动程序修复任务中，CEDAR[130]和InferFix[131]利用检索到的代码片段来辅助修复过程，通过将其与原始输入相结合，增强了模型对修复策略的理解和应用。对于代码完成任务，ReACC[132]采用了提示机制，利用检索到的代码片段作为新输入的一部分，以提高代码完成的准确性和效率。

在音频生成领域，MakeAnAudio[42]利用检索来构建无语言音频的字幕，从而缓解文本到音频训练的数据稀疏性。

最近的研究表明，检索和语言模型相结合的效果显著。Uni-Parser[133]、RNG-KBQA[121]和ECBRF[134]通过将查询和检索信息合并到提示符中，有效地提高了QA系统的性能和准确性。BLLM增强[135]代表了使用黑盒大型语言模型实现零样本KBQA的创新尝试。该方法通过直接将检索到的信息集成到模型输入中，而不需要额外的样本训练，显示了将检索和语言模型结合起来，提高模型在理解和回答未知问题方面的泛化能力的巨大潜力。

在RAG技术的科学领域，GeneGPT[54]用于解决基因组学查询，而ChatOrthopedist[136]旨在为患有特发性脊柱侧凸的青少年提供共同决策支持。两种方法都通过将检索到的信息集成到大型语言模型的提示符中，提高了大型语言模型的应用效率和信息准确性。

在图像生成任务中，RetrieveGAN[43]通过将检索到的信息(包括选定的图像补丁及其对应的边界框)集成到生成器的输入阶段，来增强生成图像的相关性和准确性。IC-GAN[137]通过将噪声向量与实例特征相连接来调节生成图像的特定条件和细节。

在3D生成领域，RetDream[48]最初利用CLIP检索相关3D资产，在输入阶段有效地将检索到的内容与用户输入合并。

#### 2) 基于潜在表征的RAG

在基于潜在表示的RAG框架中，生成模型与检索对象的潜在表示交互，从而提高模型的理解能力和生成内容的质量。

FiD[33]技术利用BM25和DPR来寻找支持性段落。它将检索到的每个段落及其标题与查询连接起来，通过编码器分别处理它们。FiD通过在解码器中融合多个检索段落的信息来有效地利用相关信息生成答案，而不是在编码器中处理每个段落，从而降低了计算复杂度。Fusionin-Decoder方法的应用超越了文本内容处理领域，在处理代码、结构化知识和各种多模态数据集方面展示了巨大的潜力和适应性。特别是在协同相关领域，EDITSUM[138]、BASHEXPLAINER[139]和RetrieveNEdit[140]等技术采用FiD方法，通过编码器处理的融合促进集成。除其他方法外，Re2Com[141]和RACE[141]还设计了针对不同类型输入的多个编码器。在知识库问答(Knowledge Base Question answer, KBQA)领域，FiD方法被广泛采用，显示出显著的有效性。UniK-QA[142]、DECAF[143]、SKP[144]、KD-CoT[145]和ReSKGC[146]通过应用Fusion-in-Decoder技术有效地提高了QA系统的性能。这表明，通过将RAG集成到KBQA中，可以显著提高QA系统的效率和准确性。在Science领域，RetMolRetMol[53]和DeepICL[147]采用Fusion-in-Decoder策略，在解码器阶段整合信息，以增强生成的分子结构的相关性和质量。

Retro[34]率先通过“分块交叉注意”(Chunked Cross-Attention)整合检索文本，这是一种将输入序列分割成离散块的新机制。每个块独立执行交叉注意操作，从而减轻了计算负担。该技术使模型能够选择性地检索和吸收不同序列段的不同文档，从而在整个生成过程中促进动态检索。这增强了模型的适应性，丰富了生成内容的上下文背景。在图像生成领域，交叉注意机制在RAG框架中被广泛采用。Re-imagen[148]、KNN-Diffusion[149]、RDM[150]和LAION-RDM & ImageNet-RDM[151]等方法利用交叉注意力整合多个检索结果，有效提高了模型的整体性能。另一方面，Li[152]介绍了ACM，这是一个文本-图像仿射组合模块，值得注意的是，它没有使用任何形式的注意力机制。

值得注意的是，TOME[153]将重点转向了全面的提及编码，优先考虑了提及的粒度，而不是单纯的实体表示。它一丝不苟地为维基百科上提到的每个实体生成编码，填充了一个拥有大约1.5亿个条目的存储库。这个存储库包含键和值编码以及实体id，支持检索更细粒度的信息。TOME集成了一个初始的转换块来处理输入文本，随后集成了具有记忆注意层的TOME块，促进了多方面信息源的综合，并增强了推理推理能力，即使对于未遇到的实体也是如此。Memorizing Transformers[29]通过在Transformer层中集成knn增强的注意力机制，彻底改变了长文档处理。这种创新在输入序列处理中触发kNN搜索，根据序列和存储的键值对之间的相似性获取数据，从而在不需要完全重新训练的情况下提高性能。这种方法不仅提高了处理效率，而且还扩大了模型的记忆范围，使其能够从生成的输出中进行自我检索，并对广泛的知识库或代码库进行微调。Unlimiformer[154]通过在预训练的编码器-解码器转换器框架中嵌入k近邻(kNN)索引，率先处理不确定长度的输入。在kNN索引中存储输入令牌的隐藏状态允许在解码期间有效地检索高度相关的令牌。这一创新扩展了模型管理长时间序列的能力。

与先前的知识方法不同，EaE[155]使语言模型能够内化显式实体知识。EaE引入了特定于实体的参数化，通过嵌入在变压器体系结构中的实体存储器层来优化推理效率。该层直接从文本数据中获取实体表示，利用稀疏检索策略根据其嵌入获取最近的实体，从而通过计算实体特定信息的聚合来改进模型的理解。

在3D生成领域，ReMoDiffuse[49]引入了语义调制的注意机制。该技术提高了基于文本描述生成相应三维运动的精度。AMD[156]通过融合原始扩散过程和参考扩散过程，实现了从文本到三维运动的高效转换。

在音频领域，Koizumi[41]利用预训练的大规模语言模型，在注意力模块中结合VGGish生成的密集特征和嵌入网络来指导音频字幕的生成。ReAudioLDM[157]使用T5和AudioMAE从文本和音频中提取深度特征，并将这些特征集成到其潜在扩散模型(Latent Diffusion Model, LDM)的注意机制中。

在视频字幕领域，R-ConvED[46]采用卷积编码器-解码器网络架构，该架构借助注意力机制对检索到的视频句子对进行处理，生成隐藏状态并随后生成字幕。CARE[158]引入了一个概念检测器作为解码器的输入，并将概念表示嵌入到混合注意机制中。EgoInstructor[47]使用门交叉注意将这些文本输入与编码的视频特征集成在一起，增强了生成的以自我为中心的视频内容的字幕的相关性和连贯性。

#### 3) 基于逻辑的RAG

在基于logit的RAG中，生成模型在解码过程中通过logit组合检索信息。通常，通过模型对对数进行求和或组合，以产生逐步生成的概率。

kNN-LM[35]模型将预训练的神经语言模型与k近邻搜索相结合。它使用预训练的模型生成候选词列表及其概率分布，同时从数据存储库中进行检索，根据当前上下文找到k个最相关的邻居，从而增强原始语言模型的输出。该模型的核心创新在于它能够从广泛的文本语料库中动态检索信息，显著提高其预测的准确性和相关性，特别是在处理罕见模式和适应各种领域方面。他引入了一个新的框架，该框架仅在必要时执行检索操作，旨在通过自适应检索提高kNN-LM模型的推理效率。该框架通过训练检索适配器来加速模型的推理速度，该适配器在某些场景中自动识别和消除不必要的检索操作。该方法允许模型根据当前上下文动态决定检索的必要性，从而平衡性能和效率之间的权衡，在保持模型性能的同时大幅提高推理速度。

与之前只在测试期间合并记忆的方法不同，TRIME[159]在训练和测试阶段都实现了内存合并，将批处理中的示例视为可访问的内存。TRIME利用新的数据批处理和内存构建技术来有效地利用外部内存。TRIME利用BM25分数将词汇重叠度高的段落打包到同一批中，构建训练记忆，进一步优化模型性能。NPM[160]是一个由编码器和参考语料库组成的非参数掩码语言模型。与在有限词汇表上应用softmax的传统模型不同，NPM在语料库上建模非参数分布。编码器的作用是将语料库中的短语映射到固定大小的向量中，通过检索与掩码位置最相似的短语来填充[MASK]。

除了文本领域之外，代码和图像等其他模式也利用在最终输出阶段将检索到的内容与语言模型集成在一起的方法。

对于代码到文本的转换任务，Rencos[119]从检索到的代码中并行生成多个摘要候选项。然后，它使用编辑距离对这些候选对象进行规范化，并计算最终的概率，以选择与原始代码最匹配的摘要输出。在代码汇总任务中，EDITSUM[138]通过在概率级别集成原型汇总，提高了汇总生成的质量。对于文本到代码的任务，kNN-TRANX[161]模型结合了置信度网络和元知识来合并检索到的代码片段。它利用seq2tree结构生成与输入查询紧密匹配的目标代码，从而提高了代码生成的准确性和相关性。

在图像字幕任务中，MA[162]结合了基于注意力的编码器，使用图像编码器提取视觉特征来构建语义部分，并使用检索到的信息逐字解码。MA在标题解码器和内存增强模块生成的两个分布之间进行插值，以确定下一个单词的分布。

#### 4) Speculative RAG

投机性（Speculative）RAG**寻找机会使用检索**而不是生成来节省资源并加快响应速度。REST[30]用检索代替推测解码[163]中的小模型，从而生成草稿。GPTCache[37]试图通过构建用于存储大语言模型响应的语义缓存来解决使用大语言模型 API时的高延迟问题。

![image-20240304142551227](./RAG综述.assets/image-20240304142551227.png)

> RAG增强的分类

### 3.2 RAG增强

在本节中，我们将介绍增强RAG性能的方法。我们根据增强目标将现有方法分为5个不同的组：输入、检索器、生成器、结果和整个管道。

#### 1) 输入增强

输入指的是用户的查询，它最初被输入到检索器中。输入的质量显著影响检索阶段的最终结果。因此，增强输入变得至关重要。在本节中，我们将介绍两种方法:查询转换和数据增强。

##### a. 查询转换

查询转换可以通过修改输入查询来增强检索结果。Query2doc[164]和HyDE[165]首先使用query生成伪文档，然后使用该文档作为检索的键。这样做的好处是，伪文档将包含更丰富的相关信息，这有助于检索更准确的结果。

##### b. 数据增强

数据增强是指在检索之前提前对数据进行改进，如去除不相关信息、消除歧义、更新过时文档、合成新数据等，可以有效地提高最终RAG系统的性能。MakeAnAudio[42]使用字幕和音频-文本检索为无语言音频生成字幕以减轻数据稀疏性，并添加随机概念音频以改进原始音频。

#### 2) 检索器增强

在RAG系统中，检索过程是至关重要的。一般来说，内容质量越好，就越容易激发大语言模型在上下文学习以及其他生成器和范式中的能力。内容质量越差，越容易引起模型幻觉。因此，在本节中，我们将讨论如何有效地提高检索过程的有效性。

##### a. 递归检索

递归检索是在检索前对查询进行拆分，并执行多次搜索以检索更多、更高质量的内容的过程。Jagerman等[166]使用思维链(Chain-of-Thought, COT)[167]使模型能够逐步分解查询，提供更丰富的相关知识。LLMCS[168]将该技术应用于会话系统，通过重写会话记录获得了更好的检索结果。

##### b. 块优化

块优化技术是指通过调整块的大小来获得更好的检索结果。句子窗口检索[169]是一种有效的方法，它通过获取小块文本并返回被检索片段周围的相关句子窗口来增强检索。这种方法确保了目标句子前后的上下文都包含在内，从而对检索到的信息有了更全面的理解。自动合并检索是LlamaIndex[169]的另一种高级RAG方法，它以树状结构组织文档，父节点包含所有子节点的内容。例如，文章和段落，以及段落和句子，都遵循亲子关系。在检索过程中，对子节点的细粒度搜索最终返回父节点，从而有效地提供更丰富的信息。

##### c. 微调检索器

检索器作为RAG系统的核心部件，在整个系统运行过程中起着至关重要的作用。一个好的嵌入模型可以使语义相似的内容在向量空间中更紧密地结合在一起。检索器的能力越强，为后续发生器提供的有用信息就越多，从而提高RAG系统的有效性。因此，嵌入模型的能力[170]-[173]对RAG系统的整体有效性至关重要。

此外，对于已经具有良好表达能力的嵌入模型，我们仍然可以使用高质量的领域数据或任务相关数据对其进行微调，以提高其在特定领域或任务中的性能。REPLUG[127]将LM视为黑盒，根据最终结果更新检索器模型。APICoder[129]使用python文件和api名称、签名、描述对检索器进行微调。EDITSUM[138]对检索器进行微调，以减少检索后摘要之间的夹板距离。SYNCHROMESH[120]在损失中添加ast的树距离，并使用目标相似度调优来微调检索器。R-ConvED[46]使用与生成器相同的数据对检索器进行微调。

##### d. 混合检索

混合检索是指同时使用多种检索方法。RAPGen[174]和ReACC[132]同时使用密集检索器和稀疏检索器来提高检索质量。Rencos[119]使用稀疏检索器在句法层面检索相似的代码片段，使用密集检索器在语义层面检索相似的代码片段。BASHEXPLAINER[139]首先使用密集检索器捕获语义信息，然后使用稀疏检索器获取词汇信息。RetDream[48]首先用文本检索，然后用图像嵌入检索。

##### e. Re-ranking

Re-ranking（重新排序）技术指的是对检索到的内容重新排序，以获得更大的多样性和更好的结果。Re2G[175]采用了传统检索器之后的重新排序[176]模型。重新排序模型的作用是对检索到的文档进行重新排序，其目的是为了减少将文本压缩成向量所造成的信息丢失对检索质量的影响。AceCoder[177]使用选择器对检索到的程序重新排序。引入选择器的目的是为了减少冗余程序，获得多样化的检索程序。XRICL[178]在检索后使用基于蒸馏的范例重新排序器。

##### f. 元数据过滤

元数据过滤[179]是另一种帮助处理检索文档的方法，它使用元数据(如时间、目的等)对检索文档进行过滤，以获得更好的结果。

#### 3) 生成器增强

在RAG系统中，生成器的质量往往决定了最终输出结果的质量。因此，生成器的能力决定了整个RAG系统效能的上限。

##### a. 提示工程

提示工程（Prompt Engineering）[180]中专注于提高大语言模型输出质量的技术，如提示压缩(prompt compression)、Stepback prompt[181]、Active prompt[182]、Chain of Thought prompt[167]等，都适用于RAG系统中的大语言模型生成器。LLMLingua[183]采用小模型压缩查询的总长度来加速模型推理，缓解不相关信息对模型的负面影响，缓解“迷失在中间”的现象[184]。ReMoDiffuse[49]通过使用ChatGPT将复杂的描述分解为解剖文本脚本。ASAP[185]将范例元组添加到提示符中以获得更好的结果。示例元组由输入代码、函数定义、分析该定义的结果及其相关注释组成。CEDAR[130]使用一个设计好的提示模板将代码演示、查询和自然语言指令组织到一个提示中。XRICL[178]利用COT技术添加翻译对作为跨语言语义解析和推理的中间步骤。MakeAnAudio[42]能够使用其他模态作为输入，这可以为下面的过程提供更丰富的信息。

##### b. 解码调优

解码调优（Decoding tuning）指的是在生成器处理期间添加额外的控件，这可以通过调整超参数来实现更大的多样性，以某种形式限制输出词汇表等等。interfix[131]通过调节解码器的温度来平衡结果的多样性和质量。SYNCHROMESH[120]通过实现补全引擎来消除实现错误，从而限制了解码器的输出词汇表。DeepICL[147]根据一个额外的温度因素控制生成的随机性。

##### c. 微调生成器

通过对生成器的微调，可以提高模型的领域知识的准确性，或更好地适应检索器。RETRO[34]固定了检索器的参数，并使用生成器中的分块交叉注意机制将查询内容与检索器结合起来。APICoder[129]对生成器CODEGEN-MONO 350M[186]进行微调，将一个经过洗牌的新文件与API信息和代码块结合起来。CARE[158]首先使用图像数据、音频数据和视频文本对对编码器进行训练，然后对解码器(生成器)进行微调，以共同降低标题损失和概念检测损失为目标，在此过程中，编码器和检索器被冻结。animation - a - story[187]使用图像数据优化视频生成器，然后对LoRA[188]适配器进行微调，以捕获给定角色的外观细节。RetDream[48]用渲染的图像微调LoRA适配器[188]。

#### 4) 结果增强

在许多情况下，RAG的最终结果可能达不到预期的效果，一些result Enhancement技术可以帮助缓解这个问题。

##### a. 重写输出

SARGAM[189]通过Levenshtein Transformer对删除分类器、占位分类器和插入分类器进行分类，修改了代码相关任务中生成的结果，以更好地适应实际的代码上下文。Ring[190]根据生成器产生的每个令牌日志概率的平均值对候选对象重新排序，从而获得多样性结果。CBR-KBQA[52]通过将生成的关系与知识图谱中查询实体的局部邻域中的关系对齐来修改结果。

#### 5) RAG Pipeline增强

##### a. 自适应检索

一些关于RAG的研究和实践表明，检索并不总是有利于最终生成的结果，当模型本身的参数化知识足以回答相关问题时，过度检索会造成资源浪费，并可能增加模型的混乱。因此，在本章中，我们将讨论确定是否检索的两种方法，即基于规则的方法和基于模型的方法。

**基于规则的方法**

FLARE[191]在发电过程中通过概率主动决定是否搜索以及何时搜索。efficiency - knnlm[36]将KNN-LM[35]和NPM[160]的生成概率与一个超参数λ相结合，以确定生成和检索的比例。malen等[192]在生成答案前对问题进行统计分析，允许模型直接回答高频问题，对低频问题引入RAG。Jiang等[193]研究了模型不确定性(Model Uncertainty)、输入不确定性(Input Uncertainty)和输入统计量(Input Statistics)，综合评估了模型的置信水平。最后，基于模型的置信度，决定是否检索。Kandpal等人[194]通过研究训练数据集中相关文档的数量与模型掌握相关知识的程度之间的关系，帮助确定是否需要检索。

**基于模型的方法**

Self-RAG[126]使用经过训练的生成器，根据不同指令下的检索令牌确定是否执行检索，并通过SelfReflection令牌评估检索文本的相关性和支持程度。最后，最终输出结果的质量基于批判令牌进行评估。Ren等人[195]使用“判断提示”来确定大语言模型是否能够回答相关问题，以及他们的答案是否正确，从而帮助确定检索的必要性。SKR[196]利用大语言模型自身的能力提前判断自己是否能够回答问题，如果能够回答，则不进行检索。

##### b. 迭代RAG

RepoCoder[197]采用迭代检索生成管道，更好地利用代码完成任务中分散在不同文件中的有用信息。它在第i次迭代期间使用先前生成的代码增强检索查询，并获得更好的结果。ITER-RETGEN[198]以迭代的方式协同检索和生成。生成器当前的输出可以在一定程度上反映它还缺少的知识，检索可以检索缺失的信息作为下一轮的上下文信息，这有助于提高下一轮生成内容的质量。



## 4. 应用

![image-20240304145858407](./RAG综述.assets/image-20240304145858407.png)























































































